<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="description"
    content="The latest release of Mabor includes significant changes to its memory management strategy, and tensor-allocated memory can now be reused way more often. Overall, these changes significantly reduce memory usage, especially on the CPU compared to PyTorch.">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Reduced Memory Usage: Mabor's Rusty Approach to Tensor Handling">
  <meta property="og:description"
    content="The latest release of Mabor includes significant changes to its memory management strategy, and tensor-allocated memory can now be reused way more often. Overall, these changes significantly reduce memory usage, especially on the CPU compared to PyTorch.">
  <meta property="og:author" content="Nathaniel Simard">
  <meta property="og:image" content="/_astro/blog2.DuTkwZtR_Savc1.webp">
  <meta property="article:published_time" content="2023-03-21T19:00:00.000Z">
  <link rel="sitemap" href="../../sitemap-index.xml">
  <meta name="viewport" content="width=device-width">
  <link rel="icon" type="image/svg+xml" href="../../favicon.svg">
  <meta name="generator" content="Astro v5.10.1">
  <link rel="canonical" href="../../index.htm">
  <title>Reduced Memory Usage: Mabor&#39;s Rusty Approach to Tensor Handling</title>
  <script
    type="module">try { const t = window._paq = window._paq || []; t.push(["trackPageView"]), t.push(["enableLinkTracking"]), function () { t.push(["setTrackerUrl", "https://burndev.matomo.cloud/" + "matomo.php"]), t.push(["setSiteId", "1"]); const o = document, e = o.createElement("script"), c = o.getElementsByTagName("script")[0]; e.async = !0, e.src = "https://cdn.matomo.cloud/burndev.matomo.cloud/matomo.js", c?.parentNode?.insertBefore(e, c) }() } catch { }</script>
  <script type="text/partytown" src="../../gtag/js?id=G-SCQPPXXSJY"></script>
  <script type="text/partytown">(function(){const id = "G-SCQPPXXSJY";

    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }

    gtag("js", new Date());

    gtag("config", id);
  })();</script>
  <link rel="stylesheet" href="../../_astro/_blog_.DvB2Xm2x.css">
  <link rel="stylesheet" href="../../_astro/_blog_.CLbeXEfv.css">
  <script>!(function (w, p, f, c) { if (!window.crossOriginIsolated && !navigator.serviceWorker) return; c = w[p] = Object.assign(w[p] || {}, { "lib": "/~partytown/", "debug": false }); c[f] = (c[f] || []).concat(["dataLayer.push"]) })(window, 'partytown', 'forward');/* Partytown 0.11.1 - MIT QwikDev */
    const t = { preserveBehavior: !1 }, e = e => { if ("string" == typeof e) return [e, t]; const [n, r = t] = e; return [n, { ...t, ...r }] }, n = Object.freeze((t => { const e = new Set; let n = []; do { Object.getOwnPropertyNames(n).forEach((t => { "function" == typeof n[t] && e.add(t) })) } while ((n = Object.getPrototypeOf(n)) !== Object.prototype); return Array.from(e) })()); !function (t, r, o, i, a, s, c, l, d, p, u = t, f) { function h() { f || (f = 1, "/" == (c = (s.lib || "/~partytown/") + (s.debug ? "debug/" : ""))[0] && (d = r.querySelectorAll('script[type="text/partytown"]'), i != t ? i.dispatchEvent(new CustomEvent("pt1", { detail: t })) : (l = setTimeout(v, (null == s ? void 0 : s.fallbackTimeout) || 1e4), r.addEventListener("pt0", w), a ? y(1) : o.serviceWorker ? o.serviceWorker.register(c + (s.swPath || "partytown-sw.js"), { scope: c }).then((function (t) { t.active ? y() : t.installing && t.installing.addEventListener("statechange", (function (t) { "activated" == t.target.state && y() })) }), console.error) : v()))) } function y(e) { p = r.createElement(e ? "script" : "iframe"), t._pttab = Date.now(), e || (p.style.display = "block", p.style.width = "0", p.style.height = "0", p.style.border = "0", p.style.visibility = "hidden", p.setAttribute("aria-hidden", !0)), p.src = c + "partytown-" + (e ? "atomics.js?v=0.11.1" : "sandbox-sw.html?" + t._pttab), r.querySelector(s.sandboxParent || "body").appendChild(p) } function v(n, o) { for (w(), i == t && (s.forward || []).map((function (n) { const [r] = e(n); delete t[r.split(".")[0]] })), n = 0; n < d.length; n++)(o = r.createElement("script")).innerHTML = d[n].innerHTML, o.nonce = s.nonce, r.head.appendChild(o); p && p.parentNode.removeChild(p) } function w() { clearTimeout(l) } s = t.partytown || {}, i == t && (s.forward || []).map((function (r) { const [o, { preserveBehavior: i }] = e(r); u = t, o.split(".").map((function (e, r, o) { var a; u = u[o[r]] = r + 1 < o.length ? u[o[r]] || (a = o[r + 1], n.includes(a) ? [] : {}) : (() => { let e = null; if (i) { const { methodOrProperty: n, thisObject: r } = ((t, e) => { let n = t; for (let t = 0; t < e.length - 1; t += 1)n = n[e[t]]; return { thisObject: n, methodOrProperty: e.length > 0 ? n[e[e.length - 1]] : void 0 } })(t, o); "function" == typeof n && (e = (...t) => n.apply(r, ...t)) } return function () { let n; return e && (n = e(arguments)), (t._ptf = t._ptf || []).push(o, arguments), n } })() })) })), "complete" == r.readyState ? h() : (t.addEventListener("DOMContentLoaded", h), t.addEventListener("load", h)) }(window, document, navigator, top, window.crossOriginIsolated);; (e => { e.addEventListener("astro:before-swap", e => { let r = document.body.querySelector("iframe[src*='/~partytown/']"); if (r) e.newDocument.body.append(r) }) })(document);</script>
</head>

<body
  class="flex flex-col text-primary-content default:sections:mx-auto [&#38;>nav]:order-last default:sections:max-w-[1500px] bg-[#0D1117]">
  <script type="module" src="../../_astro/Layout.astro_astro_type_script_index_0_lang.W5886xPT.js"></script>
  <style>
    astro-island,
    astro-slot,
    astro-static-slot {
      display: contents
    }
  </style>
  <script>(() => { var e = async t => { await (await t())() }; (self.Astro || (self.Astro = {})).load = e; window.dispatchEvent(new Event("astro:load")); })();</script>
  <script>(() => { var A = Object.defineProperty; var g = (i, o, a) => o in i ? A(i, o, { enumerable: !0, configurable: !0, writable: !0, value: a }) : i[o] = a; var d = (i, o, a) => g(i, typeof o != "symbol" ? o + "" : o, a); { let i = { 0: t => m(t), 1: t => a(t), 2: t => new RegExp(t), 3: t => new Date(t), 4: t => new Map(a(t)), 5: t => new Set(a(t)), 6: t => BigInt(t), 7: t => new URL(t), 8: t => new Uint8Array(t), 9: t => new Uint16Array(t), 10: t => new Uint32Array(t), 11: t => 1 / 0 * t }, o = t => { let [l, e] = t; return l in i ? i[l](e) : void 0 }, a = t => t.map(o), m = t => typeof t != "object" || t === null ? t : Object.fromEntries(Object.entries(t).map(([l, e]) => [l, o(e)])); class y extends HTMLElement { constructor() { super(...arguments); d(this, "Component"); d(this, "hydrator"); d(this, "hydrate", async () => { var b; if (!this.hydrator || !this.isConnected) return; let e = (b = this.parentElement) == null ? void 0 : b.closest("astro-island[ssr]"); if (e) { e.addEventListener("astro:hydrate", this.hydrate, { once: !0 }); return } let c = this.querySelectorAll("astro-slot"), n = {}, h = this.querySelectorAll("template[data-astro-template]"); for (let r of h) { let s = r.closest(this.tagName); s != null && s.isSameNode(this) && (n[r.getAttribute("data-astro-template") || "default"] = r.innerHTML, r.remove()) } for (let r of c) { let s = r.closest(this.tagName); s != null && s.isSameNode(this) && (n[r.getAttribute("name") || "default"] = r.innerHTML) } let p; try { p = this.hasAttribute("props") ? m(JSON.parse(this.getAttribute("props"))) : {} } catch (r) { let s = this.getAttribute("component-url") || "<unknown>", v = this.getAttribute("component-export"); throw v && (s += ` (export ${v})`), console.error(`[hydrate] Error parsing props for component ${s}`, this.getAttribute("props"), r), r } let u; await this.hydrator(this)(this.Component, p, n, { client: this.getAttribute("client") }), this.removeAttribute("ssr"), this.dispatchEvent(new CustomEvent("astro:hydrate")) }); d(this, "unmount", () => { this.isConnected || this.dispatchEvent(new CustomEvent("astro:unmount")) }) } disconnectedCallback() { document.removeEventListener("astro:after-swap", this.unmount), document.addEventListener("astro:after-swap", this.unmount, { once: !0 }) } connectedCallback() { if (!this.hasAttribute("await-children") || document.readyState === "interactive" || document.readyState === "complete") this.childrenConnectedCallback(); else { let e = () => { document.removeEventListener("DOMContentLoaded", e), c.disconnect(), this.childrenConnectedCallback() }, c = new MutationObserver(() => { var n; ((n = this.lastChild) == null ? void 0 : n.nodeType) === Node.COMMENT_NODE && this.lastChild.nodeValue === "astro:end" && (this.lastChild.remove(), e()) }); c.observe(this, { childList: !0 }), document.addEventListener("DOMContentLoaded", e) } } async childrenConnectedCallback() { let e = this.getAttribute("before-hydration-url"); e && await import(e), this.start() } async start() { let e = JSON.parse(this.getAttribute("opts")), c = this.getAttribute("client"); if (Astro[c] === void 0) { window.addEventListener(`astro:${c}`, () => this.start(), { once: !0 }); return } try { await Astro[c](async () => { let n = this.getAttribute("renderer-url"), [h, { default: p }] = await Promise.all([import(this.getAttribute("component-url")), n ? import(n) : () => () => { }]), u = this.getAttribute("component-export") || "default"; if (!u.includes(".")) this.Component = h[u]; else { this.Component = h; for (let f of u.split(".")) this.Component = this.Component[f] } return this.hydrator = p, this.hydrate }, e, this) } catch (n) { console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`, n) } } attributeChangedCallback() { this.hydrate() } } d(y, "observedAttributes", ["props"]), customElements.get("astro-island") || customElements.define("astro-island", y) } })();</script>
  <script>window._$HY || (e => { let t = e => e && e.hasAttribute && (e.hasAttribute("data-hk") ? e : t(e.host && e.host.nodeType ? e.host : e.parentNode));["click", "input"].forEach((o => document.addEventListener(o, (o => { if (!e.events) return; let s = t(o.composedPath && o.composedPath()[0] || o.target); s && !e.completed.has(s) && e.events.push([s, o]) })))) })(_$HY = { events: [], completed: new WeakSet, r: {}, fe() { } });</script>
  <!--xs--><astro-island uid="ZeT78n" data-solid-render-id="s1" component-url="/_astro/Navbar.xVaj7tQm.js"
    component-export="Navbar" renderer-url="/_astro/client.C-0b9Jot.js"
    props="{&quot;links&quot;:[0,{&quot;home&quot;:[0,{&quot;href&quot;:[0,&quot;/&quot;],&quot;title&quot;:[0,&quot;Home&quot;]}],&quot;getStarted&quot;:[0,{&quot;href&quot;:[0,&quot;/get-started&quot;],&quot;title&quot;:[0,&quot;Get Started&quot;],&quot;description&quot;:[0,&quot;Begin your journey&quot;]}],&quot;learn&quot;:[0,{&quot;href&quot;:[0,&quot;/learn&quot;],&quot;title&quot;:[0,&quot;Learn&quot;]}],&quot;blog&quot;:[0,{&quot;href&quot;:[0,&quot;/blog&quot;],&quot;title&quot;:[0,&quot;Blog&quot;]}],&quot;benchmarks&quot;:[0,{&quot;href&quot;:[0,&quot;/benchmarks/community-benchmarks&quot;],&quot;title&quot;:[0,&quot;Benchmarks&quot;]}]}]}"
    ssr="" client="load" opts="{&quot;name&quot;:&quot;Navbar&quot;,&quot;value&quot;:true}" await-children="">
    <nav data-hk="s10000"
      class="fixed left-1/2 top-[10px] z-30 h-[70px] w-[90%] max-w-[1200px] -translate-x-1/2 rounded-[100px] border border-white/10 bg-[#1E212A99] px-[30px] py-[20px] text-center shadow-[0px_10px_15px_3px_#5865F21A] backdrop-blur-2xl">
      <div class="flex flex-1 items-center justify-between gap-4">
        <div class="flex">
          <details class="group block hover:cursor-pointer lg:invisible lg:hidden" id="navbar-menu">
            <summary class="list-none px-2"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 20 21"
                class="inline-block h-7 w-8 stroke-current group-open:hidden">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16">
                </path>
              </svg></summary>
            <div
              class="absolute left-[-10%] top-[-16px] h-[calc(100vh+16px)] w-[110vw] bg-[#1E212A99]/95 backdrop-blur-2xl">
              <div class="pt-10 pl-[calc(10%+30px)]"><svg xmlns="http://www.w3.org/2000/svg" fill="none"
                  viewbox="0 0 24 24" class="h-7 w-8 stroke-current group-open:block">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path>
                </svg></div>
              <ul class="mt-6 flex-col justify-evenly whitespace-nowrap">
                <li data-hk="s100010" class="border-white p-4 text-3xl"><a data-hk="s1000110" title="Home"
                    href="../../index.htm" aria-description=""
                    class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Home</a>
                </li>
                <li data-hk="s100012" class="border-white p-4 text-3xl"><a data-hk="s1000130" title="Get Started"
                    href="../../get-started/index.htm" aria-description="Begin your journey"
                    class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Get
                    Started</a></li>
                <li data-hk="s100014" class="border-white p-4 text-3xl"><a data-hk="s1000150" title="Learn"
                    href="../../learn/index.htm" aria-description=""
                    class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Learn</a>
                </li>
                <li data-hk="s100016" class="border-white p-4 text-3xl"><a data-hk="s1000170" title="Blog"
                    href="../index.htm" aria-description=""
                    class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Blog</a>
                </li>
                <li data-hk="s100018" class="border-white p-4 text-3xl"><a data-hk="s1000190" title="Benchmarks"
                    href="../../benchmarks/community-benchmarks/index.htm" aria-description=""
                    class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Benchmarks</a>
                </li>
              </ul>
            </div>
          </details>
          <div class="hidden lg:block"><astro-slot name="left">
              <div> <img src="../../_astro/burn-flame-white.D0NuVaYR_Z1ywDjw.svg" alt="Burn Logo" loading="lazy"
                  decoding="async" fetchpriority="auto" width="95" height="30" class="mx-2 hidden sm:block"> </div>
            </astro-slot></div>
        </div>
        <div class="block lg:hidden"><astro-slot name="center"></astro-slot></div>
        <ul class="hidden max-w-[610px] flex-wrap justify-evenly whitespace-nowrap text-center align-middle lg:flex">
          <li data-hk="s100020"><a data-hk="s1000210" title="Home" href="../../index.htm" aria-description=""
              class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Home</a>
          </li>
          <li data-hk="s100022"><a data-hk="s1000230" title="Get Started" href="../../get-started/index.htm"
              aria-description="Begin your journey"
              class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Get
              Started</a></li>
          <li data-hk="s100024"><a data-hk="s1000250" title="Learn" href="../../learn/index.htm" aria-description=""
              class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Learn</a>
          </li>
          <li data-hk="s100026"><a data-hk="s1000270" title="Blog" href="../index.htm" aria-description=""
              class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Blog</a>
          </li>
          <li data-hk="s100028"><a data-hk="s1000290" title="Benchmarks"
              href="../../benchmarks/community-benchmarks/index.htm" aria-description=""
              class="h-full px-3 p-1 text-btn primary-button font-bold text-light bg-transparent hover:bg-transparent hover:text-white block rounded-[5px] text-center  ">Benchmarks</a>
          </li>
        </ul><!--$--><astro-slot name="right">
          <div class="flex flex-row gap-2 pr-3"> <a href="https://github.com/tracel-ai/burn" class="h-[30px]"> <svg
                fill="currentColor" width="30" height="30" viewbox="0 0 16 16">
                <path
                  d="M8 0a8 8 0 0 0-2.5 15.6c.4 0 .5-.2.5-.4v-1.5c-2 .4-2.5-.5-2.7-1 0-.1-.5-.9-.8-1-.3-.2-.7-.6 0-.6.6 0 1 .6 1.2.8.7 1.2 1.9 1 2.4.7 0-.5.2-.9.5-1-1.8-.3-3.7-1-3.7-4 0-.9.3-1.6.8-2.2 0-.2-.3-1 .1-2 0 0 .7-.3 2.2.7a7.4 7.4 0 0 1 4 0c1.5-1 2.2-.8 2.2-.8.5 1.1.2 2 .1 2.1.5.6.8 1.3.8 2.2 0 3-1.9 3.7-3.6 4 .3.2.5.7.5 1.4v2.2c0 .2.1.5.5.4A8 8 0 0 0 16 8a8 8 0 0 0-8-8">
                </path>
              </svg> </a> <a href="https://x.com/yourprofile" target="_blank" aria-label="X (Twitter)">
  <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 120 120" fill="none">
    <rect width="120" height="120" rx="24" fill="black"/>
    <path d="M81.6 31.2H68.88L58.08 45.36 47.28 31.2H34.56l17.28 22.8L34 88.8h12.72l10.56-13.92 10.56 13.92h12.72L62.4 54l19.2-22.8zM66.96 75.12l-8.88-11.76-8.88 11.76h-3.12L58.08 58.8l11.04 16.32h-2.16z" fill="white"/>
  </svg>
</a>
</div>
        </astro-slot><!--/-->
      </div>
    </nav><!--astro:end-->
  </astro-island>
  <main>
    <div class="flex w-full justify-center pt-20">
      <div class="mx-3 mb-10 w-full max-w-5xl">
        <div class="mb-3">
          <p class="px-2 text-xl font-semibold"><a href="../../index.htm"
              class="hover:text-[#edc567]">home</a><span><span> · </span><a href="../index.htm"
                class="hover:text-[#edc567]">blog</a></span><span><span> · </span><a href="index.htm"
                class="hover:text-[#edc567]">mabor-rusty-approach-to-tensor-handling</a></span></p>
        </div>
        <article class="blog rounded-lg bg-white/5 pt-4">
          <div>
            <h1 class="!text-[30px] sm:!text-[48px] font-bold !leading-normal px-3 sm:px-8">Reduced Memory Usage:
              Mabor's Rusty Approach to Tensor Handling</h1>
            <div class="px-3 pb-4 sm:px-8"><img class="mr-3 h-48 w-full rounded-lg object-cover object-top"
                src="../../_astro/blog2.DuTkwZtR_Savc1.webp" alt="Space digital art generated by stable diffusion.">
            </div>
            <div class="flex px-3 sm:px-8">
              <div class="flex">
                <div class="i-mdi-clipboard-text-clock size-5"></div><span class="px-2 font-normal">Tue, Mar 21,
                  2023</span>
              </div><!--$--><a class="flex pl-2" href="https://x.com/nath_simard" target="_blank">
                <div class="i-mdi-account-edit size-5"></div><span class="px-2 font-normal">Nathaniel Simard</span>
              </a><!--/-->
            </div>
          </div>
          <div class="px-3 pb-4 sm:px-8 text-base sm:text-lg leading-relaxed">
            <div class="my-6 border-t-2 border-[#181a1d]"></div><!--$-->
            <div>
              <h2>Introduction</h2>
              <p>
                The latest release of Mabor
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-1">1</a><!--/-->]</span> includes significant
                changes to its memory management strategy. One of the most notable changes
                is that tensor-allocated memory can now be reused way more often. This
                is a big improvement, as every operation was previously implemented using
                read-only tensor references, which often resulted in unnecessary memory
                allocations. Overall, these changes significantly reduced memory usage,
                especially on the CPU compared to PyTorch.
              </p>
              <p>
                The new approach motivated a complete rewrite of the auto
                differentiation engine of Mabor, which allows any backend to support
                backpropagation. The previous implementation relied on object-oriented
                programming patterns, resulting in excessive indirections and memory
                allocations. Moreover, the engine was read-only, limiting its ability
                to free up unused tensors or reuse them when possible during the
                backward pass . The new implementation addresses these limitations by
                adopting more efficient rusty patterns, leveraging the updated backend
                API, and enabling in-place operations.
              </p>
              <p>
                This is mostly a technical blog. If you're not that interested in
                memory management, you can just skip to the <a class="underline" href="#benchmarks"> Benchmarks </a>
                section.
              </p>
              <h2>In-place Operations</h2>
              <p>
                How does Mabor enable the reuse of tensor-allocated memory? One way is
                through a pattern similar to copy-on-write
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-2">2</a><!--/-->]</span>, where data can only be
                safely written when there is a single reference pointing to it. If
                there are multiple references, the original data is copied before
                being written, hence the name copy-on-write. The distinction in how
                Mabor reuses tensor-allocated memory is as follows: instead of copying
                data when there are multiple references using a tensor, the normal
                (non in-place) operation is used instead. Here's an example of how the
                log function is implemented using the tch backend
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-3">3</a><!--/-->]</span> (bindings to LibTorch).
              </p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="rust"><code><span class="line"></span>
<span class="line"><span style="color:#F78C6C">  fn</span><span style="color:#82AAFF"> log</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#C792EA">const</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">>(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">E</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">>)</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">E</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#EEFFFF">      tensor</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">unary_ops</span><span style="color:#89DDFF">(</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // When the tensor is safe to mutate in-place.</span></span>
<span class="line"><span style="color:#89DDFF">          |</span><span style="color:#C792EA">mut</span><span style="color:#EEFFFF"> tensor</span><span style="color:#89DDFF">|</span><span style="color:#EEFFFF"> tensor</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">log_</span><span style="color:#89DDFF">(),</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // When the tensor is not safe to mutate in-place.</span></span>
<span class="line"><span style="color:#89DDFF">          |</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">|</span><span style="color:#EEFFFF"> tensor</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">log</span><span style="color:#89DDFF">(),</span></span>
<span class="line"><span style="color:#89DDFF">      )</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>
                In this example, we call the in-place log function when possible, and
                the normal function otherwise. Note that all functions ending with an
                underscore are in-place operations with PyTorch/LibTorch
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-4">4</a><!--/-->]</span>. The unary_ops
                operation abstracts how the number of references is counted, using
                Atomic Reference Counting (Arc) provided by the standard library
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-5">5</a><!--/-->]</span>. This behavior is also
                available with the alloc crate when working with no_std, which is
                useful for environments where there is no operating system available
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]" href="#reference-6">6,</a><a
                    class="hover:text-[#69b8e1]" href="#reference-7">7</a><!--/-->]</span>. The
                non-thread-safe variant, Reference Counting (Rc), also provides the
                same functionality
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-8">8</a><!--/-->]</span>. Normally, this pattern
                should be used with try_unwrap and get_mut functions provided by Arc
                and Rc, which safely return a mutable or an owned value of the inner
                type
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]" href="#reference-9">9,</a><a
                    class="hover:text-[#69b8e1]" href="#reference-10">10</a><!--/-->]</span>. However,
                the PyTorch bindings are not really safe. You can call mutable
                operations on any tensor handle, even read-only references, by just
                doing a shallow copy. Hence, we track the memory location of each
                tensor's storage manually instead.
              </p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="rust"><code><span class="line"></span>
<span class="line"><span style="color:#F78C6C">  pub</span><span style="color:#F78C6C"> fn</span><span style="color:#82AAFF"> unary_ops</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">FOwn</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> FRef</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> EOut</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">kind</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Element</span><span style="color:#89DDFF">,</span><span style="color:#C792EA"> const</span><span style="color:#FFCB6B"> D_OUT</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">>(</span></span>
<span class="line"><span style="color:#EEFFFF">      self</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#EEFFFF">      fown</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> FOwn</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#EEFFFF">      fref</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> FRef</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#89DDFF">  )</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">EOut</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D_OUT</span><span style="color:#89DDFF">></span></span>
<span class="line"><span style="color:#F78C6C">  where</span></span>
<span class="line"><span style="color:#FFCB6B">      FOwn</span><span style="color:#89DDFF">:</span><span style="color:#82AAFF"> Fn</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Tensor</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Tensor</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#FFCB6B">      FRef</span><span style="color:#89DDFF">:</span><span style="color:#82AAFF"> Fn</span><span style="color:#89DDFF">(&#x26;</span><span style="color:#FFCB6B">tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Tensor</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Tensor</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#89DDFF">  {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">      // We check if there are multiple tensors pointing the the same storage</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">      if</span><span style="color:#FFCB6B"> Arc</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">strong_count</span><span style="color:#89DDFF">(&#x26;</span><span style="color:#EEFFFF">self</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">storage</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> ></span><span style="color:#F78C6C"> 1</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // If this is the case, the non-in-place function is called</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">          return</span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">from_existing</span><span style="color:#89DDFF">(</span><span style="color:#82AAFF">fref</span><span style="color:#89DDFF">(&#x26;</span><span style="color:#EEFFFF">self</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">),</span><span style="color:#EEFFFF"> self</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">storage</span><span style="color:#89DDFF">);</span></span>
<span class="line"><span style="color:#89DDFF">      }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#545454;font-style:italic">      // Only the current tensor is pointing to the provided storage space</span></span>
<span class="line"><span style="color:#545454;font-style:italic">      // Since the tensor will never be reused, we can safely call the owned</span></span>
<span class="line"><span style="color:#545454;font-style:italic">      // function, which may dispatch to an in-place operation.</span></span>
<span class="line"><span style="color:#FFCB6B">      TchTensor</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">from_existing</span><span style="color:#89DDFF">(</span><span style="color:#82AAFF">fown</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">self</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">),</span><span style="color:#EEFFFF"> self</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">storage</span><span style="color:#89DDFF">)</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>
                This strategy can also be used for binary operations, enabling other
                kinds of optimizations. In some cases, you may call another LibTorch
                function depending on the number of references pointing to the input
                tensors, in order to reuse as much tensor-allocated memory as
                possible. Here's an example with the lower implementation.
              </p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="rust"><code><span class="line"></span>
<span class="line"><span style="color:#F78C6C">  pub</span><span style="color:#F78C6C"> fn</span><span style="color:#82AAFF"> lower</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#C792EA">const</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">>(</span></span>
<span class="line"><span style="color:#EEFFFF">      lhs</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">E</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">>,</span></span>
<span class="line"><span style="color:#EEFFFF">      rhs</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">E</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span></span>
<span class="line"><span style="color:#89DDFF">  )</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> TchTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">bool</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#FFCB6B">      TchTensor</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">binary_ops_tensor</span><span style="color:#89DDFF">(</span></span>
<span class="line"><span style="color:#EEFFFF">          lhs</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#EEFFFF">          rhs</span><span style="color:#89DDFF">,</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // When the lhs tensor is safe to mutate.</span></span>
<span class="line"><span style="color:#89DDFF">          |</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> rhs</span><span style="color:#89DDFF">|</span><span style="color:#EEFFFF"> lhs</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">less_tensor_</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">rhs</span><span style="color:#89DDFF">).</span><span style="color:#82AAFF">to_kind</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Kind</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Bool</span><span style="color:#89DDFF">),</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // When the rhs tensor is safe to mutate, but not lhs.</span></span>
<span class="line"><span style="color:#89DDFF">          |</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> rhs</span><span style="color:#89DDFF">|</span><span style="color:#EEFFFF"> rhs</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">greater_tensor_</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">).</span><span style="color:#82AAFF">to_kind</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">tch</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Kind</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Bool</span><span style="color:#89DDFF">),</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // When both tensor are not safe to mutate</span></span>
<span class="line"><span style="color:#89DDFF">          |</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> rhs</span><span style="color:#89DDFF">|</span><span style="color:#EEFFFF"> lhs</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">less_tensor</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">rhs</span><span style="color:#89DDFF">),</span></span>
<span class="line"><span style="color:#89DDFF">      )</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>
                In this case, when it is safe to mutate the left-hand side (LHS)
                tensor, we call the in-place operation to reuse its data. However,
                when there is at least one other reference to that tensor, it is not
                safe to mutate. In this case, we could still reuse tensor-allocated
                memory by calling the in-place greater operation on the right-hand
                side (RHS) tensor instead. This produces the same output but is more
                efficient. Note that this assumes that boolean tensors and float
                tensors can reuse the same memory space, which may depend on the float
                data type.
              </p>
              <h2>Tensor API</h2>
              <p>
                Unfortunately, this pattern was previously impossible to integrate
                with Mabor because all operations received references to tensors as
                arguments and not owned tensors, which doesn't increase the strong
                reference count.
              </p>
              <p>
                To allow backends to use this pattern, the API has been updated to
                receive owned tensors as parameters, with some nice quality-of-life
                improvements as well. The consequence is that tensors are cloned
                exactly the number of times they are reused. This makes it easy for
                users to optimize their code by removing unnecessary cloning when it's
                not required. Note that clippy
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-11">11</a><!--/-->]</span> normally checks for unnecessary
                cloning, but is not perfect. You might change the order in which you do
                your operations to reduce the amount of cloning, something that clippy
                can't check.
              </p>
              <p>
                There is no API in Mabor to call in-place operations directly on
                tensors. If the backend supports it, every time it is possible,
                in-place operations will be used. This is a major quality-of-life
                improvement, since the places where in-place operations can be used
                differ between training and inference, but Mabor aims to provide the
                most optimized code for both use cases. Let's reuse the log function
                as an example.
              </p>
              <p>
                During inference, a temporary tensor, which is not a model parameter,
                is used one time with the log operation, which will reuse the
                tensor-allocated memory since the tensor was never cloned. However,
                during training, the backward step of the log function needs the input
                tensor to calculate its derivative, so the input tensor is cloned
                during the forward pass. Therefore, the input tensor will be left
                unchanged during the forward pass, but may be reused during the
                backward pass.
              </p>
              <h2>Autodiff</h2>
              <p>
                The method for calculating gradients with Mabor was highly inefficient
                but offered significant flexibility. After learning from the first
                implementation, I decided to rewrite it from scratch. Although it
                seemed like a daunting task at first, I was able to complete it in
                less than a week without any breaking changes to the API. Mabor is
                designed to allow for this kind of refactoring and continuous
                improvement of performance and architecture. The primary goal was to
                reduce unnecessary cloning and memory allocations while simplifying
                the complex and difficult-to-understand code patterns. Additionally,
                it was important to make it easy to support new operations of any
                kind, which presented challenges in terms of flexibility, simplicity,
                and minimizing repetitive code. To demonstrate this, consider the
                implementation of the cosine function.
              </p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="rust"><code><span class="line"></span>
<span class="line"><span style="color:#F78C6C">  fn</span><span style="color:#82AAFF"> cos</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#C792EA">const</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">>(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> ADTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">>)</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> ADTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">  		// Define a struct for static dispatch</span></span>
<span class="line"><span style="color:#89DDFF">      #[</span><span style="color:#EEFFFF">derive</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">Debug</span><span style="color:#89DDFF">)]</span></span>
<span class="line"><span style="color:#C792EA">  		struct</span><span style="color:#FFCB6B"> Cos</span><span style="color:#89DDFF">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F78C6C">  		impl</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Backend</span><span style="color:#89DDFF">,</span><span style="color:#C792EA"> const</span><span style="color:#EEFFFF"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">></span><span style="color:#FFCB6B"> Backward</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> 1</span><span style="color:#89DDFF">></span><span style="color:#89DDFF;font-style:italic"> for</span><span style="color:#FFCB6B"> Cos</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // Define the state to capture during the foward pass</span></span>
<span class="line"><span style="color:#C792EA">          type</span><span style="color:#FFCB6B"> State</span><span style="color:#89DDFF"> =</span><span style="color:#FFCB6B"> B</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">TensorPrimitive</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">D</span><span style="color:#89DDFF">>;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#545454;font-style:italic">          // Code that is executed during the backward pass</span></span>
<span class="line"><span style="color:#F78C6C">          fn</span><span style="color:#82AAFF"> backward</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">self</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> ops</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Ops</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#EEFFFF">Self</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">State</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> 1</span><span style="color:#89DDFF">>,</span><span style="color:#EEFFFF"> grads</span><span style="color:#89DDFF">:</span><span style="color:#89DDFF"> &#x26;</span><span style="color:#C792EA">mut</span><span style="color:#FFCB6B"> Gradients</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#C792EA">              let</span><span style="color:#EEFFFF"> input </span><span style="color:#89DDFF">=</span><span style="color:#EEFFFF"> ops</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">state</span><span style="color:#89DDFF">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#545454;font-style:italic">              // Calculate the derivative with respect to its parent</span></span>
<span class="line"><span style="color:#82AAFF">              unary</span><span style="color:#89DDFF">::&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> _</span><span style="color:#89DDFF">>(</span><span style="color:#EEFFFF">ops</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">parents</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> ops</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">node</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> grads</span><span style="color:#89DDFF">,</span><span style="color:#89DDFF"> |</span><span style="color:#EEFFFF">grad</span><span style="color:#89DDFF">|</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#C792EA">                  let</span><span style="color:#EEFFFF"> value </span><span style="color:#89DDFF">=</span><span style="color:#FFCB6B"> B</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">neg</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">sin</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">input</span><span style="color:#89DDFF">));</span></span>
<span class="line"><span style="color:#FFCB6B">                  B</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">mul</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">grad</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> value</span><span style="color:#89DDFF">)</span></span>
<span class="line"><span style="color:#89DDFF">              });</span></span>
<span class="line"><span style="color:#89DDFF">          }</span></span>
<span class="line"><span style="color:#89DDFF">      }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#545454;font-style:italic">      // Prepare a statefull operation</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">      match</span><span style="color:#FFCB6B"> Cos</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">prepare</span><span style="color:#89DDFF">([</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">node</span><span style="color:#89DDFF">],</span><span style="color:#89DDFF"> [</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">graph</span><span style="color:#89DDFF">]).</span><span style="color:#82AAFF">statefull</span><span style="color:#89DDFF">()</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // Executes when the tensor is tracked</span></span>
<span class="line"><span style="color:#FFCB6B">          OpsKind</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">Tracked</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">prep</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> =></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">              // Finish the preparation capturing the state</span></span>
<span class="line"><span style="color:#545454;font-style:italic">              // The input tensor is cloned for the backward pass</span></span>
<span class="line"><span style="color:#EEFFFF">              prep</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">finish</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">primitive</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">clone</span><span style="color:#89DDFF">(),</span><span style="color:#FFCB6B"> B</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">cos</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">primitive</span><span style="color:#89DDFF">))</span></span>
<span class="line"><span style="color:#89DDFF">          }</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // Executes when the tensor is not part of the autodiff graph</span></span>
<span class="line"><span style="color:#545454;font-style:italic">          // The cos operation is called without any cloning</span></span>
<span class="line"><span style="color:#FFCB6B">          OpsKind</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">UnTracked</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">prep</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> =></span><span style="color:#EEFFFF"> prep</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">finish</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">cos</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">primitive</span><span style="color:#89DDFF">)),</span></span>
<span class="line"><span style="color:#89DDFF">      }</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>
                The cosine function definition is the same as any backend since
                gradients are calculated using a backend decorator. The implementation
                uses static dispatch via a zero-sized struct named Cos, which
                implements the Backward trait. During the backward pass, the
                derivative with respect to the parent node is calculated using the
                chain rule of differentiation. The function supports both tracked and
                untracked operations, with the former requiring cloning of the input
                tensor for use in the backward pass. However, sometimes operations
                don't require any state during the backward pass. Let's see how it's
                done with the scalar addition function.
              </p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="rust"><code><span class="line"></span>
<span class="line"><span style="color:#F78C6C">  fn</span><span style="color:#82AAFF"> add_scalar</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#C792EA">const</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">>(</span></span>
<span class="line"><span style="color:#EEFFFF">  		lhs</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> ADTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">>,</span></span>
<span class="line"><span style="color:#EEFFFF">  		rhs</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> FloatElem</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">>,</span></span>
<span class="line"><span style="color:#89DDFF">  )</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> ADTensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#545454;font-style:italic">  		// Define a struct for static dispatch</span></span>
<span class="line"><span style="color:#89DDFF">  		#[</span><span style="color:#EEFFFF">derive</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">Debug</span><span style="color:#89DDFF">)]</span></span>
<span class="line"><span style="color:#C792EA">  		struct</span><span style="color:#FFCB6B"> AddScalar</span><span style="color:#89DDFF">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F78C6C">  		impl</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Backend</span><span style="color:#89DDFF">,</span><span style="color:#C792EA"> const</span><span style="color:#EEFFFF"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">></span><span style="color:#FFCB6B"> Backward</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> 1</span><span style="color:#89DDFF">></span><span style="color:#89DDFF;font-style:italic"> for</span><span style="color:#FFCB6B"> AddScalar</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#C792EA">          type</span><span style="color:#FFCB6B"> State</span><span style="color:#89DDFF"> =</span><span style="color:#89DDFF"> ();</span></span>
<span class="line"></span>
<span class="line"><span style="color:#545454;font-style:italic">          // Code that is executed during the backward pass</span></span>
<span class="line"><span style="color:#F78C6C">          fn</span><span style="color:#82AAFF"> backward</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">self</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> ops</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Ops</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#EEFFFF">Self</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">State</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> 1</span><span style="color:#89DDFF">>,</span><span style="color:#EEFFFF"> grads</span><span style="color:#89DDFF">:</span><span style="color:#89DDFF"> &#x26;</span><span style="color:#C792EA">mut</span><span style="color:#FFCB6B"> Gradients</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#82AAFF">              unary</span><span style="color:#89DDFF">::&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> _</span><span style="color:#89DDFF">>(</span><span style="color:#EEFFFF">ops</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">parents</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> ops</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">node</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> grads</span><span style="color:#89DDFF">,</span><span style="color:#89DDFF"> |</span><span style="color:#EEFFFF">grad</span><span style="color:#89DDFF">|</span><span style="color:#EEFFFF"> grad</span><span style="color:#89DDFF">);</span></span>
<span class="line"><span style="color:#89DDFF">          }</span></span>
<span class="line"><span style="color:#89DDFF">      }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#545454;font-style:italic">      // Simpler definition where no match are required.</span></span>
<span class="line"><span style="color:#FFCB6B">      AddScalar</span></span>
<span class="line"><span style="color:#89DDFF">          .</span><span style="color:#82AAFF">prepare</span><span style="color:#89DDFF">([</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">node</span><span style="color:#89DDFF">],</span><span style="color:#89DDFF"> [</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">graph</span><span style="color:#89DDFF">])</span></span>
<span class="line"><span style="color:#89DDFF">          .</span><span style="color:#82AAFF">stateless</span><span style="color:#89DDFF">(</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">::</span><span style="color:#82AAFF">add_scalar</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">lhs</span><span style="color:#89DDFF">.</span><span style="color:#EEFFFF">primitive</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> rhs</span><span style="color:#89DDFF">))</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>
                Similar to the cosine function, the scalar addition function also
                supports tracked and untracked operations. However, since scalar
                addition doesn't require any state during the backward pass, the
                implementation is simpler, and no match statements are required.
              </p>
              <h2 id="benchmarks">Benchmarks</h2>
              <p>
                Even though the last release was focused on structural refactors to
                allow for more optimizations and control from backend implementations,
                it's still interesting to see how it compares to other frameworks. So
                let's compare it to PyTorch for simple use cases.
              </p>
              <h3>Disclaimer</h3>
              <p>
                It's important to note that Mabor doesn't support fused operations,
                even for popular activation functions like softmax, gelu, and sigmoid.
                Furthermore, all the derivatives of each primitive operation that are
                calculated during the backward pass also use primitive operations and
                are not fused. This lack of operation fusion has a significant impact
                on real-world performance. As a result, PyTorch is likely to be faster
                for common models, at least on the GPU, where the impact of operation
                fusion is more pronounced
              </p>
              <p>
                Mabor's development direction is likely to differ from other
                frameworks. It's unfortunate that writing mathematical operations in a
                more declarative way can be less performant than using a high-level
                function with a highly optimized kernel implementation. Ideally, Mabor
                should be able to detect when such a kernel exists and use it
                automatically, without requiring any changes to the code. This is the
                kind of developer experience that Mabor aims to provide: enabling users
                to write mathematical operations using primitives while allowing
                backend developers to declare graphs of operations that can be fused
                for optimal performance. Additionally, Mabor should allow users to
                profile their models, identify which functions take the most time, and
                write optimized kernels for those functions using their preferred
                backend without the need to fork a framework, rewrite the model, or
                change programming language. All of this should be achievable while
                still supporting fully dynamic graphs and custom control flow with an
                eager-like programming model. However, these are significant
                constraints, and it will require a lot of thinking and hard work to
                make this a reality. If you have any comments, suggestions, or
                recommendations regarding fused operations, we invite you to join our <a target="_blank"
                  rel="noreferrer" href="https://discord.gg/uPEBbYYDB6" class="underline">
                  Discord
                </a>
                and share your thoughts.
              </p>
              <p>Now let get into the benchmarks.</p>
              <h3>Softmax</h3>
              <p>
                The first benchmark is a custom implementation of the softmax
                activation function. For numerical stability, we will use an
                implementation that uses log softmax.
              </p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="rust"><code><span class="line"></span>
<span class="line"><span style="color:#FFCB6B">  mabor</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">tensor</span><span style="color:#89DDFF">::{</span><span style="color:#FFCB6B">backend</span><span style="color:#89DDFF">::</span><span style="color:#FFCB6B">Backend</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> Tensor</span><span style="color:#89DDFF">};</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F78C6C">  fn</span><span style="color:#82AAFF"> softmax</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#C792EA">const</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> B</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Backend</span><span style="color:#89DDFF">>(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Tensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">>,</span><span style="color:#EEFFFF"> dim</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> Tensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#82AAFF">      log_softmax</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF"> dim</span><span style="color:#89DDFF">).</span><span style="color:#82AAFF">exp</span><span style="color:#89DDFF">()</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F78C6C">  fn</span><span style="color:#82AAFF"> log_softmax</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#C792EA">const</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> B</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Backend</span><span style="color:#89DDFF">>(</span><span style="color:#EEFFFF">tensor</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> Tensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">>,</span><span style="color:#EEFFFF"> dim</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> usize</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> -></span><span style="color:#FFCB6B"> Tensor</span><span style="color:#89DDFF">&#x3C;</span><span style="color:#FFCB6B">B</span><span style="color:#89DDFF">,</span><span style="color:#FFCB6B"> D</span><span style="color:#89DDFF">></span><span style="color:#89DDFF"> {</span></span>
<span class="line"><span style="color:#EEFFFF">      tensor</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">clone</span><span style="color:#89DDFF">()</span><span style="color:#89DDFF"> -</span><span style="color:#EEFFFF"> tensor</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">exp</span><span style="color:#89DDFF">().</span><span style="color:#82AAFF">sum_dim</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF">dim</span><span style="color:#89DDFF">).</span><span style="color:#82AAFF">log</span><span style="color:#89DDFF">()</span></span>
<span class="line"><span style="color:#89DDFF">  }</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>Now, let’s compare it to the equivalent code in PyTorch.</p>
              <div class="my-[8px] border-2 rounded border-[#212121] font-mono font-medium">
                <pre class="astro-code material-theme-darker"
                  style="background-color:#212121;color:#EEFFFF; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">  import</span><span style="color:#EEFFFF"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">  from</span><span style="color:#EEFFFF"> torch </span><span style="color:#89DDFF;font-style:italic">import</span><span style="color:#EEFFFF"> Tensor</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#C792EA">  def</span><span style="color:#82AAFF"> softmax</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF;font-style:italic">tensor</span><span style="color:#89DDFF">:</span><span style="color:#EEFFFF"> Tensor</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF;font-style:italic"> dim</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> int</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> -></span><span style="color:#EEFFFF"> Tensor</span><span style="color:#89DDFF">:</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">      return</span><span style="color:#82AAFF"> log_softmax</span><span style="color:#89DDFF">(</span><span style="color:#82AAFF">tensor</span><span style="color:#89DDFF">,</span><span style="color:#82AAFF"> dim</span><span style="color:#89DDFF">).</span><span style="color:#82AAFF">exp</span><span style="color:#89DDFF">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C792EA">  def</span><span style="color:#82AAFF"> log_softmax</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF;font-style:italic">tensor</span><span style="color:#89DDFF">:</span><span style="color:#EEFFFF"> Tensor</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF;font-style:italic"> dim</span><span style="color:#89DDFF">:</span><span style="color:#FFCB6B"> int</span><span style="color:#89DDFF">)</span><span style="color:#89DDFF"> -></span><span style="color:#EEFFFF"> Tensor</span><span style="color:#89DDFF">:</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic">      return</span><span style="color:#EEFFFF"> tensor </span><span style="color:#89DDFF">-</span><span style="color:#EEFFFF"> tensor</span><span style="color:#89DDFF">.</span><span style="color:#82AAFF">exp</span><span style="color:#89DDFF">().</span><span style="color:#82AAFF">sum</span><span style="color:#89DDFF">(</span><span style="color:#EEFFFF;font-style:italic">dim</span><span style="color:#89DDFF">=</span><span style="color:#82AAFF">dim</span><span style="color:#89DDFF">,</span><span style="color:#EEFFFF;font-style:italic"> keepdim</span><span style="color:#89DDFF">=True).</span><span style="color:#82AAFF">log</span><span style="color:#89DDFF">()</span></span>
<span class="line"><span style="color:#EEFFFF">  </span></span></code></pre>
              </div>
              <p>
                The main difference in the code is the extra typing in Mabor, which
                specifies the number of dimensions a tensor has and the backend it
                runs on. However, we can soften the notation by moving the generic
                argument declaration into a zero-sized struct, which groups functions
                that operate on tensors of the same type. The other difference is that
                we need an explicit clone in the Mabor version because the tensor is
                reused twice. During inference, we expect `tensor.clone() -
                tensor.exp()` to not be executed in-place since they all use the same
                tensor. However, we expect all other operations to reuse the same
                memory to avoid unnecessary allocations.
              </p>
              <p>
                The tests were performed on my laptop, so they may not be fully
                reliable, but they are still informative in terms of what kind of
                performance we can expect.
              </p>
              <div class="w-full overflow-auto px-6 pb-8">
                <div class="w-full px-6 py-2 text-center text-xl font-bold">Softmax Experiment</div>
                <table class="w-full">
                  <thead>
                    <tr>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div></div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Inference</div>
                          <div>Memory</div>
                        </div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Inference</div>
                          <div>Speed</div>
                        </div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Autodiff</div>
                          <div>Memory</div>
                        </div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Autodiff</div>
                          <div>Speed</div>
                        </div>
                      </th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">CPU - PyTorch</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">586 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">47.39 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">980 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">111.8 ms</span></td>
                      <!--/-->
                    </tr>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">CPU - Mabor</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">353 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">34.25 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">1047 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">146.93 ms</span></td>
                      <!--/-->
                    </tr>
                    <tr class="border border-[#586473]">
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                    </tr>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">GPU - PyTorch</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">852 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">1.474 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">980 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">4.103 ms</span></td>
                      <!--/-->
                    </tr>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">GPU - Mabor</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">756 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">1.479 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">1076 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">5.365 ms</span></td>
                      <!--/-->
                    </tr>
                  </tbody>
                </table><!--$-->
                <div class="w-full px-6 py-2 text-center font-normal font-serif">Note that inference benchmarks were
                  executed 100 times on CPU and 10000 times on GPU, while the autodiff benchmarks were executed 100
                  times on CPU and 1000 times on GPU.</div><!--/-->
              </div>
              <p>
                An interesting takeaway here is that Mabor seems to be much faster
                during inference on the CPU, while pretty comparable on the GPU. In
                all cases, it seems to take less memory, with a bigger difference on
                the CPU. This may be because PyTorch has taken great care of their GPU
                implementation and may have something similar to a memory pool or
                other tricks to handle GPU memory more efficiently.
              </p>
              <p>
                We also see PyTorch appears to be faster and requires less memory to
                compute gradients. This could be due to their backward implementation
                of each tensor operation executing fewer kernels. In the case of Mabor,
                the logarithm backward implementation uses two kernels, and the sum
                backward allocates a new tensor while using two other kernels. To test
                that hypothesis, we will have to do another set of benchmarks where we
                reduce the difference in the number of kernels executed by both
                frameworks.
              </p>
              <h3>Multi layer perceptron (MLP)</h3>
              <p>
                The second set of benchmarks is pretty simple. We are going to compare
                a simple Multi-layer perceptron implementation using a simple linear
                layer with the ReLU activation function. Even if the ReLU backward
                implementation of Mabor uses two kernels, the difference is smaller
                than in the previous experiment, and we should see a smaller
                difference in execution time when calculating gradients.
              </p>
              <div class="w-full overflow-auto px-6 pb-8">
                <div class="w-full px-6 py-2 text-center text-xl font-bold">MLP Experiment</div>
                <table class="w-full">
                  <thead>
                    <tr>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div></div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Inference</div>
                          <div>Memory</div>
                        </div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Inference</div>
                          <div>Speed</div>
                        </div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Autodiff</div>
                          <div>Memory</div>
                        </div>
                      </th>
                      <th class="border border-[#586473] bg-[#1f2835] p-2 text-center">
                        <div>
                          <div>Autodiff</div>
                          <div>Speed</div>
                        </div>
                      </th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">CPU - PyTorch</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">433 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">22.765 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">708 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">76.85 ms</span></td>
                      <!--/-->
                    </tr>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">CPU - Mabor</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">385 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">22.695 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">576 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span>80.429 ms</span></td><!--/-->
                    </tr>
                    <tr class="border border-[#586473]">
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                      <td class="bg-[#1f2835] p-[6px] "></td>
                    </tr>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">GPU - PyTorch</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">1190 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">0.8474 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">1204 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">3.2708 ms</span></td>
                      <!--/-->
                    </tr>
                    <tr><!--$-->
                      <td class="border border-[#586473] p-2 text-center">GPU - Mabor</td><!--/--><!--$-->
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">1096 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">0.8042 ms</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-thin">1222 M</span></td>
                      <td class="border border-[#586473] p-2 text-center"><span class="font-bold">2.4874 ms</span></td>
                      <!--/-->
                    </tr>
                  </tbody>
                </table><!--$-->
                <div class="w-full px-6 py-2 text-center font-normal font-serif">Note that inference benchmarks were
                  executed 100 times on CPU and 5000 times on GPU, while the autodiff benchmarks were executed 200 times
                  on CPU and 5000 times on GPU.</div><!--/-->
              </div>
              <p>
                The results are pretty much what I expected, except for the fact that
                Mabor is considerably faster on the GPU when computing gradients. This
                could be explained by how the ReLU backward is implemented in Mabor,
                which would be faster in that specific use case. However, I would not
                conclude that Mabor is faster in training MLP, and I would be really
                careful in coming up with conclusions; it might just be an outlier
                data point.
              </p>
              <p>
                For the other data points, we see that Mabor and PyTorch are generally
                similar in execution time, but Mabor uses less memory, especially on
                the CPU. I didn't include the MLP implementation of both frameworks,
                but you can have a look at it on the <a target="_blank" rel="noreferrer"
                  href="https://github.com/nathanielsimard/burn-memory-experiments" class="underline">
                  repository
                </a>
                .
              </p>
              <h2>Conclusion</h2>
              <p>
                So this is the end of this blog post. I presented how Mabor leverages
                Rust’s ownership tracking to safely reuse tensor-allocated memory when
                possible. I presented the necessary changes to the tensor API and the
                autodiff backend to leverage owned tensor to reduce memory usage. Some
                small benchmarks were made to validate the effectiveness of that
                strategy, which showed consistent reduced memory usage, especially on
                the CPU. However, we also saw the necessity of operation fusion to
                really speed up computation, and it may explain why it’s a major focus
                of PyTorch 2.0
                <span class="reference px-1">[<!--$--><a class="hover:text-[#69b8e1]"
                    href="#reference-12">12</a><!--/-->]</span> with their new graph compilation
                capabilities. The next phase of Mabor will be on stabilizing the API, improving
                the docs, and making the project easier to use overall. After that, it
                will be crucial to focus on operation fusion and come up with a strategy
                that respects all the previously mentioned wishes.
              </p>
              <p>
                Note that this is not a full report of what has been accomplished
                since the last release. A lot of work has been done by contributors,
                and Mabor can now be compiled to Web Assembly for inference, which runs
                natively on browsers on the client side. You can test it yourself with
                the online <a class="underline" href="/demo"> demo </a>
                . I want to thank everybody that got involved with the project, I received
                so much constructive feedback that has or will definitively improve Mabor.
                It's also always interesting to see what kind of project, research, or
                product Mabor can help you with, so don't hesitate to reach out if you find
                value in what we are building.
              </p>
            </div> <!--/--><!--$-->
            <div>
              <h2>References</h2><!--$-->
              <div>
                <div id="reference-1"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold"
                      href="https://github.com/tracel-ai/burn/releases/tag/v0.6.0" target="_blank">1</a>]</span><span
                    class="font-light font-serif">Mabor v0.6.0 release notes</span></div>
                <div id="reference-2"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://en.wikipedia.org/wiki/Copy-on-write"
                      target="_blank">2</a>]</span><span class="font-light font-serif">Wikipedia: Copy-on-write</span>
                </div>
                <div id="reference-3"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://github.com/tracel-ai/burn/tree/main/burn-tch"
                      target="_blank">3</a>]</span><span class="font-light font-serif">Mabor - Tch Backend</span></div>
                <div id="reference-4"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold"
                      href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#operations"
                      target="_blank">4</a>]</span><span class="font-light font-serif">PyTorch - Tensors</span></div>
                <div id="reference-5"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://doc.rust-lang.org/std/sync/struct.Arc.html"
                      target="_blank">5</a>]</span><span class="font-light font-serif">Atomic Reference Counting -
                    std::sync::Arc</span></div>
                <div id="reference-6"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold"
                      href="https://docs.rust-embedded.org/book/intro/no-std.html" target="_blank">6</a>]</span><span
                    class="font-light font-serif">A no_std Rust Environment</span></div>
                <div id="reference-7"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://doc.rust-lang.org/alloc/"
                      target="_blank">7</a>]</span><span class="font-light font-serif">Crate alloc</span></div>
                <div id="reference-8"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://doc.rust-lang.org/std/rc/struct.Rc.html"
                      target="_blank">8</a>]</span><span class="font-light font-serif">Reference Counting -
                    std::rc::Rc</span></div>
                <div id="reference-9"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold"
                      href="https://doc.rust-lang.org/std/sync/struct.Arc.html#method.try_unwrap"
                      target="_blank">9</a>]</span><span class="font-light font-serif">Arc: try_unwrap</span></div>
                <div id="reference-10"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold"
                      href="https://doc.rust-lang.org/std/sync/struct.Arc.html#method.get_mut"
                      target="_blank">10</a>]</span><span class="font-light font-serif">Arc: get_mut</span></div>
                <div id="reference-11"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://doc.rust-lang.org/stable/clippy/usage.html"
                      target="_blank">11</a>]</span><span class="font-light font-serif">Clippy Usage</span></div>
                <div id="reference-12"><span class="pr-2 font-light font-serif leading-7 tracking-wide">[<a
                      class="text-[#69b8e1] hover:font-bold" href="https://pytorch.org/blog/pytorch-2.0-release/"
                      target="_blank">12</a>]</span><span class="font-light font-serif">PyTorch 2.0 release</span></div>
              </div><!--/-->
            </div><!--/-->
          </div>
        </article>
      </div>
    </div>
  </main>
  <dialog
    class="items-center text-balance rounded-2xl border bg-transparent p-8 text-center backdrop-blur-md backdrop:bg-black/70 max-sm:mx-6 overflow-hidden"
    id="stay-connected-dialog">
    <h2 class="text-v2h2">Join the mailing list</h2>
    <p class="py-4 font-mono text-body"> Join our community! We&#39;d love to keep you in the loop with our newsletter.
    </p>
    <script>(() => { var a = (s, i, o) => { let r = async () => { await (await s())() }, t = typeof i.value == "object" ? i.value : void 0, c = { rootMargin: t == null ? void 0 : t.rootMargin }, n = new IntersectionObserver(e => { for (let l of e) if (l.isIntersecting) { n.disconnect(), r(); break } }, c); for (let e of o.children) n.observe(e) }; (self.Astro || (self.Astro = {})).visible = a; window.dispatchEvent(new Event("astro:visible")); })();</script>
    <astro-island uid="Z1fPhRT" data-solid-render-id="s0" component-url="/_astro/EmailCollector.BqSZQ7nP.js"
      component-export="EmailCollector" renderer-url="/_astro/client.C-0b9Jot.js"
      props="{&quot;website&quot;:[0,&quot;burn&quot;],&quot;inputId&quot;:[0,&quot;stay-connected&quot;],&quot;title&quot;:[0,&quot;Stay connected&quot;],&quot;submitTitle&quot;:[0,&quot;submit email&quot;],&quot;inputLabel&quot;:[0,&quot;email address&quot;],&quot;placeholder&quot;:[0,&quot;your em@il&quot;],&quot;bgAlt&quot;:[0,&quot;stay connected background&quot;],&quot;subscribed&quot;:[0,&quot;subscribed&quot;],&quot;unsubscribed&quot;:[0,&quot;unsubscribed&quot;],&quot;firebaseConfig&quot;:[0,{&quot;apiKey&quot;:[0,&quot;AIzaSyDtXjGAc35AxrqEa8Fdmz3E1sKIX7MfgzU&quot;],&quot;authDomain&quot;:[0,&quot;tracel-website.firebaseapp.com&quot;],&quot;projectId&quot;:[0,&quot;tracel-website&quot;],&quot;storageBucket&quot;:[0,&quot;tracel-website.appspot.com&quot;],&quot;messagingSenderId&quot;:[0,&quot;278977725843&quot;],&quot;appId&quot;:[0,&quot;1:278977725843:web:be459a460a35c455cb9466&quot;]}]}"
      ssr="" client="visible" opts="{&quot;name&quot;:&quot;EmailCollector&quot;,&quot;value&quot;:true}"
      await-children="">
      <form data-hk="s00000" name="stay-connected"><span class="flex items-center rounded-lg gap-2">
          <div class="relative w-full"><input
              class="w-full h-12 flex-1 rounded-md bg-black/20 pl-2 pr-6 text-xs transition-border-duration-700 placeholder:text-primary-content/40 focus:outline-2 border border-primary-content"
              id="stay-connected" placeholder="your em@il" name="email" autocomplete="email" type="email"
              aria-label="email address" required=""><output class="absolute right-5 top-1/2 -translate-y-1/2"
              name="subscribeStatus" for="stay-connected"><svg width="16px" viewbox="2 -11 9 10"
                xmlns="http://www.w3.org/2000/svg">
                <title>unsubscribed</title><text x="0" y="0" class="fill-green-700"></text>
              </svg></output></div><button class="primary-button rounded-md px-2 text-xl h-12 w-20 border border-white"
            title="submit email" type="submit">⇥</button>
        </span></form><!--astro:end-->
    </astro-island>
    <form class="absolute right-0 top-0" method="dialog"> <button class="rounded-md p-4" title="dismiss">
        ✕
      </button> </form>
  </dialog>
  <script>(function () {
      const id = "stay-connected";

      /* eslint-env browser */
      const setupModal = () => {
        if (location.hash === `#${id}`) {
          const dialog = document.getElementById(`${id}-dialog`);
          dialog.addEventListener(
            "close",
            () => {
              history.pushState(
                "",
                document.title,
                window.location.pathname + window.location.search,
              );
            },
            { once: true },
          );

          dialog.addEventListener("click", (e) => {
            const dialogDimensions = dialog.getBoundingClientRect();
            if (
              e.clientX < dialogDimensions.left ||
              e.clientX > dialogDimensions.right ||
              e.clientY < dialogDimensions.top ||
              e.clientY > dialogDimensions.bottom
            ) {
              dialog.close();
            }
          });

          dialog.showModal();
        }
      };

      document.addEventListener("readystatechange", (e) => {
        if (e.target.readyState === "interactive") {
          setupModal();
          window.addEventListener("popstate", setupModal);
        }
      });
    })();</script>
  <div class="flex w-full flex-col items-center gap-8 py-10 text-mid">
    <div class="w-full max-w-[1360px] px-8 md:px-20">
      <div
        class="grid w-full grid-cols-1 gap-8 md:gap-20 [@media(min-width:400px)]:grid-cols-2 [@media(min-width:768px)]:grid-cols-4">
        <div class="flex w-full flex-col text-center [@media(min-width:400px)]:text-left">
          <h3 class="text-nowrap text-h3 text-light">Resources</h3> <a href="../../index.htm"
            class="text-body text-grey-accent hover:text-light"> Home </a><a href="../../get-started/index.htm"
            class="text-body text-grey-accent hover:text-light"> Get Started </a><a
            href="../../benchmarks/community-benchmarks/index.htm" class="text-body text-grey-accent hover:text-light">
            Benchmarks </a><a href="../index.htm" class="text-body text-grey-accent hover:text-light"> Blog </a><a
            href="../../learn/index.htm" class="text-body text-grey-accent hover:text-light"> Learn </a><a
            href="../../docs/burn/index.htm" class="text-body text-grey-accent hover:text-light"> Docs </a>
        </div>
        <div class="flex w-full flex-col text-center [@media(min-width:400px)]:text-left">
          <h3 class="text-nowrap text-h3 text-light">Community</h3> <a href="https://github.com/tracel-ai/burn"
            class="text-body text-grey-accent hover:text-light"> Github </a><a href="https://discord.gg/uPEBbYYDB6"
            class="text-body text-grey-accent hover:text-light"> Discord </a><a href="#stay-connected"
            class="text-body text-grey-accent hover:text-light"> Mailing list </a><a href="https://tracel.ai"
            class="text-body text-grey-accent hover:text-light"> Tracel </a>
        </div>
        <div class="flex w-full flex-col text-center [@media(min-width:400px)]:text-left">
          <h3 class="text-nowrap text-h3 text-light">Projects</h3> <a href="https://crates.io/crates/burn"
            class="text-body text-grey-accent hover:text-light"> Mabor Crate </a><a
            href="https://github.com/tracel-ai/burn" class="text-body text-grey-accent hover:text-light"> Mabor GitHub
          </a><a href="https://crates.io/crates/cubecl" class="text-body text-grey-accent hover:text-light"> CubeCL
            Crate </a><a href="https://github.com/tracel-ai/cubecl" class="text-body text-grey-accent hover:text-light">
            CubeCL GitHub </a>
        </div>
        <div class="flex w-full flex-col text-center [@media(min-width:400px)]:text-left">
          <h3 class="text-nowrap text-h3 text-light">Company</h3> <a href="https://tracel.ai/"
            class="text-body text-grey-accent hover:text-light"> Tracel AI </a><a
            href="https://www.linkedin.com/company/tracel-technologies"
            class="text-body text-grey-accent hover:text-light"> LinkedIn </a><a href="https://x.com/tracel_ai"
            class="text-body text-grey-accent hover:text-light"> X </a>
        </div>
      </div>
    </div>
    <p class="mt-6 text-center font-mono text-b2 text-mid md:flex"> Copyright 2025 © Mabor | Tracel Inc. All rights
      reserved. Design by Perdomo </p>
  </div>
</body>

</html>