<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Mabor Book 🔥</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async="" src="../../ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Mabor Book 🔥</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>Welcome to The Mabor Book 👋</p>
<p>This book will help you get started with the Mabor deep learning framework, whether you are an
advanced user or a beginner. We have crafted some sections for you:</p>
<ul>
<li>
<p><a href="basic-workflow/index.htm">Basic Workflow: From Training to Inference</a>: We'll start with the fundamentals,
guiding you through the entire workflow, from training your models to deploying them for
inference. This section lays the groundwork for your Mabor expertise.</p>
</li>
<li>
<p><a href="building-blocks/index.htm">Building Blocks</a>: Dive deeper into Mabor's core components, understanding how
they fit together. This knowledge forms the basis for more advanced usage and customization.</p>
</li>
<li>
<p><a href="saving-and-loading.html">Saving &amp; Loading Models</a>: Learn how to easily save and load your trained
models.</p>
</li>
<li>
<p><a href="custom-training-loop.html">Custom Training Loop</a>: Gain the power to customize your training
loops, fine-tuning your models to meet your specific requirements. This section empowers you to
harness Mabor's flexibility to its fullest.</p>
</li>
<li>
<p><a href="import/index.htm">Importing Models</a>: Learn how to import ONNX and PyTorch models, expanding your
compatibility with other deep learning ecosystems.</p>
</li>
<li>
<p><a href="advanced/index.htm">Advanced</a>: Finally, venture into advanced topics, exploring Mabor's capabilities at
their peak. This section caters to those who want to push the boundaries of what's possible with
Mabor.</p>
</li>
</ul>
<p>Throughout the book, we assume a basic understanding of deep learning concepts, but we may refer to
additional material when it seems appropriate.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="why-burn"><a class="header" href="#why-burn">Why Mabor?</a></h1>
<p>Why bother with the effort of creating an entirely new deep learning framework from scratch when
PyTorch, TensorFlow, and other frameworks already exist? Spoiler alert: Mabor isn't merely a
replication of PyTorch or TensorFlow in Rust. It represents a novel approach, placing significant
emphasis on making the right compromises in the right areas to facilitate exceptional flexibility,
high performance, and a seamless developer experience. Mabor isn’t a framework specialized for only
one type of application, it is designed to serve as a versatile framework suitable for a wide range
of research and production uses. The foundation of Mabor's design revolves around three key user
profiles:</p>
<p><strong>Machine Learning Researchers</strong> require tools to construct and execute experiments efficiently.
It’s essential for them to iterate quickly on their ideas and design testable experiments which can
help them discover new findings. The framework should facilitate the swift implementation of
cutting-edge research while ensuring fast execution for testing.</p>
<p><strong>Machine Learning Engineers</strong> are another important demographic to keep in mind. Their focus leans
less on swift implementation and more on establishing robustness, seamless deployment, and
cost-effective operations. They seek dependable, economical models capable of achieving objectives
without excessive expense. The whole machine learning workflow —from training to inference— must be
as efficient as possible with minimal unpredictable behavior.</p>
<p><strong>Low level Software Engineers</strong> working with hardware vendors want their processing units to run
models as fast as possible to gain competitive advantage. This endeavor involves harnessing
hardware-specific features such as Tensor Core for Nvidia. Since they are mostly working at a system
level, they want to have absolute control over how the computation will be executed.</p>
<p>The goal of Mabor is to satisfy all of those personas!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>Mabor is a deep learning framework in the Rust programming language. Therefore, it goes without
saying that one must understand the basic notions of Rust. Reading the first chapters of the
<a href="https://doc.rust-lang.org/book/">Rust Book</a> is recommended, but don't worry if you're just starting
out. We'll try to provide as much context and reference to external resources when required. Just
look out for the <strong>🦀 Rust Note</strong> indicators.</p>
<h2 id="installing-rust"><a class="header" href="#installing-rust">Installing Rust</a></h2>
<p>For installation instructions, please refer to the
<a href="https://doc.rust-lang.org/book/ch01-01-installation.html">installation page</a>. It explains in
details the most convenient way for you to install Rust on your computer, which is the very first
thing to do to start using Mabor.</p>
<h2 id="creating-a-burn-application"><a class="header" href="#creating-a-burn-application">Creating a Mabor application</a></h2>
<p>Once Rust is correctly installed, create a new Rust application by using Rust's build system and
package manager Cargo. It is automatically installed with Rust.</p>
<details>
<summary><strong>🦀 Cargo Cheat Sheet</strong></summary>
<p><a href="https://doc.rust-lang.org/cargo/">Cargo</a> is a very useful tool to manage Rust projects because it
handles a lot of tasks. More precisely, it is used to compile your code, download the
libraries/packages your code depends on, and build said libraries.</p>
<p>Below is a quick cheat sheet of the main <code>cargo</code> commands you might use throughout this guide.</p>
<div class="table-wrapper"><table><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody>
<tr><td><code>cargo new</code> <em>path</em></td><td>Create a new Cargo package in the given directory.</td></tr>
<tr><td><code>cargo add</code> <em>crate</em></td><td>Add dependencies to the Cargo.toml manifest file.</td></tr>
<tr><td><code>cargo build</code></td><td>Compile the local package and all of its dependencies (in debug mode, use <code>-r</code> for release).</td></tr>
<tr><td><code>cargo check</code></td><td>Check the local package for compilation errors (much faster).</td></tr>
<tr><td><code>cargo run</code></td><td>Run the local package binary.</td></tr>
</tbody></table>
</div>
<p>For more information, check out
<a href="https://doc.rust-lang.org/book/ch01-03-hello-cargo.html">Hello, Cargo!</a> in the Rust Book.</p>
</details><br>
<p>In the directory of your choice, run the following:</p>
<pre><code class="language-console">cargo new my_mabor_app
</code></pre>
<p>This will initialize the <code>my_mabor_app</code> project directory with a <code>Cargo.toml</code> file and a <code>src</code>
directory with an auto-generated <code>main.rs</code> file inside. Head inside the directory to check:</p>
<pre><code class="language-console">cd my_mabor_app
</code></pre>
<p>Then, add Mabor as a dependency:</p>
<pre><code class="language-console">cargo add mabor --features wgpu
</code></pre>
<p>Finally, compile the local package by executing the following:</p>
<pre><code class="language-console">cargo build
</code></pre>
<p>That's it, you're ready to start! You have a project configured with Mabor and the WGPU backend,
which allows to execute low-level operations on any platform using the GPU.</p>
<div class="warning">
<p>When using one of the <code>wgpu</code> backends, you may encounter compilation errors related to recursive
type evaluation. This is due to complex type nesting within the <code>wgpu</code> dependency chain.</p>
<p>To resolve this issue, add the following line at the top of your <code>main.rs</code> or <code>lib.rs</code> file:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span>#![recursion_limit = "256"]
<span class="boring">fn main() {
</span><span class="boring">}</span></code></pre></pre>
<p>The default recursion limit (128) is often just below the required depth (typically 130-150) due to
deeply nested associated types and trait bounds.</p>
</div>
<h2 id="writing-a-code-snippet"><a class="header" href="#writing-a-code-snippet">Writing a code snippet</a></h2>
<p>The <code>src/main.rs</code> was automatically generated by Cargo, so let's replace its content with the
following:</p>
<pre><code class="language-rust  ignore">use mabor::tensor::Tensor;
use mabor::backend::Wgpu;

// Type alias for the backend to use.
type Backend = Wgpu;

fn main() {
    let device = Default::default();
    // Creation of two tensors, the first with explicit values and the second one with ones, with the same shape as the first
    let tensor_1 = Tensor::&lt;Backend, 2&gt;::from_data([[2., 3.], [4., 5.]], &amp;device);
    let tensor_2 = Tensor::&lt;Backend, 2&gt;::ones_like(&amp;tensor_1);

    // Print the element-wise addition (done with the WGPU backend) of the two tensors.
    println!("{}", tensor_1 + tensor_2);
}</code></pre>
<details>
<summary><strong>🦀 Use Declarations</strong></summary>
<p>To bring any of the Mabor module or item into scope, a <code>use</code> declaration is added.</p>
<p>In the example above, we wanted bring the <code>Tensor</code> struct and <code>Wgpu</code> backend into scope with the
following:</p>
<pre><code class="language-rust  ignore">use mabor::tensor::Tensor;
use mabor::backend::Wgpu;</code></pre>
<p>This is pretty self-explanatory in this case. But, the same declaration could be written as a
shortcut to simultaneously binding of multiple paths with a common prefix:</p>
<pre><code class="language-rust  ignore">use mabor::{tensor::Tensor, backend::Wgpu};</code></pre>
<p>In this example, the common prefix is pretty short and there are only two items to bind locally.
Therefore, the first usage with two <code>use</code> declarations might be preferred. But know that both
examples are valid. For more details on the <code>use</code> keyword, take a look at
<a href="https://doc.rust-lang.org/book/ch07-04-bringing-paths-into-scope-with-the-use-keyword.html">this section</a>
of the Rust Book or the
<a href="https://doc.rust-lang.org/reference/items/use-declarations.html">Rust reference</a>.</p>
</details><br>
<details>
<summary><strong>🦀 Generic Data Types</strong></summary>
<p>If you're new to Rust, you're probably wondering why we had to use <code>Tensor::&lt;Backend, 2&gt;::...</code>.
That's because the <code>Tensor</code> struct is <a href="https://doc.rust-lang.org/book/ch10-01-syntax.html">generic</a>
over multiple concrete data types. More specifically, a <code>Tensor</code> can be defined using three generic
parameters: the backend, the number of dimensions (rank) and the data type (defaults to <code>Float</code>).
Here, we only specify the backend and number of dimensions since a <code>Float</code> tensor is used by
default. For more details on the <code>Tensor</code> struct, take a look at
<a href="building-blocks/tensor.html">this section</a>.</p>
<p>Most of the time when generics are involved, the compiler can infer the generic parameters
automatically. In this case, the compiler needs a little help. This can usually be done in one of
two ways: providing a type annotation or binding the gereneric parameter via the <em>turbofish</em> <code>::&lt;&gt;</code>
syntax. In the example above we used the so-called <em>turbofish</em> syntax, but we could have used type
annotations instead like this:</p>
<pre><code class="language-rust  ignore">let tensor_1: Tensor&lt;Backend, 2&gt; = Tensor::from_data([[2., 3.], [4., 5.]]);
let tensor_2 = Tensor::ones_like(&amp;tensor_1);</code></pre>
<p>You probably noticed that we provided a type annotation for the first tensor only and yet this
example still works. That's because the compiler (correctly) inferred that <code>tensor_2</code> had the same
generic parameters. The same could have been done in the original example, but specifying the
parameters for both is more explicit.</p>
</details><br>
<p>By running <code>cargo run</code>, you should now see the result of the addition:</p>
<pre><code class="language-console">Tensor {
  data:
[[3.0, 4.0],
 [5.0, 6.0]],
  shape:  [2, 2],
  device:  DefaultDevice,
  backend:  "wgpu",
  kind:  "Float",
  dtype:  "f32",
}
</code></pre>
<p>While the previous example is somewhat trivial, the upcoming basic workflow section will walk you
through a much more relevant example for deep learning applications.</p>
<h2 id="using-prelude"><a class="header" href="#using-prelude">Using <code>prelude</code></a></h2>
<p>Mabor comes with a variety of things in its core library. When creating a new model or using an
existing one for inference, you may need to import every single component you used, which could be a
little verbose.</p>
<p>To address it, a <code>prelude</code> module is provided, allowing you to easily import commonly used structs
and macros as a group:</p>
<pre><code class="language-rust  ignore">use mabor::prelude::*;</code></pre>
<p>which is equal to:</p>
<pre><code class="language-rust  ignore">use mabor::{
    config::Config,
    module::Module,
    nn,
    tensor::{
        backend::Backend, Bool, Device, ElementConversion, Float, Int, Shape, Tensor,
        TensorData,
    },
};</code></pre>
<div class="warning">
<p>For the sake of simplicity, the subsequent chapters of this book will all use this form of importing
except in the <a href="building-blocks/index.htm">Building Blocks</a> chapter, as explicit importing aids users in
grasping the usage of particular structures and macros.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples"><a class="header" href="#examples">Examples</a></h1>
<p>In the <a href="basic-workflow/index.htm">next chapter</a> you'll have the opportunity to implement the whole Mabor
<code>guide</code> example yourself in a step by step manner.</p>
<p>Many additional Mabor examples are available in the
<a href="https://github.com/tracel-ai/burn/tree/main/examples">examples</a> directory. Mabor examples are
organized as library crates with one or more examples that are executable binaries. An example can
then be executed using the following cargo command line in the root of the Mabor repository:</p>
<pre><code class="language-bash">cargo run --example &lt;example name&gt;
</code></pre>
<p>To learn more about crates and examples, read the Rust section below.</p>
<details>
<summary><strong>🦀 About Rust crates</strong></summary>
<p>Each Mabor example is a <strong>package</strong> which are subdirectories of the <code>examples</code> directory. A package
is composed of one or more <strong>crates</strong>.</p>
<p>A package is a bundle of one or more crates that provides a set of functionality. A package contains
a <code>Cargo.toml</code> file that describes how to build those crates.</p>
<p>A crate is a compilation unit in Rust. It could be a single file, but it is often easier to split up
crates into multiple <strong>modules</strong>.</p>
<p>A module lets us organize code within a crate for readability and easy reuse. Modules also allow us
to control the <em>privacy</em> of items. For instance the <code>pub(crate)</code> keyword is employed to make a
module publicly available inside the crate. In the snippet below there are four modules declared,
two of them are public and visible to the users of the crates, one of them is public inside the
crate only and crate users cannot see it, at last one is private when there is no keyword. These
modules can be single files or a directory with a <code>mod.rs</code> file inside.</p>
<pre><code class="language-rust  ignore">pub mod data;
pub mod inference;
pub(crate) mod model;
mod training;</code></pre>
<p>A crate can come in one of two forms: a <strong>binary crate</strong> or a <strong>library crate</strong>. When compiling a
crate, the compiler first looks in the crate root file (<code>src/lib.rs</code> for a library crate and
<code>src/main.rs</code> for a binary crate). Any module declared in the crate root file will be inserted in
the crate for compilation.</p>
<p>All Mabor examples are library crates and they can contain one or more executable examples that uses
the library. We even have some Mabor examples that uses the library crate of other examples.</p>
<p>The examples are unique files under the <code>examples</code> directory. Each file produces an executable file
with the same name. Each example can then be executed with <code>cargo run --example &lt;executable name&gt;</code>.</p>
<p>Below is a file tree of a typical Mabor example package:</p>
<pre><code>examples/mabor-example
├── Cargo.toml
├── examples
│   ├── example1.rs      ---&gt; compiled to example1 binary
│   ├── example2.rs      ---&gt; compiled to example2 binary
│   └── ...
└── src
    ├── lib.rs           ---&gt; this is the root file for a library
    ├── module1.rs
    ├── module2.rs
    └── ...
</code></pre>
</details><br>
<p>The following additional examples are currently available if you want to check them out:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">Example</th><th>Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-csv-dataset">Custom CSV Dataset</a></td><td>Implements a dataset to parse CSV data for a regression task.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/simple-regression">Regression</a></td><td>Trains a simple MLP on the California Housing dataset to predict the median house value for a district.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-image-dataset">Custom Image Dataset</a></td><td>Trains a simple CNN on custom image dataset following a simple folder structure.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-renderer">Custom Renderer</a></td><td>Implements a custom renderer to display the <a href="building-blocks/learner.html"><code>Learner</code></a> progress.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/image-classification-web">Image Classification Web</a></td><td>Image classification web browser demo using Mabor, WGPU and WebAssembly.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/mnist-inference-web">MNIST Inference on Web</a></td><td>An interactive MNIST inference demo in the browser. The demo is available <a href="https://burn.dev/demo/">online</a>.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/mnist">MNIST Training</a></td><td>Demonstrates how to train a custom <a href="building-blocks/module.html"><code>Module</code></a> (MLP) with the <a href="building-blocks/learner.html"><code>Learner</code></a> configured to log metrics and keep training checkpoints.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/named-tensor">Named Tensor</a></td><td>Performs operations with the experimental <code>NamedTensor</code> feature.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/onnx-inference">ONNX Import Inference</a></td><td>Imports an ONNX model pre-trained on MNIST to perform inference on a sample image with Mabor.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/pytorch-import">PyTorch Import Inference</a></td><td>Imports a PyTorch model pre-trained on MNIST to perform inference on a sample image with Mabor.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/text-classification">Text Classification</a></td><td>Trains a text classification transformer model on the AG News or DbPedia datasets. The trained model can then be used to classify a text sample.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/text-generation">Text Generation</a></td><td>Trains a text generation transformer model on the DbPedia dataset.</td></tr>
<tr><td style="text-align: left"><a href="https://github.com/tracel-ai/burn/tree/main/examples/wgan">Wasserstein GAN MNIST</a></td><td>Trains a WGAN model to generate new handwritten digits based on MNIST.</td></tr>
</tbody></table>
</div>
<p>For more information on each example, see their respective <code>README.md</code> file. Be sure to check out
the <a href="https://github.com/tracel-ai/burn/tree/main/examples">examples</a> directory for an up-to-date
list.</p>
<div class="warning">
<p>Note that some examples use the
<a href="https://huggingface.co/docs/datasets/index"><code>datasets</code> library by HuggingFace</a> to download the
datasets required in the examples. This is a Python library, which means that you will need to
install Python before running these examples. This requirement will be clearly indicated in the
example's README when applicable.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="guide"><a class="header" href="#guide">Guide</a></h1>
<p>This guide will walk you through the process of creating a custom model built with Mabor. We will
train a simple convolutional neural network model on the MNIST dataset and prepare it for inference.</p>
<p>For clarity, we sometimes omit imports in our code snippets. For more details, please refer to the
corresponding code in the <code>examples/guide</code> <a href="https://github.com/tracel-ai/burn/tree/release/0.18/examples/guide">directory</a>.
We reproduce this example in a step-by-step fashion, from dataset creation to modeling and training
in the following sections. It is recommended to use the capabilities of your IDE or text editor to
automatically add the missing imports as you add the code snippets to your code.</p>
<div class="warning">
<p>Be sure to checkout the git branch corresponding to the version of Mabor you are using to follow
this guide.</p>
<p>The current version of Mabor is <code>0.18</code> and the corresponding branch to checkout is <code>main</code>.</p>
</div>
<p>The code for this demo can be executed from Mabor's base directory using the command:</p>
<pre><code class="language-bash">cargo run --example guide
</code></pre>
<h2 id="key-learnings"><a class="header" href="#key-learnings">Key Learnings</a></h2>
<ul>
<li>Creating a project</li>
<li>Creating neural network models</li>
<li>Importing and preparing datasets</li>
<li>Training models on data</li>
<li>Choosing a backend</li>
<li>Using a model for inference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model"><a class="header" href="#model">Model</a></h1>
<p>The first step is to create a project and add the different Mabor dependencies. Start by creating a
new project with Cargo:</p>
<pre><code class="language-console">cargo new guide
</code></pre>
<p>As <a href="getting-started.html#creating-a-burn-application">mentioned previously</a>, this will initialize
your <code>guide</code> project directory with a <code>Cargo.toml</code> and a <code>src/main.rs</code> file.</p>
<p>In the <code>Cargo.toml</code> file, add the <code>mabor</code> dependency with <code>train</code>, <code>vision</code> and <code>wgpu</code> features.
Since we disable the default features, we also want to enable <code>std</code>, <code>tui</code> (for the dashboard) and
<code>fusion</code> for wgpu. Then run <code>cargo build</code> to build the project and import all the dependencies.</p>
<pre><code class="language-toml">[package]
name = "guide"
version = "0.1.0"
edition = "2024"

[dependencies]
# Disable autotune default for convolutions
mabor = { version = "~0.18", features = ["std", "tui", "train", "vision", "wgpu", "fusion"], default-features = false }
# mabor = { version = "~0.18", features = ["train", "vision", "wgpu"] }
</code></pre>
<p>Our goal will be to create a basic convolutional neural network used for image classification. We
will keep the model simple by using two convolution layers followed by two linear layers, some
pooling and ReLU activations. We will also use dropout to improve training performance.</p>
<p>Let us start by defining our model struct in a new file <code>src/model.rs</code>.</p>
<pre><code class="language-rust   ignore">use mabor::{
    nn::{
        conv::{Conv2d, Conv2dConfig},
        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
        Dropout, DropoutConfig, Linear, LinearConfig, Relu,
    },
    prelude::*,
};

#[derive(Module, Debug)]
pub struct Model&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
    pool: AdaptiveAvgPool2d,
    dropout: Dropout,
    linear1: Linear&lt;B&gt;,
    linear2: Linear&lt;B&gt;,
    activation: Relu,
}</code></pre>
<p>There are two major things going on in this code sample.</p>
<ol>
<li>
<p>You can create a deep learning module with the <code>#[derive(Module)]</code> attribute on top of a struct.
This will generate the necessary code so that the struct implements the <code>Module</code> trait. This
trait will make your module both trainable and (de)serializable while adding related
functionalities. Like other attributes often used in Rust, such as <code>Clone</code>, <code>PartialEq</code> or
<code>Debug</code>, each field within the struct must also implement the <code>Module</code> trait.</p>
<details>
<summary><strong>🦀 Trait</strong></summary>
<p>Traits are a powerful and flexible Rust language feature. They provide a way to define shared
behavior for a particular type, which can be shared with other types.</p>
<p>A type's behavior consists of the methods called on that type. Since all <code>Module</code>s should
implement the same functionality, it is defined as a trait. Implementing a trait on a particular
type usually requires the user to implement the defined behaviors of the trait for their types,
though that is not the case here as explained above with the <code>derive</code> attribute. Check out the
<a href="basic-workflow/model.html#derive-attribute">explainer below</a> to learn why.</p>
<p>For more details on traits, take a look at the
<a href="https://doc.rust-lang.org/book/ch10-02-traits.html">associated chapter</a> in the Rust Book.</p>
</details><br>
<details id="derive-attribute">
<summary><strong>🦀 Derive Macro</strong></summary>
<p>The <code>derive</code> attribute allows traits to be implemented easily by generating code that will
implement a trait with its own default implementation on the type that was annotated with the
<code>derive</code> syntax.</p>
<p>This is accomplished through a feature of Rust called
<a href="https://doc.rust-lang.org/reference/procedural-macros.html">procedural macros</a>, which allow us
to run code at compile time that operates over Rust syntax, both consuming and producing Rust
syntax. Using the attribute <code>#[my_macro]</code>, you can effectively extend the provided code. You will
see that the derive macro is very frequently employed to recursively implement traits, where the
implementation consists of the composition of all fields.</p>
<p>In this example, we want to derive the <a href="building-blocks/module.html"><code>Module</code></a> and <code>Debug</code>
traits.</p>
<pre><code class="language-rust  ignore">#[derive(Module, Debug)]
pub struct MyCustomModule&lt;B: Backend&gt; {
    linear1: Linear&lt;B&gt;,
    linear2: Linear&lt;B&gt;,
    activation: Relu,
}</code></pre>
<p>The basic <code>Debug</code> implementation is provided by the compiler to format a value using the <code>{:?}</code>
formatter. For ease of use, the <code>Module</code> trait implementation is automatically handled by Mabor so
you don't have to do anything special. It essentially acts as parameter container.</p>
<p>For more details on derivable traits, take a look at the Rust
<a href="https://doc.rust-lang.org/book/appendix-03-derivable-traits.html">appendix</a>,
<a href="https://doc.rust-lang.org/reference/attributes/derive.html">reference</a> or
<a href="https://doc.rust-lang.org/rust-by-example/trait/derive.html">example</a>.</p>
</details><br>
</li>
<li>
<p>Note that the struct is generic over the <a href="building-blocks/backend.html"><code>Backend</code></a> trait. The
backend trait abstracts the underlying low level implementations of tensor operations, allowing
your new model to run on any backend. Contrary to other frameworks, the backend abstraction isn't
determined by a compilation flag or a device type. This is important because you can extend the
functionalities of a specific backend (see
<a href="advanced/backend-extension/index.htm">backend extension section</a>), and it allows for an innovative
<a href="building-blocks/autodiff.html">autodiff system</a>. You can also change backend during runtime,
for instance to compute training metrics on a cpu backend while using a gpu one only to train the
model. In our example, the backend in use will be determined later on.</p>
<details>
<summary><strong>🦀 Trait Bounds</strong></summary>
<p>Trait bounds provide a way for generic items to restrict which types are used as their
parameters. The trait bounds stipulate what functionality a type implements. Therefore, bounding
restricts the generic to types that conform to the bounds. It also allows generic instances to
access the methods of traits specified in the bounds.</p>
<p>For a simple but concrete example, check out the
<a href="https://doc.rust-lang.org/rust-by-example/generics/bounds.html">Rust By Example on bounds</a>.</p>
<p>In Mabor, the <code>Backend</code> trait enables you to run tensor operations using different implementations
as it abstracts tensor, device and element types. The
<a href="getting-started.html#writing-a-code-snippet">getting started example</a> illustrates the advantage
of having a simple API that works for different backend implementations. While it used the WGPU
backend, you could easily swap it with any other supported backend.</p>
<pre><code class="language-rust  ignore">// Choose from any of the supported backends.
// type Backend = Candle&lt;f32, i64&gt;;
// type Backend = LibTorch&lt;f32&gt;;
// type Backend = NdArray&lt;f32&gt;;
type Backend = Wgpu;

// Creation of two tensors.
let tensor_1 = Tensor::&lt;Backend, 2&gt;::from_data([[2., 3.], [4., 5.]], &amp;device);
let tensor_2 = Tensor::&lt;Backend, 2&gt;::ones_like(&amp;tensor_1);

// Print the element-wise addition (done with the selected backend) of the two tensors.
println!("{}", tensor_1 + tensor_2);</code></pre>
<p>For more details on trait bounds, check out the Rust
<a href="https://doc.rust-lang.org/book/ch10-02-traits.html#trait-bound-syntax">trait bound section</a> or
<a href="https://doc.rust-lang.org/reference/items/traits.html#trait-bounds">reference</a>.</p>
</details><br>
</li>
</ol>
<p>Note that each time you create a new file in the <code>src</code> directory you also need to explicitly add
this module to the <code>main.rs</code> file. For instance after creating the <code>model.rs</code>, you need to add the
following at the top of the main file:</p>
<pre><code class="language-rust   ignore">mod model;
<span class="boring">
</span><span class="boring">fn main() {
</span><span class="boring">}</span></code></pre>
<p>Next, we need to instantiate the model for training.</p>
<pre><code class="language-rust   ignore"><span class="boring">use mabor::{
</span><span class="boring">    nn::{
</span><span class="boring">        conv::{Conv2d, Conv2dConfig},
</span><span class="boring">        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
</span><span class="boring">        Dropout, DropoutConfig, Linear, LinearConfig, Relu,
</span><span class="boring">    },
</span><span class="boring">    prelude::*,
</span><span class="boring">};
</span><span class="boring">
</span><span class="boring">#[derive(Module, Debug)]
</span><span class="boring">pub struct Model&lt;B: Backend&gt; {
</span><span class="boring">    conv1: Conv2d&lt;B&gt;,
</span><span class="boring">    conv2: Conv2d&lt;B&gt;,
</span><span class="boring">    pool: AdaptiveAvgPool2d,
</span><span class="boring">    dropout: Dropout,
</span><span class="boring">    linear1: Linear&lt;B&gt;,
</span><span class="boring">    linear2: Linear&lt;B&gt;,
</span><span class="boring">    activation: Relu,
</span><span class="boring">}
</span><span class="boring">
</span>#[derive(Config, Debug)]
pub struct ModelConfig {
    num_classes: usize,
    hidden_size: usize,
    #[config(default = "0.5")]
    dropout: f64,
}

impl ModelConfig {
    /// Returns the initialized model.
    pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; Model&lt;B&gt; {
        Model {
            conv1: Conv2dConfig::new([1, 8], [3, 3]).init(device),
            conv2: Conv2dConfig::new([8, 16], [3, 3]).init(device),
            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),
            activation: Relu::new(),
            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init(device),
            linear2: LinearConfig::new(self.hidden_size, self.num_classes).init(device),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<p>At a glance, you can view the model configuration by printing the model instance:</p>
<pre><code class="language-rust   ignore">#![recursion_limit = "256"]
mod model;

use crate::model::ModelConfig;
use mabor::backend::Wgpu;

fn main() {
    type MyBackend = Wgpu&lt;f32, i32&gt;;

    let device = Default::default();
    let model = ModelConfig::new(10, 512).init::&lt;MyBackend&gt;(&amp;device);

    println!("{model}");
}</code></pre>
<p>Output:</p>
<pre><code class="language-rust   ignore">Model {
  conv1: Conv2d {stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Valid, params: 80}
  conv2: Conv2d {stride: [1, 1], kernel_size: [3, 3], dilation: [1, 1], groups: 1, padding: Valid, params: 1168}
  pool: AdaptiveAvgPool2d {output_size: [8, 8]}
  dropout: Dropout {prob: 0.5}
  linear1: Linear {d_input: 1024, d_output: 512, bias: true, params: 524800}
  linear2: Linear {d_input: 512, d_output: 10, bias: true, params: 5130}
  activation: Relu
  params: 531178
}</code></pre>
<details>
<summary><strong>🦀 References</strong></summary>
<p>In the previous example, the <code>init()</code> method signature uses <code>&amp;</code> to indicate that the parameter types
are references: <code>&amp;self</code>, a reference to the current receiver (<code>ModelConfig</code>), and
<code>device: &amp;B::Device</code>, a reference to the backend device.</p>
<pre><code class="language-rust  ignore">pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; Model&lt;B&gt; {
    Model {
        // ...
    }
}</code></pre>
<p>References in Rust allow us to point to a resource to access its data without owning it. The idea of
ownership is quite core to Rust and is worth
<a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html">reading up on</a>.</p>
<p>In a language like C, memory management is explicit and up to the programmer, which means it is easy
to make mistakes. In a language like Java or Python, memory management is automatic with the help of
a garbage collector. This is very safe and straightforward, but also incurs a runtime cost.</p>
<p>In Rust, memory management is rather unique. Aside from simple types that implement
<a href="https://doc.rust-lang.org/std/marker/trait.Copy.html"><code>Copy</code></a> (e.g.,
<a href="https://doc.rust-lang.org/rust-by-example/primitives.html">primitives</a> like integers, floats,
booleans and <code>char</code>), every value is <em>owned</em> by some variable called the <em>owner</em>. Ownership can be
transferred from one variable to another and sometimes a value can be <em>borrowed</em>. Once the <em>owner</em>
variable goes out of scope, the value is <em>dropped</em>, which means that any memory it allocated can be
freed safely.</p>
<p>Because the method does not own the <code>self</code> and <code>device</code> variables, the values the references point
to will not be dropped when the reference stops being used (i.e., the scope of the method).</p>
<p>For more information on references and borrowing, be sure to read the
<a href="https://doc.rust-lang.org/book/ch04-02-references-and-borrowing.html">corresponding chapter</a> in the
Rust Book.</p>
</details><br>
<p>When creating a custom neural network module, it is often a good idea to create a config alongside
the model struct. This allows you to define default values for your network, thanks to the <code>Config</code>
attribute. The benefit of this attribute is that it makes the configuration serializable, enabling
you to painlessly save your model hyperparameters, enhancing your experimentation process. Note that
a constructor will automatically be generated for your configuration, which will take in as input
values the parameters which do not have default values:
<code>let config = ModelConfig::new(num_classes, hidden_size);</code>. The default values can be overridden
easily with builder-like methods: (e.g <code>config.with_dropout(0.2);</code>)</p>
<p>The first implementation block is related to the initialization method. As we can see, all fields
are set using the configuration of the corresponding neural network's underlying module. In this
specific case, we have chosen to expand the tensor channels from 1 to 8 with the first layer, then
from 8 to 16 with the second layer, using a kernel size of 3 on all dimensions. We also use the
adaptive average pooling module to reduce the dimensionality of the images to an 8 by 8 matrix,
which we will flatten in the forward pass to have a 1024 (16 * 8 * 8) resulting tensor.</p>
<p>Now let's see how the forward pass is defined.</p>
<pre><code class="language-rust   ignore"><span class="boring">use mabor::{
</span><span class="boring">    nn::{
</span><span class="boring">        conv::{Conv2d, Conv2dConfig},
</span><span class="boring">        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
</span><span class="boring">        Dropout, DropoutConfig, Linear, LinearConfig, Relu,
</span><span class="boring">    },
</span><span class="boring">    prelude::*,
</span><span class="boring">};
</span><span class="boring">
</span><span class="boring">#[derive(Module, Debug)]
</span><span class="boring">pub struct Model&lt;B: Backend&gt; {
</span><span class="boring">    conv1: Conv2d&lt;B&gt;,
</span><span class="boring">    conv2: Conv2d&lt;B&gt;,
</span><span class="boring">    pool: AdaptiveAvgPool2d,
</span><span class="boring">    dropout: Dropout,
</span><span class="boring">    linear1: Linear&lt;B&gt;,
</span><span class="boring">    linear2: Linear&lt;B&gt;,
</span><span class="boring">    activation: Relu,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">#[derive(Config, Debug)]
</span><span class="boring">pub struct ModelConfig {
</span><span class="boring">    num_classes: usize,
</span><span class="boring">    hidden_size: usize,
</span><span class="boring">    #[config(default = "0.5")]
</span><span class="boring">    dropout: f64,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl ModelConfig {
</span><span class="boring">    /// Returns the initialized model.
</span><span class="boring">    pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; Model&lt;B&gt; {
</span><span class="boring">        Model {
</span><span class="boring">            conv1: Conv2dConfig::new([1, 8], [3, 3]).init(device),
</span><span class="boring">            conv2: Conv2dConfig::new([8, 16], [3, 3]).init(device),
</span><span class="boring">            pool: AdaptiveAvgPool2dConfig::new([8, 8]).init(),
</span><span class="boring">            activation: Relu::new(),
</span><span class="boring">            linear1: LinearConfig::new(16 * 8 * 8, self.hidden_size).init(device),
</span><span class="boring">            linear2: LinearConfig::new(self.hidden_size, self.num_classes).init(device),
</span><span class="boring">            dropout: DropoutConfig::new(self.dropout).init(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">}
</span><span class="boring">
</span>impl&lt;B: Backend&gt; Model&lt;B&gt; {
    /// # Shapes
    ///   - Images [batch_size, height, width]
    ///   - Output [batch_size, num_classes]
    pub fn forward(&amp;self, images: Tensor&lt;B, 3&gt;) -&gt; Tensor&lt;B, 2&gt; {
        let [batch_size, height, width] = images.dims();

        // Create a channel at the second dimension.
        let x = images.reshape([batch_size, 1, height, width]);


        let x = self.conv1.forward(x); // [batch_size, 8, _, _]
        let x = self.dropout.forward(x);
        let x = self.conv2.forward(x); // [batch_size, 16, _, _]
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        let x = self.pool.forward(x); // [batch_size, 16, 8, 8]
        let x = x.reshape([batch_size, 16 * 8 * 8]);
        let x = self.linear1.forward(x);
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        self.linear2.forward(x) // [batch_size, num_classes]
    }
}</code></pre>
<p>For former PyTorch users, this might feel very intuitive, as each module is directly incorporated
into the code using an eager API. Note that no abstraction is imposed for the forward method. You
are free to define multiple forward functions with the names of your liking. Most of the neural
network modules already built with Mabor use the <code>forward</code> nomenclature, simply because it is
standard in the field.</p>
<p>Similar to neural network modules, the <a href="building-blocks/tensor.html"><code>Tensor</code></a> struct given as a
parameter also takes the Backend trait as a generic argument, alongside its dimensionality. Even if
it is not used in this specific example, it is possible to add the kind of the tensor as a third
generic argument. For example, a 3-dimensional Tensor of different data types(float, int, bool)
would be defined as following:</p>
<pre><code class="language-rust   ignore">Tensor&lt;B, 3&gt; // Float tensor (default)
Tensor&lt;B, 3, Float&gt; // Float tensor (explicit)
Tensor&lt;B, 3, Int&gt; // Int tensor
Tensor&lt;B, 3, Bool&gt; // Bool tensor</code></pre>
<p>Note that the specific element type, such as <code>f16</code>, <code>f32</code> and the likes, will be defined later with
the backend.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data"><a class="header" href="#data">Data</a></h1>
<p>Typically, one trains a model on some dataset. Mabor provides a library of very useful dataset
sources and transformations, such as Hugging Face dataset utilities that allow to download and store
data into an SQLite database for extremely efficient data streaming and storage. For this guide
though, we will use the MNIST dataset from <code>mabor::data::dataset::vision</code> which requires no external
dependency.</p>
<p>To iterate over a dataset efficiently, we will define a struct which will implement the <code>Batcher</code>
trait. The goal of a batcher is to map individual dataset items into a batched tensor that can be
used as input to our previously defined model.</p>
<p>Let us start by defining our dataset functionalities in a file <code>src/data.rs</code>. We shall omit some of
the imports for brevity, but the full code for following this guide can be found at
<code>examples/guide/</code> <a href="https://github.com/tracel-ai/burn/tree/main/examples/guide">directory</a>.</p>
<pre><code class="language-rust   ignore">use mabor::{
    data::{dataloader::batcher::Batcher, dataset::vision::MnistItem},
    prelude::*,
};


#[derive(Clone, Default)]
pub struct MnistBatcher {}</code></pre>
<p>This batcher is pretty straightforward, as it only defines a struct that will implement the
<code>Batcher</code> trait. The trait is generic over the <code>Backend</code> trait, which includes an associated type
for the device, as not all backends expose the same devices. As an example, the Libtorch-based
backend exposes <code>Cuda(gpu_index)</code>, <code>Cpu</code>, <code>Vulkan</code> and <code>Metal</code> devices, while the ndarray backend
only exposes the <code>Cpu</code> device.</p>
<p>Next, we need to actually implement the batching logic.</p>
<pre><code class="language-rust   ignore"><span class="boring">use mabor::{
</span><span class="boring">    data::{dataloader::batcher::Batcher, dataset::vision::MnistItem},
</span><span class="boring">    prelude::*,
</span><span class="boring">};
</span><span class="boring">
</span><span class="boring">#[derive(Clone, Default)]
</span><span class="boring">pub struct MnistBatcher {}
</span><span class="boring">
</span>#[derive(Clone, Debug)]
pub struct MnistBatch&lt;B: Backend&gt; {
    pub images: Tensor&lt;B, 3&gt;,
    pub targets: Tensor&lt;B, 1, Int&gt;,
}

impl&lt;B: Backend&gt; Batcher&lt;B, MnistItem, MnistBatch&lt;B&gt;&gt; for MnistBatcher {
    fn batch(&amp;self, items: Vec&lt;MnistItem&gt;, device: &amp;B::Device) -&gt; MnistBatch&lt;B&gt; {
        let images = items
            .iter()
            .map(|item| TensorData::from(item.image).convert::&lt;B::FloatElem&gt;())
            .map(|data| Tensor::&lt;B, 2&gt;::from_data(data, device))
            .map(|tensor| tensor.reshape([1, 28, 28]))
            // Normalize: scale between [0,1] and make the mean=0 and std=1
            // values mean=0.1307,std=0.3081 are from the PyTorch MNIST example
            // https://github.com/pytorch/examples/blob/54f4572509891883a947411fd7239237dd2a39c3/mnist/main.py#L122
            .map(|tensor| ((tensor / 255) - 0.1307) / 0.3081)
            .collect();

        let targets = items
            .iter()
            .map(|item| {
                Tensor::&lt;B, 1, Int&gt;::from_data([(item.label as i64).elem::&lt;B::IntElem&gt;()], device)
            })
            .collect();

        let images = Tensor::cat(images, 0);
        let targets = Tensor::cat(targets, 0);

        MnistBatch { images, targets }
    }
}</code></pre>
<details>
<summary><strong>🦀 Iterators and Closures</strong></summary>
<p>The iterator pattern allows you to perform some tasks on a sequence of items in turn.</p>
<p>In this example, an iterator is created over the <code>MnistItem</code>s in the vector <code>items</code> by calling the
<code>iter</code> method.</p>
<p><em>Iterator adaptors</em> are methods defined on the <code>Iterator</code> trait that produce different iterators by
changing some aspect of the original iterator. Here, the <code>map</code> method is called in a chain to
transform the original data before consuming the final iterator with <code>collect</code> to obtain the
<code>images</code> and <code>targets</code> vectors. Both vectors are then concatenated into a single tensor for the
current batch.</p>
<p>You probably noticed that each call to <code>map</code> is different, as it defines a function to execute on
the iterator items at each step. These anonymous functions are called
<a href="https://doc.rust-lang.org/book/ch13-01-closures.html"><em>closures</em></a> in Rust. They're easy to
recognize due to their syntax which uses vertical bars <code>||</code>. The vertical bars capture the input
variables (if applicable) while the rest of the expression defines the function to execute.</p>
<p>If we go back to the example, we can break down and comment the expression used to process the
images.</p>
<pre><code class="language-rust  ignore">let images = items                                                       // take items Vec&lt;MnistItem&gt;
    .iter()                                                              // create an iterator over it
    .map(|item| TensorData::from(item.image).convert::&lt;B::FloatElem&gt;())  // for each item, convert the image to float data struct
    .map(|data| Tensor::&lt;B, 2&gt;::from_data(data, device))                 // for each data struct, create a tensor on the device
    .map(|tensor| tensor.reshape([1, 28, 28]))                           // for each tensor, reshape to the image dimensions [C, H, W]
    .map(|tensor| ((tensor / 255) - 0.1307) / 0.3081)                    // for each image tensor, apply normalization
    .collect();                                                          // consume the resulting iterator &amp; collect the values into a new vector</code></pre>
<p>For more information on iterators and closures, be sure to check out the
<a href="https://doc.rust-lang.org/book/ch13-00-functional-features.html">corresponding chapter</a> in the Rust
Book.</p>
</details><br>
<p>In the previous example, we implement the <code>Batcher</code> trait with a list of <code>MnistItem</code> as input and a
single <code>MnistBatch</code> as output. The batch contains the images in the form of a 3D tensor, along with
a targets tensor that contains the indexes of the correct digit class. The first step is to parse
the image array into a <code>TensorData</code> struct. Mabor provides the <code>TensorData</code> struct to encapsulate
tensor storage information without being specific for a backend. When creating a tensor from data,
we often need to convert the data precision to the current backend in use. This can be done with the
<code>.convert()</code> method (in this example, the data is converted backend's float element type
<code>B::FloatElem</code>). While importing the <code>mabor::tensor::ElementConversion</code> trait, you can call <code>.elem()</code>
on a specific number to convert it to the current backend element type in use.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="training"><a class="header" href="#training">Training</a></h1>
<p>We are now ready to write the necessary code to train our model on the MNIST dataset. We shall
define the code for this training section in the file: <code>src/training.rs</code>.</p>
<p>Instead of a simple tensor, the model should output an item that can be understood by the learner, a
struct whose responsibility is to apply an optimizer to the model. The output struct is used for all
metrics calculated during the training. Therefore it should include all the necessary information to
calculate any metric that you want for a task.</p>
<p>Mabor provides two basic output types: <code>ClassificationOutput</code> and <code>RegressionOutput</code>. They implement
the necessary trait to be used with metrics. It is possible to create your own item, but it is
beyond the scope of this guide.</p>
<p>Since the MNIST task is a classification problem, we will use the <code>ClassificationOutput</code> type.</p>
<pre><code class="language-rust   ignore"><span class="boring">use crate::{
</span><span class="boring">    data::{MnistBatch, MnistBatcher},
</span><span class="boring">    model::{Model, ModelConfig},
</span><span class="boring">};
</span><span class="boring">use mabor::{
</span><span class="boring">    data::{dataloader::DataLoaderBuilder, dataset::vision::MnistDataset},
</span><span class="boring">    nn::loss::CrossEntropyLossConfig,
</span><span class="boring">    optim::AdamConfig,
</span><span class="boring">    prelude::*,
</span><span class="boring">    record::CompactRecorder,
</span><span class="boring">    tensor::backend::AutodiffBackend,
</span><span class="boring">    train::{
</span><span class="boring">        metric::{AccuracyMetric, LossMetric},
</span><span class="boring">        ClassificationOutput, LearnerBuilder, TrainOutput, TrainStep, ValidStep,
</span><span class="boring">    },
</span><span class="boring">};
</span><span class="boring">
</span>impl&lt;B: Backend&gt; Model&lt;B&gt; {
    pub fn forward_classification(
        &amp;self,
        images: Tensor&lt;B, 3&gt;,
        targets: Tensor&lt;B, 1, Int&gt;,
    ) -&gt; ClassificationOutput&lt;B&gt; {
        let output = self.forward(images);
        let loss = CrossEntropyLossConfig::new()
            .init(&amp;output.device())
            .forward(output.clone(), targets.clone());

        ClassificationOutput::new(loss, output, targets)
    }
}</code></pre>
<p>As evident from the preceding code block, we employ the cross-entropy loss module for loss
calculation, without the inclusion of any padding token. We then return the classification output
containing the loss, the output tensor with all logits and the targets.</p>
<p>Please take note that tensor operations receive owned tensors as input. For reusing a tensor
multiple times, you need to use the <code>clone()</code> function. There's no need to worry; this process won't
involve actual copying of the tensor data. Instead, it will simply indicate that the tensor is
employed in multiple instances, implying that certain operations won't be performed in place. In
summary, our API has been designed with owned tensors to optimize performance.</p>
<p>Moving forward, we will proceed with the implementation of both the training and validation steps
for our model.</p>
<pre><code class="language-rust   ignore"><span class="boring">use crate::{
</span><span class="boring">    data::{MnistBatch, MnistBatcher},
</span><span class="boring">    model::{Model, ModelConfig},
</span><span class="boring">};
</span><span class="boring">use mabor::{
</span><span class="boring">    data::{dataloader::DataLoaderBuilder, dataset::vision::MnistDataset},
</span><span class="boring">    nn::loss::CrossEntropyLossConfig,
</span><span class="boring">    optim::AdamConfig,
</span><span class="boring">    prelude::*,
</span><span class="boring">    record::CompactRecorder,
</span><span class="boring">    tensor::backend::AutodiffBackend,
</span><span class="boring">    train::{
</span><span class="boring">        metric::{AccuracyMetric, LossMetric},
</span><span class="boring">        ClassificationOutput, LearnerBuilder, TrainOutput, TrainStep, ValidStep,
</span><span class="boring">    },
</span><span class="boring">};
</span><span class="boring">
</span><span class="boring">impl&lt;B: Backend&gt; Model&lt;B&gt; {
</span><span class="boring">    pub fn forward_classification(
</span><span class="boring">        &amp;self,
</span><span class="boring">        images: Tensor&lt;B, 3&gt;,
</span><span class="boring">        targets: Tensor&lt;B, 1, Int&gt;,
</span><span class="boring">    ) -&gt; ClassificationOutput&lt;B&gt; {
</span><span class="boring">        let output = self.forward(images);
</span><span class="boring">        let loss = CrossEntropyLossConfig::new()
</span><span class="boring">            .init(&amp;output.device())
</span><span class="boring">            .forward(output.clone(), targets.clone());
</span><span class="boring">
</span><span class="boring">        ClassificationOutput::new(loss, output, targets)
</span><span class="boring">    }
</span><span class="boring">}
</span><span class="boring">
</span>impl&lt;B: AutodiffBackend&gt; TrainStep&lt;MnistBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
    fn step(&amp;self, batch: MnistBatch&lt;B&gt;) -&gt; TrainOutput&lt;ClassificationOutput&lt;B&gt;&gt; {
        let item = self.forward_classification(batch.images, batch.targets);

        TrainOutput::new(self, item.loss.backward(), item)
    }
}

impl&lt;B: Backend&gt; ValidStep&lt;MnistBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
    fn step(&amp;self, batch: MnistBatch&lt;B&gt;) -&gt; ClassificationOutput&lt;B&gt; {
        self.forward_classification(batch.images, batch.targets)
    }
}</code></pre>
<p>Here we define the input and output types as generic arguments in the <code>TrainStep</code> and <code>ValidStep</code>.
We will call them <code>MnistBatch</code> and <code>ClassificationOutput</code>. In the training step, the computation of
gradients is straightforward, necessitating a simple invocation of <code>backward()</code> on the loss. Note
that contrary to PyTorch, gradients are not stored alongside each tensor parameter, but are rather
returned by the backward pass, as such: <code>let gradients = loss.backward();</code>. The gradient of a
parameter can be obtained with the grad function: <code>let grad = tensor.grad(&amp;gradients);</code>. Although it
is not necessary when using the learner struct and the optimizers, it can prove to be quite useful
when debugging or writing custom training loops. One of the differences between the training and the
validation steps is that the former requires the backend to implement <code>AutodiffBackend</code> and not just
<code>Backend</code>. Otherwise, the <code>backward</code> function is not available, as the backend does not support
autodiff. We will see later how to create a backend with autodiff support.</p>
<details>
<summary><strong>🦀 Generic Type Constraints in Method Definitions</strong></summary>
<p>Although generic data types, trait and trait bounds were already introduced in previous sections of
this guide, the previous code snippet might be a lot to take in at first.</p>
<p>In the example above, we implement the <code>TrainStep</code> and <code>ValidStep</code> trait for our <code>Model</code> struct,
which is generic over the <code>Backend</code> trait as has been covered before. These traits are provided by
<code>mabor::train</code> and define a common <code>step</code> method that should be implemented for all structs. Since
the trait is generic over the input and output types, the trait implementation must specify the
concrete types used. This is where the additional type constraints appear
<code>&lt;MnistBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt;</code>. As we saw previously, the concrete input type for the
batch is <code>MnistBatch</code>, and the output of the forward pass is <code>ClassificationOutput</code>. The <code>step</code>
method signature matches the concrete input and output types.</p>
<p>For more details specific to constraints on generic types when defining methods, take a look at
<a href="https://doc.rust-lang.org/book/ch10-01-syntax.html#in-method-definitions">this section</a> of the Rust
Book.</p>
</details><br>
<p>Let us move on to establishing the practical training configuration.</p>
<pre><code class="language-rust   ignore"><span class="boring">use crate::{
</span><span class="boring">    data::{MnistBatch, MnistBatcher},
</span><span class="boring">    model::{Model, ModelConfig},
</span><span class="boring">};
</span><span class="boring">use mabor::{
</span><span class="boring">    data::{dataloader::DataLoaderBuilder, dataset::vision::MnistDataset},
</span><span class="boring">    nn::loss::CrossEntropyLossConfig,
</span><span class="boring">    optim::AdamConfig,
</span><span class="boring">    prelude::*,
</span><span class="boring">    record::CompactRecorder,
</span><span class="boring">    tensor::backend::AutodiffBackend,
</span><span class="boring">    train::{
</span><span class="boring">        metric::{AccuracyMetric, LossMetric},
</span><span class="boring">        ClassificationOutput, LearnerBuilder, TrainOutput, TrainStep, ValidStep,
</span><span class="boring">    },
</span><span class="boring">};
</span><span class="boring">
</span><span class="boring">impl&lt;B: Backend&gt; Model&lt;B&gt; {
</span><span class="boring">    pub fn forward_classification(
</span><span class="boring">        &amp;self,
</span><span class="boring">        images: Tensor&lt;B, 3&gt;,
</span><span class="boring">        targets: Tensor&lt;B, 1, Int&gt;,
</span><span class="boring">    ) -&gt; ClassificationOutput&lt;B&gt; {
</span><span class="boring">        let output = self.forward(images);
</span><span class="boring">        let loss = CrossEntropyLossConfig::new()
</span><span class="boring">            .init(&amp;output.device())
</span><span class="boring">            .forward(output.clone(), targets.clone());
</span><span class="boring">
</span><span class="boring">        ClassificationOutput::new(loss, output, targets)
</span><span class="boring">    }
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl&lt;B: AutodiffBackend&gt; TrainStep&lt;MnistBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
</span><span class="boring">    fn step(&amp;self, batch: MnistBatch&lt;B&gt;) -&gt; TrainOutput&lt;ClassificationOutput&lt;B&gt;&gt; {
</span><span class="boring">        let item = self.forward_classification(batch.images, batch.targets);
</span><span class="boring">
</span><span class="boring">        TrainOutput::new(self, item.loss.backward(), item)
</span><span class="boring">    }
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl&lt;B: Backend&gt; ValidStep&lt;MnistBatch&lt;B&gt;, ClassificationOutput&lt;B&gt;&gt; for Model&lt;B&gt; {
</span><span class="boring">    fn step(&amp;self, batch: MnistBatch&lt;B&gt;) -&gt; ClassificationOutput&lt;B&gt; {
</span><span class="boring">        self.forward_classification(batch.images, batch.targets)
</span><span class="boring">    }
</span><span class="boring">}
</span><span class="boring">
</span>#[derive(Config)]
pub struct TrainingConfig {
    pub model: ModelConfig,
    pub optimizer: AdamConfig,
    #[config(default = 10)]
    pub num_epochs: usize,
    #[config(default = 64)]
    pub batch_size: usize,
    #[config(default = 4)]
    pub num_workers: usize,
    #[config(default = 42)]
    pub seed: u64,
    #[config(default = 1.0e-4)]
    pub learning_rate: f64,
}

fn create_artifact_dir(artifact_dir: &amp;str) {
    // Remove existing artifacts before to get an accurate learner summary
    std::fs::remove_dir_all(artifact_dir).ok();
    std::fs::create_dir_all(artifact_dir).ok();
}

pub fn train&lt;B: AutodiffBackend&gt;(artifact_dir: &amp;str, config: TrainingConfig, device: B::Device) {
    create_artifact_dir(artifact_dir);
    config
        .save(format!("{artifact_dir}/config.json"))
        .expect("Config should be saved successfully");

    B::seed(config.seed);

    let batcher = MnistBatcher::default();

    let dataloader_train = DataLoaderBuilder::new(batcher.clone())
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MnistDataset::train());

    let dataloader_test = DataLoaderBuilder::new(batcher)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MnistDataset::test());

    let learner = LearnerBuilder::new(artifact_dir)
        .metric_train_numeric(AccuracyMetric::new())
        .metric_valid_numeric(AccuracyMetric::new())
        .metric_train_numeric(LossMetric::new())
        .metric_valid_numeric(LossMetric::new())
        .with_file_checkpointer(CompactRecorder::new())
        .devices(vec![device.clone()])
        .num_epochs(config.num_epochs)
        .summary()
        .build(
            config.model.init::&lt;B&gt;(&amp;device),
            config.optimizer.init(),
            config.learning_rate,
        );

    let model_trained = learner.fit(dataloader_train, dataloader_test);

    model_trained
        .save_file(format!("{artifact_dir}/model"), &amp;CompactRecorder::new())
        .expect("Trained model should be saved successfully");
}</code></pre>
<p>It is a good practice to use the <code>Config</code> derive to create the experiment configuration. In the
<code>train</code> function, the first thing we are doing is making sure the <code>artifact_dir</code> exists, using the
standard rust library for file manipulation. All checkpoints, logging and metrics will be stored
under this directory. We initialize the dataloaders using the previously created batcher. Since no
automatic differentiation is needed during the validation phase, the <code>learner.fit(...)</code> method
defines the necessary backend bounds on the data loader for <code>B::InnerBackend</code> (see
<a href="basic-workflow/backend.html">Backend</a>). The autodiff capabilities are available through a type system, making it
nearly impossible to forget to deactivate gradient calculation.</p>
<p>Next, we create our learner with the accuracy and loss metric on both training and validation steps
along with the device and the epoch. We also configure the checkpointer using the <code>CompactRecorder</code>
to indicate how weights should be stored. This struct implements the <code>Recorder</code> trait, which makes
it capable of saving records for persistency.</p>
<p>We then build the learner with the model, the optimizer and the learning rate. Notably, the third
argument of the build function should actually be a learning rate <em>scheduler</em>. When provided with a
float as in our example, it is automatically transformed into a <em>constant</em> learning rate scheduler.
The learning rate is not part of the optimizer config as it is often done in other frameworks, but
rather passed as a parameter when executing the optimizer step. This avoids having to mutate the
state of the optimizer and is therefore more functional. It makes no difference when using the
learner struct, but it will be an essential nuance to grasp if you implement your own training loop.</p>
<p>Once the learner is created, we can simply call <code>fit</code> and provide the training and validation
dataloaders. For the sake of simplicity in this example, we employ the test set as the validation
set; however, we do not recommend this practice for actual usage.</p>
<p>Finally, the trained model is returned by the <code>fit</code> method. The trained weights are then saved using
the <code>CompactRecorder</code>. This recorder employs the <code>MessagePack</code> format with half precision, <code>f16</code> for
floats and <code>i16</code> for integers. Other recorders are available, offering support for various formats,
such as <code>BinCode</code> and <code>JSON</code>, with or without compression. Any backend, regardless of precision, can
load recorded data of any kind.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend"><a class="header" href="#backend">Backend</a></h1>
<p>We have effectively written most of the necessary code to train our model. However, we have not
explicitly designated the backend to be used at any point. This will be defined in the main
entrypoint of our program, namely the <code>main</code> function defined in <code>src/main.rs</code>.</p>
<pre><code class="language-rust   ignore"><span class="boring">#![recursion_limit = "256"]
</span><span class="boring">mod data;
</span><span class="boring">mod model;
</span><span class="boring">mod training;
</span><span class="boring">
</span>use crate::{model::ModelConfig, training::TrainingConfig};
use mabor::{
    backend::{Autodiff, Wgpu},
<span class="boring">    data::dataset::Dataset,
</span>    optim::AdamConfig,
};

fn main() {
    type MyBackend = Wgpu&lt;f32, i32&gt;;
    type MyAutodiffBackend = Autodiff&lt;MyBackend&gt;;

    let device = mabor::backend::wgpu::WgpuDevice::default();
    let artifact_dir = "/tmp/guide";
    crate::training::train::&lt;MyAutodiffBackend&gt;(
        artifact_dir,
        TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),
        device.clone(),
    );
}</code></pre>
<p>In this code snippet, we use the <code>Wgpu</code> backend which is compatible with any operating system and will
use the GPU. For other options, see the Mabor README. This backend type takes the graphics API, the
float type and the int type as generic arguments that will be used during the training. The autodiff
backend is simply the same backend, wrapped within the <code>Autodiff</code> struct which imparts differentiability
to any backend.</p>
<p>We call the <code>train</code> function defined earlier with a directory for artifacts, the configuration of
the model (the number of digit classes is 10 and the hidden dimension is 512), the optimizer
configuration which in our case will be the default Adam configuration, and the device which can be
obtained from the backend.</p>
<p>You can now train your freshly created model with the command:</p>
<pre><code class="language-console">cargo run --release
</code></pre>
<p>When running your project with the command above, you should see the training progression through a
basic CLI dashboard:</p>
<img title="a title" alt="Alt text" src="basic-workflow/training-output.png">
<div style="break-before: page; page-break-before: always;"></div><h1 id="inference"><a class="header" href="#inference">Inference</a></h1>
<p>Now that we have trained our model, the next natural step is to use it for inference.</p>
<p>You need two things in order to load weights for a model: the model's record and the model's config.
Since parameters in Mabor are lazy initialized, no allocation and GPU/CPU kernels are executed by the
<code>ModelConfig::init</code> function. The weights are initialized when used for the first time, therefore
you can safely use <code>config.init(device).load_record(record)</code> without any meaningful performance
cost. Let's create a simple <code>infer</code> method in a new file <code>src/inference.rs</code> which we will use to
load our trained model.</p>
<pre><code class="language-rust   ignore"><span class="boring">use crate::{data::MnistBatcher, training::TrainingConfig};
</span><span class="boring">use mabor::{
</span><span class="boring">    data::{dataloader::batcher::Batcher, dataset::vision::MnistItem},
</span><span class="boring">    prelude::*,
</span><span class="boring">    record::{CompactRecorder, Recorder},
</span><span class="boring">};
</span><span class="boring">
</span>pub fn infer&lt;B: Backend&gt;(artifact_dir: &amp;str, device: B::Device, item: MnistItem) {
    let config = TrainingConfig::load(format!("{artifact_dir}/config.json"))
        .expect("Config should exist for the model; run train first");
    let record = CompactRecorder::new()
        .load(format!("{artifact_dir}/model").into(), &amp;device)
        .expect("Trained model should exist; run train first");

    let model = config.model.init::&lt;B&gt;(&amp;device).load_record(record);

    let label = item.label;
    let batcher = MnistBatcher::default();
    let batch = batcher.batch(vec![item], &amp;device);
    let output = model.forward(batch.images);
    let predicted = output.argmax(1).flatten::&lt;1&gt;(0, 1).into_scalar();

    println!("Predicted {predicted} Expected {label}");
}</code></pre>
<p>The first step is to load the configuration of the training to fetch the correct model
configuration. Then we can fetch the record using the same recorder as we used during training.
Finally we can init the model with the configuration and the record. For simplicity we can use the
same batcher used during the training to pass from a MnistItem to a tensor.</p>
<p>By running the infer function, you should see the predictions of your model!</p>
<p>Add the call to <code>infer</code> to the <code>main.rs</code> file after the <code>train</code> function call:</p>
<pre><code class="language-rust   ignore"><span class="boring">#![recursion_limit = "256"]
</span><span class="boring">mod data;
</span><span class="boring">mod inference;
</span><span class="boring">mod model;
</span><span class="boring">mod training;
</span><span class="boring">
</span><span class="boring">use crate::{model::ModelConfig, training::TrainingConfig};
</span><span class="boring">use mabor::{
</span><span class="boring">    backend::{Autodiff, Wgpu},
</span><span class="boring">    data::dataset::Dataset,
</span><span class="boring">    optim::AdamConfig,
</span><span class="boring">};
</span><span class="boring">
</span><span class="boring">fn main() {
</span><span class="boring">    type MyBackend = Wgpu&lt;f32, i32&gt;;
</span><span class="boring">    type MyAutodiffBackend = Autodiff&lt;MyBackend&gt;;
</span><span class="boring">
</span><span class="boring">    let device = mabor::backend::wgpu::WgpuDevice::default();
</span><span class="boring">    let artifact_dir = "/tmp/guide";
</span><span class="boring">    crate::training::train::&lt;MyAutodiffBackend&gt;(
</span><span class="boring">        artifact_dir,
</span><span class="boring">        TrainingConfig::new(ModelConfig::new(10, 512), AdamConfig::new()),
</span><span class="boring">        device.clone(),
</span><span class="boring">    );
</span>    crate::inference::infer::&lt;MyBackend&gt;(
        artifact_dir,
        device,
        mabor::data::dataset::vision::MnistDataset::test()
            .get(42)
            .unwrap(),
    );
<span class="boring">}</span></code></pre>
<p>The number <code>42</code> is the index of the image in the MNIST dataset. You can explore and verify them using
this <a href="https://observablehq.com/@davidalber/mnist-viewer">MNIST viewer</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h1>
<p>In this short guide, we've introduced you to the fundamental building blocks for getting started
with Mabor. While there's still plenty to explore, our goal has been to provide you with the
essential knowledge to kickstart your productivity within the framework.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-blocks"><a class="header" href="#building-blocks">Building Blocks</a></h1>
<p>In this section, we'll guide you through the core elements that make up Mabor. We'll walk you through
the key components that serve as the building blocks of the framework and your future projects.</p>
<p>As you explore Mabor, you might notice that we occasionally draw comparisons to PyTorch. We believe
it can provide a smoother learning curve and help you grasp the nuances more effectively.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-1"><a class="header" href="#backend-1">Backend</a></h1>
<p>Nearly everything in Mabor is based on the <code>Backend</code> trait, which enables you to run tensor
operations using different implementations without having to modify your code. While a backend may
not necessarily have autodiff capabilities, the <code>AutodiffBackend</code> trait specifies when autodiff is
needed. This trait not only abstracts operations but also tensor, device, and element types,
providing each backend the flexibility they need. It's worth noting that the trait assumes eager
mode since mabor fully supports dynamic graphs. However, we may create another API to assist with
integrating graph-based backends, without requiring any changes to the user's code.</p>
<p>Users are not expected to directly use the backend trait methods, as it is primarily designed with
backend developers in mind rather than Mabor users. Therefore, most Mabor userland APIs are generic
across backends. This approach helps users discover the API more organically with proper
autocomplete and documentation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensor"><a class="header" href="#tensor">Tensor</a></h1>
<p>As previously explained in the <a href="basic-workflow/model.html">model section</a>, the Tensor struct has 3
generic arguments: the backend B, the dimensionality D, and the data type.</p>
<pre><code class="language-rust   ignore">Tensor&lt;B, D&gt;           // Float tensor (default)
Tensor&lt;B, D, Float&gt;    // Explicit float tensor
Tensor&lt;B, D, Int&gt;      // Int tensor
Tensor&lt;B, D, Bool&gt;     // Bool tensor</code></pre>
<p>Note that the specific element types used for <code>Float</code>, <code>Int</code>, and <code>Bool</code> tensors are defined by
backend implementations.</p>
<p>Mabor Tensors are defined by the number of dimensions D in its declaration as opposed to its shape.
The actual shape of the tensor is inferred from its initialization. For example, a Tensor of size
(5,) is initialized as below:</p>
<pre><code class="language-rust  ignore">let floats = [1.0, 2.0, 3.0, 4.0, 5.0];

// Get the default device
let device = Default::default();

// correct: Tensor is 1-Dimensional with 5 elements
let tensor_1 = Tensor::&lt;Backend, 1&gt;::from_floats(floats, &amp;device);

// incorrect: let tensor_1 = Tensor::&lt;Backend, 5&gt;::from_floats(floats, &amp;device);
// this will lead to an error and is for creating a 5-D tensor</code></pre>
<h3 id="initialization"><a class="header" href="#initialization">Initialization</a></h3>
<p>Mabor Tensors are primarily initialized using the <code>from_data()</code> method which takes the <code>TensorData</code>
struct as input. The <code>TensorData</code> struct has two public fields: <code>shape</code> and <code>dtype</code>. The <code>value</code>,
now stored as bytes, is private but can be accessed via any of the following methods: <code>as_slice</code>,
<code>as_mut_slice</code>, <code>to_vec</code> and <code>iter</code>. To retrieve the data from a tensor, the method <code>.to_data()</code>
should be employed when intending to reuse the tensor afterward. Alternatively, <code>.into_data()</code> is
recommended for one-time use. Let's look at a couple of examples for initializing a tensor from
different inputs.</p>
<pre><code class="language-rust  ignore">
// Initialization from a given Backend (Wgpu)
let tensor_1 = Tensor::&lt;Wgpu, 1&gt;::from_data([1.0, 2.0, 3.0], &amp;device);

// Initialization from a generic Backend
let tensor_2 = Tensor::&lt;Backend, 1&gt;::from_data(TensorData::from([1.0, 2.0, 3.0]), &amp;device);

// Initialization using from_floats (Recommended for f32 ElementType)
// Will be converted to TensorData internally.
let tensor_3 = Tensor::&lt;Backend, 1&gt;::from_floats([1.0, 2.0, 3.0], &amp;device);

// Initialization of Int Tensor from array slices
let arr: [i32; 6] = [1, 2, 3, 4, 5, 6];
let tensor_4 = Tensor::&lt;Backend, 1, Int&gt;::from_data(TensorData::from(&amp;arr[0..3]), &amp;device);

// Initialization from a custom type

struct BodyMetrics {
    age: i8,
    height: i16,
    weight: f32
}

let bmi = BodyMetrics{
        age: 25,
        height: 180,
        weight: 80.0
    };
let data  = TensorData::from([bmi.age as f32, bmi.height as f32, bmi.weight]);
let tensor_5 = Tensor::&lt;Backend, 1&gt;::from_data(data, &amp;device);
</code></pre>
<h2 id="ownership-and-cloning"><a class="header" href="#ownership-and-cloning">Ownership and Cloning</a></h2>
<p>Almost all Mabor operations take ownership of the input tensors. Therefore, reusing a tensor multiple
times will necessitate cloning it. Let's look at an example to understand the ownership rules and
cloning better. Suppose we want to do a simple min-max normalization of an input tensor.</p>
<pre><code class="language-rust  ignore">let input = Tensor::&lt;Wgpu, 1&gt;::from_floats([1.0, 2.0, 3.0, 4.0], &amp;device);
let min = input.min();
let max = input.max();
let input = (input - min).div(max - min);</code></pre>
<p>With PyTorch tensors, the above code would work as expected. However, Rust's strict ownership rules
will give an error and prevent using the input tensor after the first <code>.min()</code> operation. The
ownership of the input tensor is transferred to the variable <code>min</code> and the input tensor is no longer
available for further operations. Mabor Tensors like most complex primitives do not implement the
<code>Copy</code> trait and therefore have to be cloned explicitly. Now let's rewrite a working example of
doing min-max normalization with cloning.</p>
<pre><code class="language-rust  ignore">let input = Tensor::&lt;Wgpu, 1&gt;::from_floats([1.0, 2.0, 3.0, 4.0], &amp;device);
let min = input.clone().min();
let max = input.clone().max();
let input = (input.clone() - min.clone()).div(max - min);
println!("{}", input.to_data());// Success: [0.0, 0.33333334, 0.6666667, 1.0]

// Notice that max, min have been moved in last operation so
// the below print will give an error.
// If we want to use them for further operations,
// they will need to be cloned in similar fashion.
// println!("{:?}", min.to_data());</code></pre>
<p>We don't need to be worried about memory overhead because with cloning, the tensor's buffer isn't
copied, and only a reference to it is increased. This makes it possible to determine exactly how
many times a tensor is used, which is very convenient for reusing tensor buffers or even fusing
operations into a single kernel (<a href="https://burn.dev/docs/burn_fusion/index.htmls">mabor-fusion</a>). For
that reason, we don't provide explicit inplace operations. If a tensor is used only one time,
inplace operations will always be used when available.</p>
<h2 id="tensor-operations"><a class="header" href="#tensor-operations">Tensor Operations</a></h2>
<p>Normally with PyTorch, explicit inplace operations aren't supported during the backward pass, making
them useful only for data preprocessing or inference-only model implementations. With Mabor, you can
focus more on <em>what</em> the model should do, rather than on <em>how</em> to do it. We take the responsibility
of making your code run as fast as possible during training as well as inference. The same
principles apply to broadcasting; all operations support broadcasting unless specified otherwise.</p>
<p>Here, we provide a list of all supported operations along with their PyTorch equivalents. Note that
for the sake of simplicity, we ignore type signatures. For more details, refer to the
<a href="https://docs.rs/burn/latest/burn/tensor/struct.Tensor.html">full documentation</a>.</p>
<h3 id="basic-operations"><a class="header" href="#basic-operations">Basic Operations</a></h3>
<p>Those operations are available for all tensor kinds: <code>Int</code>, <code>Float</code>, and <code>Bool</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Tensor::cat(tensors, dim)</code></td><td><code>torch.cat(tensors, dim)</code></td></tr>
<tr><td><code>Tensor::empty(shape, device)</code></td><td><code>torch.empty(shape, device=device)</code></td></tr>
<tr><td><code>Tensor::from_primitive(primitive)</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::stack(tensors, dim)</code></td><td><code>torch.stack(tensors, dim)</code></td></tr>
<tr><td><code>tensor.all()</code></td><td><code>tensor.all()</code></td></tr>
<tr><td><code>tensor.all_dim(dim)</code></td><td><code>tensor.all(dim)</code></td></tr>
<tr><td><code>tensor.any()</code></td><td><code>tensor.any()</code></td></tr>
<tr><td><code>tensor.any_dim(dim)</code></td><td><code>tensor.any(dim)</code></td></tr>
<tr><td><code>tensor.chunk(num_chunks, dim)</code></td><td><code>tensor.chunk(num_chunks, dim)</code></td></tr>
<tr><td><code>tensor.split(split_size, dim)</code></td><td><code>tensor.split(split_size, dim)</code></td></tr>
<tr><td><code>tensor.split_with_sizes(split_sizes, dim)</code></td><td><code>tensor.split([split_sizes], dim)</code></td></tr>
<tr><td><code>tensor.device()</code></td><td><code>tensor.device</code></td></tr>
<tr><td><code>tensor.dtype()</code></td><td><code>tensor.dtype</code></td></tr>
<tr><td><code>tensor.dims()</code></td><td><code>tensor.size()</code></td></tr>
<tr><td><code>tensor.equal(other)</code></td><td><code>x == y</code></td></tr>
<tr><td><code>tensor.expand(shape)</code></td><td><code>tensor.expand(shape)</code></td></tr>
<tr><td><code>tensor.flatten(start_dim, end_dim)</code></td><td><code>tensor.flatten(start_dim, end_dim)</code></td></tr>
<tr><td><code>tensor.flip(axes)</code></td><td><code>tensor.flip(axes)</code></td></tr>
<tr><td><code>tensor.into_data()</code></td><td>N/A</td></tr>
<tr><td><code>tensor.into_primitive()</code></td><td>N/A</td></tr>
<tr><td><code>tensor.into_scalar()</code></td><td><code>tensor.item()</code></td></tr>
<tr><td><code>tensor.narrow(dim, start, length)</code></td><td><code>tensor.narrow(dim, start, length)</code></td></tr>
<tr><td><code>tensor.not_equal(other)</code></td><td><code>x != y</code></td></tr>
<tr><td><code>tensor.permute(axes)</code></td><td><code>tensor.permute(axes)</code></td></tr>
<tr><td><code>tensor.movedim(src, dst)</code></td><td><code>tensor.movedim(src, dst)</code></td></tr>
<tr><td><code>tensor.repeat_dim(dim, times)</code></td><td><code>tensor.repeat(*[times if i == dim else 1 for i in range(tensor.dim())])</code></td></tr>
<tr><td><code>tensor.repeat(sizes)</code></td><td><code>tensor.repeat(sizes)</code></td></tr>
<tr><td><code>tensor.reshape(shape)</code></td><td><code>tensor.view(shape)</code></td></tr>
<tr><td><code>tensor.roll(shfts, dims)</code></td><td><code>tensor.roll(shifts, dims)</code></td></tr>
<tr><td><code>tensor.roll_dim(shift, dim)</code></td><td><code>tensor.roll([shift], [dim])</code></td></tr>
<tr><td><code>tensor.shape()</code></td><td><code>tensor.shape</code></td></tr>
<tr><td><code>tensor.slice(ranges)</code></td><td><code>tensor[(*ranges,)]</code></td></tr>
<tr><td><code>tensor.slice_assign(ranges, values)</code></td><td><code>tensor[(*ranges,)] = values</code></td></tr>
<tr><td><code>tensor.slice_fill(ranges, value)</code></td><td><code>tensor[(*ranges,)] = value</code></td></tr>
<tr><td><code>tensor.slice_dim(dim, range)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.squeeze(dim)</code></td><td><code>tensor.squeeze(dim)</code></td></tr>
<tr><td><code>tensor.swap_dims(dim1, dim2)</code></td><td><code>tensor.transpose(dim1, dim2)</code></td></tr>
<tr><td><code>tensor.to_data()</code></td><td>N/A</td></tr>
<tr><td><code>tensor.to_device(device)</code></td><td><code>tensor.to(device)</code></td></tr>
<tr><td><code>tensor.transpose()</code></td><td><code>tensor.T</code></td></tr>
<tr><td><code>tensor.unsqueeze()</code></td><td><code>tensor.unsqueeze(0)</code></td></tr>
<tr><td><code>tensor.unsqueeze_dim(dim)</code></td><td><code>tensor.unsqueeze(dim)</code></td></tr>
<tr><td><code>tensor.unsqueeze_dims(dims)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h3 id="numeric-operations"><a class="header" href="#numeric-operations">Numeric Operations</a></h3>
<p>Those operations are available for numeric tensor kinds: <code>Float</code> and <code>Int</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Tensor::eye(size, device)</code></td><td><code>torch.eye(size, device=device)</code></td></tr>
<tr><td><code>Tensor::full(shape, fill_value, device)</code></td><td><code>torch.full(shape, fill_value, device=device)</code></td></tr>
<tr><td><code>Tensor::ones(shape, device)</code></td><td><code>torch.ones(shape, device=device)</code></td></tr>
<tr><td><code>Tensor::zeros(shape, device)</code></td><td><code>torch.zeros(shape, device=device)</code></td></tr>
<tr><td><code>tensor.abs()</code></td><td><code>torch.abs(tensor)</code></td></tr>
<tr><td><code>tensor.add(other)</code> or <code>tensor + other</code></td><td><code>tensor + other</code></td></tr>
<tr><td><code>tensor.add_scalar(scalar)</code> or <code>tensor + scalar</code></td><td><code>tensor + scalar</code></td></tr>
<tr><td><code>tensor.all_close(other, atol, rtol)</code></td><td><code>torch.allclose(tensor, other, atol, rtol)</code></td></tr>
<tr><td><code>tensor.argmax(dim)</code></td><td><code>tensor.argmax(dim)</code></td></tr>
<tr><td><code>tensor.argmin(dim)</code></td><td><code>tensor.argmin(dim)</code></td></tr>
<tr><td><code>tensor.argsort(dim)</code></td><td><code>tensor.argsort(dim)</code></td></tr>
<tr><td><code>tensor.argsort_descending(dim)</code></td><td><code>tensor.argsort(dim, descending=True)</code></td></tr>
<tr><td><code>tensor.bool()</code></td><td><code>tensor.bool()</code></td></tr>
<tr><td><code>tensor.clamp(min, max)</code></td><td><code>torch.clamp(tensor, min=min, max=max)</code></td></tr>
<tr><td><code>tensor.clamp_max(max)</code></td><td><code>torch.clamp(tensor, max=max)</code></td></tr>
<tr><td><code>tensor.clamp_min(min)</code></td><td><code>torch.clamp(tensor, min=min)</code></td></tr>
<tr><td><code>tensor.contains_nan()</code></td><td>N/A</td></tr>
<tr><td><code>tensor.div(other)</code> or <code>tensor / other</code></td><td><code>tensor / other</code></td></tr>
<tr><td><code>tensor.div_scalar(scalar)</code> or <code>tensor / scalar</code></td><td><code>tensor / scalar</code></td></tr>
<tr><td><code>tensor.equal_elem(other)</code></td><td><code>tensor.eq(other)</code></td></tr>
<tr><td><code>tensor.full_like(fill_value)</code></td><td>`torch.full_like(tensor, fill_value)</td></tr>
<tr><td><code>tensor.gather(dim, indices)</code></td><td><code>torch.gather(tensor, dim, indices)</code></td></tr>
<tr><td><code>tensor.greater(other)</code></td><td><code>tensor.gt(other)</code></td></tr>
<tr><td><code>tensor.greater_elem(scalar)</code></td><td><code>tensor.gt(scalar)</code></td></tr>
<tr><td><code>tensor.greater_equal(other)</code></td><td><code>tensor.ge(other)</code></td></tr>
<tr><td><code>tensor.greater_equal_elem(scalar)</code></td><td><code>tensor.ge(scalar)</code></td></tr>
<tr><td><code>tensor.lower(other)</code></td><td><code>tensor.lt(other)</code></td></tr>
<tr><td><code>tensor.lower_elem(scalar)</code></td><td><code>tensor.lt(scalar)</code></td></tr>
<tr><td><code>tensor.lower_equal(other)</code></td><td><code>tensor.le(other)</code></td></tr>
<tr><td><code>tensor.lower_equal_elem(scalar)</code></td><td><code>tensor.le(scalar)</code></td></tr>
<tr><td><code>tensor.mask_fill(mask, value)</code></td><td><code>tensor.masked_fill(mask, value)</code></td></tr>
<tr><td><code>tensor.mask_where(mask, value_tensor)</code></td><td><code>torch.where(mask, value_tensor, tensor)</code></td></tr>
<tr><td><code>tensor.max()</code></td><td><code>tensor.max()</code></td></tr>
<tr><td><code>tensor.max_abs()</code></td><td><code>tensor.abs().max()</code></td></tr>
<tr><td><code>tensor.max_abs_dim(dim)</code></td><td><code>tensor.abs().max(dim, keepdim=True)</code></td></tr>
<tr><td><code>tensor.max_dim(dim)</code></td><td><code>tensor.max(dim, keepdim=True)</code></td></tr>
<tr><td><code>tensor.max_dim_with_indices(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.max_pair(other)</code></td><td><code>torch.Tensor.max(a,b)</code></td></tr>
<tr><td><code>tensor.mean()</code></td><td><code>tensor.mean()</code></td></tr>
<tr><td><code>tensor.mean_dim(dim)</code></td><td><code>tensor.mean(dim, keepdim=True)</code></td></tr>
<tr><td><code>tensor.min()</code></td><td><code>tensor.min()</code></td></tr>
<tr><td><code>tensor.min_dim(dim)</code></td><td><code>tensor.min(dim, keepdim=True)</code></td></tr>
<tr><td><code>tensor.min_dim_with_indices(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.min_pair(other)</code></td><td><code>torch.Tensor.min(a,b)</code></td></tr>
<tr><td><code>tensor.mul(other)</code> or <code>tensor * other</code></td><td><code>tensor * other</code></td></tr>
<tr><td><code>tensor.mul_scalar(scalar)</code> or <code>tensor * scalar</code></td><td><code>tensor * scalar</code></td></tr>
<tr><td><code>tensor.neg()</code> or <code>-tensor</code></td><td><code>-tensor</code></td></tr>
<tr><td><code>tensor.not_equal_elem(scalar)</code></td><td><code>tensor.ne(scalar)</code></td></tr>
<tr><td><code>tensor.ones_like()</code></td><td><code>torch.ones_like(tensor)</code></td></tr>
<tr><td><code>tensor.one_hot(num_classes)</code></td><td><code>torch.nn.functional.one_hot</code></td></tr>
<tr><td><code>tensor.one_hot_fill(num_classes, on_value, off_value, axis)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.pad(pads, value)</code></td><td><code>torch.nn.functional.pad(input, pad, value)</code></td></tr>
<tr><td><code>tensor.powf(other)</code> or <code>tensor.powi(intother)</code></td><td><code>tensor.pow(other)</code></td></tr>
<tr><td><code>tensor.powf_scalar(scalar)</code> or <code>tensor.powi_scalar(intscalar)</code></td><td><code>tensor.pow(scalar)</code></td></tr>
<tr><td><code>tensor.prod()</code></td><td><code>tensor.prod()</code></td></tr>
<tr><td><code>tensor.prod_dim(dim)</code></td><td><code>tensor.prod(dim, keepdim=True)</code></td></tr>
<tr><td><code>tensor.rem(other)</code> or <code>tensor % other</code></td><td><code>tensor % other</code></td></tr>
<tr><td><code>tensor.scatter(dim, indices, values)</code></td><td><code>tensor.scatter_add(dim, indices, values)</code></td></tr>
<tr><td><code>tensor.select(dim, indices)</code></td><td><code>tensor.index_select(dim, indices)</code></td></tr>
<tr><td><code>tensor.select_assign(dim, indices, values)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.sign()</code></td><td><code>tensor.sign()</code></td></tr>
<tr><td><code>tensor.sort(dim)</code></td><td><code>tensor.sort(dim).values</code></td></tr>
<tr><td><code>tensor.sort_descending(dim)</code></td><td><code>tensor.sort(dim, descending=True).values</code></td></tr>
<tr><td><code>tensor.sort_descending_with_indices(dim)</code></td><td><code>tensor.sort(dim, descending=True)</code></td></tr>
<tr><td><code>tensor.sort_with_indices(dim)</code></td><td><code>tensor.sort(dim)</code></td></tr>
<tr><td><code>tensor.sub(other)</code> or <code>tensor - other</code></td><td><code>tensor - other</code></td></tr>
<tr><td><code>tensor.sub_scalar(scalar)</code> or <code>tensor - scalar</code></td><td><code>tensor - scalar</code></td></tr>
<tr><td><code>scalar - tensor</code></td><td><code>scalar - tensor</code></td></tr>
<tr><td><code>tensor.sum()</code></td><td><code>tensor.sum()</code></td></tr>
<tr><td><code>tensor.sum_dim(dim)</code></td><td><code>tensor.sum(dim, keepdim=True)</code></td></tr>
<tr><td><code>tensor.topk(k, dim)</code></td><td><code>tensor.topk(k, dim).values</code></td></tr>
<tr><td><code>tensor.topk_with_indices(k, dim)</code></td><td><code>tensor.topk(k, dim)</code></td></tr>
<tr><td><code>tensor.tril(diagonal)</code></td><td><code>torch.tril(tensor, diagonal)</code></td></tr>
<tr><td><code>tensor.triu(diagonal)</code></td><td><code>torch.triu(tensor, diagonal)</code></td></tr>
<tr><td><code>tensor.zeros_like()</code></td><td><code>torch.zeros_like(tensor)</code></td></tr>
</tbody></table>
</div>
<h3 id="float-operations"><a class="header" href="#float-operations">Float Operations</a></h3>
<p>Those operations are only available for <code>Float</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.cast(dtype)</code></td><td><code>tensor.to(dtype)</code></td></tr>
<tr><td><code>tensor.ceil()</code></td><td><code>tensor.ceil()</code></td></tr>
<tr><td><code>tensor.cos()</code></td><td><code>tensor.cos()</code></td></tr>
<tr><td><code>tensor.cosh()</code></td><td><code>tensor.cosh()</code></td></tr>
<tr><td><code>tensor.erf()</code></td><td><code>tensor.erf()</code></td></tr>
<tr><td><code>tensor.exp()</code></td><td><code>tensor.exp()</code></td></tr>
<tr><td><code>tensor.floor()</code></td><td><code>tensor.floor()</code></td></tr>
<tr><td><code>tensor.from_floats(floats, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.from_full_precision(tensor)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.int()</code></td><td>Similar to <code>tensor.to(torch.long)</code></td></tr>
<tr><td><code>tensor.is_close(other, atol, rtol)</code></td><td><code>torch.isclose(tensor, other, atol, rtol)</code></td></tr>
<tr><td><code>tensor.is_finite()</code></td><td><code>torch.isfinite(tensor)</code></td></tr>
<tr><td><code>tensor.is_inf()</code></td><td><code>torch.isinf(tensor)</code></td></tr>
<tr><td><code>tensor.is_nan()</code></td><td><code>torch.isnan(tensor)</code></td></tr>
<tr><td><code>tensor.log()</code></td><td><code>tensor.log()</code></td></tr>
<tr><td><code>tensor.log1p()</code></td><td><code>tensor.log1p()</code></td></tr>
<tr><td><code>tensor.matmul(other)</code></td><td><code>tensor.matmul(other)</code></td></tr>
<tr><td><code>tensor.random(shape, distribution, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.random_like(distribution)</code></td><td><code>torch.rand_like()</code> only uniform</td></tr>
<tr><td><code>tensor.recip()</code> or <code>1.0 / tensor</code></td><td><code>tensor.reciprocal()</code> or <code>1.0 / tensor</code></td></tr>
<tr><td><code>tensor.round()</code></td><td><code>tensor.round()</code></td></tr>
<tr><td><code>tensor.sin()</code></td><td><code>tensor.sin()</code></td></tr>
<tr><td><code>tensor.sinh()</code></td><td><code>tensor.sinh()</code></td></tr>
<tr><td><code>tensor.sqrt()</code></td><td><code>tensor.sqrt()</code></td></tr>
<tr><td><code>tensor.tan()</code></td><td><code>tensor.tan()</code></td></tr>
<tr><td><code>tensor.tanh()</code></td><td><code>tensor.tanh()</code></td></tr>
<tr><td><code>tensor.to_full_precision()</code></td><td><code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.var(dim)</code></td><td><code>tensor.var(dim)</code></td></tr>
<tr><td><code>tensor.var_bias(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.var_mean(dim)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.var_mean_bias(dim)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h3 id="int-operations"><a class="header" href="#int-operations">Int Operations</a></h3>
<p>Those operations are only available for <code>Int</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Tensor::arange(5..10, device)</code></td><td><code>tensor.arange(start=5, end=10, device=device)</code></td></tr>
<tr><td><code>Tensor::arange_step(5..10, 2, device)</code></td><td><code>tensor.arange(start=5, end=10, step=2, device=device)</code></td></tr>
<tr><td><code>tensor.bitwise_and(other)</code></td><td><code>torch.bitwise_and(tensor, other)</code></td></tr>
<tr><td><code>tensor.bitwise_and_scalar(scalar)</code></td><td><code>torch.bitwise_and(tensor, scalar)</code></td></tr>
<tr><td><code>tensor.bitwise_not()</code></td><td><code>torch.bitwise_not(tensor)</code></td></tr>
<tr><td><code>tensor.bitwise_left_shift(other)</code></td><td><code>torch.bitwise_left_shift(tensor, other)</code></td></tr>
<tr><td><code>tensor.bitwise_left_shift_scalar(scalar)</code></td><td><code>torch.bitwise_left_shift(tensor, scalar)</code></td></tr>
<tr><td><code>tensor.bitwise_right_shift(other)</code></td><td><code>torch.bitwise_right_shift(tensor, other)</code></td></tr>
<tr><td><code>tensor.bitwise_right_shift_scalar(scalar)</code></td><td><code>torch.bitwise_right_shift(tensor, scalar)</code></td></tr>
<tr><td><code>tensor.bitwise_or(other)</code></td><td><code>torch.bitwise_or(tensor, other)</code></td></tr>
<tr><td><code>tensor.bitwise_or_scalar(scalar)</code></td><td><code>torch.bitwise_or(tensor, scalar)</code></td></tr>
<tr><td><code>tensor.bitwise_xor(other)</code></td><td><code>torch.bitwise_xor(tensor, other)</code></td></tr>
<tr><td><code>tensor.bitwise_xor_scalar(scalar)</code></td><td><code>torch.bitwise_xor(tensor, scalar)</code></td></tr>
<tr><td><code>tensor.float()</code></td><td><code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.from_ints(ints)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.int_random(shape, distribution, device)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.cartesian_grid(shape, device)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h3 id="bool-operations"><a class="header" href="#bool-operations">Bool Operations</a></h3>
<p>Those operations are only available for <code>Bool</code> tensors.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Tensor::diag_mask(shape, diagonal)</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::tril_mask(shape, diagonal)</code></td><td>N/A</td></tr>
<tr><td><code>Tensor::triu_mask(shape, diagonal)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.argwhere()</code></td><td><code>tensor.argwhere()</code></td></tr>
<tr><td><code>tensor.bool_and()</code></td><td><code>tensor.logical_and()</code></td></tr>
<tr><td><code>tensor.bool_not()</code></td><td><code>tensor.logical_not()</code></td></tr>
<tr><td><code>tensor.bool_or()</code></td><td><code>tensor.logical_or()</code></td></tr>
<tr><td><code>tensor.float()</code></td><td><code>tensor.to(torch.float)</code></td></tr>
<tr><td><code>tensor.int()</code></td><td><code>tensor.to(torch.long)</code></td></tr>
<tr><td><code>tensor.nonzero()</code></td><td><code>tensor.nonzero(as_tuple=True)</code></td></tr>
</tbody></table>
</div>
<h3 id="quantization-operations"><a class="header" href="#quantization-operations">Quantization Operations</a></h3>
<p>Those operations are only available for <code>Float</code> tensors on backends that implement quantization
strategies.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.quantize(scheme, qparams)</code></td><td>N/A</td></tr>
<tr><td><code>tensor.dequantize()</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<h2 id="activation-functions"><a class="header" href="#activation-functions">Activation Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>activation::gelu(tensor)</code></td><td><code>nn.functional.gelu(tensor)</code></td></tr>
<tr><td><code>activation::hard_sigmoid(tensor, alpha, beta)</code></td><td><code>nn.functional.hardsigmoid(tensor)</code></td></tr>
<tr><td><code>activation::leaky_relu(tensor, negative_slope)</code></td><td><code>nn.functional.leaky_relu(tensor, negative_slope)</code></td></tr>
<tr><td><code>activation::log_sigmoid(tensor)</code></td><td><code>nn.functional.log_sigmoid(tensor)</code></td></tr>
<tr><td><code>activation::log_softmax(tensor, dim)</code></td><td><code>nn.functional.log_softmax(tensor, dim)</code></td></tr>
<tr><td><code>activation::mish(tensor)</code></td><td><code>nn.functional.mish(tensor)</code></td></tr>
<tr><td><code>activation::prelu(tensor,alpha)</code></td><td><code>nn.functional.prelu(tensor,weight)</code></td></tr>
<tr><td><code>activation::quiet_softmax(tensor, dim)</code></td><td><code>nn.functional.quiet_softmax(tensor, dim)</code></td></tr>
<tr><td><code>activation::relu(tensor)</code></td><td><code>nn.functional.relu(tensor)</code></td></tr>
<tr><td><code>activation::sigmoid(tensor)</code></td><td><code>nn.functional.sigmoid(tensor)</code></td></tr>
<tr><td><code>activation::silu(tensor)</code></td><td><code>nn.functional.silu(tensor)</code></td></tr>
<tr><td><code>activation::softmax(tensor, dim)</code></td><td><code>nn.functional.softmax(tensor, dim)</code></td></tr>
<tr><td><code>activation::softmin(tensor, dim)</code></td><td><code>nn.functional.softmin(tensor, dim)</code></td></tr>
<tr><td><code>activation::softplus(tensor, beta)</code></td><td><code>nn.functional.softplus(tensor, beta)</code></td></tr>
<tr><td><code>activation::tanh(tensor)</code></td><td><code>nn.functional.tanh(tensor)</code></td></tr>
</tbody></table>
</div>
<h2 id="grid-functions"><a class="header" href="#grid-functions">Grid Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>grid::meshgrid(tensors, GridIndexing::Matrix)</code></td><td>`torch.meshgrid(tensors, indexing="ij")</td></tr>
<tr><td><code>grid::meshgrid(tensors, GridIndexing::Cartesian)</code></td><td>`torch.meshgrid(tensors, indexing="xy")</td></tr>
</tbody></table>
</div>
<h2 id="linalg-functions"><a class="header" href="#linalg-functions">Linalg Functions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>linalg::vector_norm(tensors, p, dim)</code></td><td>`torch.linalg.vector_norm(tensor, p, dim)</td></tr>
</tbody></table>
</div>
<h2 id="displaying-tensor-details"><a class="header" href="#displaying-tensor-details">Displaying Tensor Details</a></h2>
<p>Mabor provides flexible options for displaying tensor information, allowing you to control the level
of detail and formatting to suit your needs.</p>
<h3 id="basic-display"><a class="header" href="#basic-display">Basic Display</a></h3>
<p>To display a detailed view of a tensor, you can simply use Rust's <code>println!</code> or <code>format!</code> macros:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let tensor = Tensor::&lt;Backend, 2&gt;::full([2, 3], 0.123456789, &amp;Default::default());
println!("{}", tensor);
<span class="boring">}</span></code></pre></pre>
<p>This will output:</p>
<pre><code>Tensor {
  data:
[[0.12345679, 0.12345679, 0.12345679],
 [0.12345679, 0.12345679, 0.12345679]],
  shape:  [2, 3],
  device:  Cpu,
  backend:  "ndarray",
  kind:  "Float",
  dtype:  "f32",
}
</code></pre>
<h3 id="controlling-precision"><a class="header" href="#controlling-precision">Controlling Precision</a></h3>
<p>You can control the number of decimal places displayed using Rust's formatting syntax:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>println!("{:.2}", tensor);
<span class="boring">}</span></code></pre></pre>
<p>Output:</p>
<pre><code>Tensor {
  data:
[[0.12, 0.12, 0.12],
 [0.12, 0.12, 0.12]],
  shape:  [2, 3],
  device:  Cpu,
  backend:  "ndarray",
  kind:  "Float",
  dtype:  "f32",
}
</code></pre>
<h3 id="global-print-options"><a class="header" href="#global-print-options">Global Print Options</a></h3>
<p>For more fine-grained control over tensor printing, Mabor provides a <code>PrintOptions</code> struct and a
<code>set_print_options</code> function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mabor::tensor::{set_print_options, PrintOptions};

let print_options = PrintOptions {
    precision: Some(2),
    ..Default::default()
};

set_print_options(print_options);
<span class="boring">}</span></code></pre></pre>
<p>Options:</p>
<ul>
<li>
<p><code>precision</code>: Number of decimal places for floating-point numbers (default: None)</p>
</li>
<li>
<p><code>threshold</code>: Maximum number of elements to display before summarizing (default: 1000)</p>
</li>
<li>
<p><code>edge_items</code>: Number of items to show at the beginning and end of each dimension when summarizing
(default: 3)</p>
<h3 id="checking-tensor-closeness"><a class="header" href="#checking-tensor-closeness">Checking Tensor Closeness</a></h3>
<p>Mabor provides a utility function <code>check_closeness</code> to compare two tensors and assess their
similarity. This function is particularly useful for debugging and validating tensor operations,
especially when working with floating-point arithmetic where small numerical differences can
accumulate. It's also valuable when comparing model outputs during the process of importing models
from other frameworks, helping to ensure that the imported model produces results consistent with
the original.</p>
<p>Here's an example of how to use <code>check_closeness</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mabor::tensor::{check_closeness, Tensor};
type B = mabor::backend::NdArray;

let device = Default::default();
let tensor1 = Tensor::&lt;B, 1&gt;::from_floats(
    [1.0, 2.0, 3.0, 4.0, 5.0, 6.001, 7.002, 8.003, 9.004, 10.1],
    &amp;device,
);
let tensor2 = Tensor::&lt;B, 1&gt;::from_floats(
    [1.0, 2.0, 3.0, 4.000, 5.0, 6.0, 7.001, 8.002, 9.003, 10.004],
    &amp;device,
);

check_closeness(&amp;tensor1, &amp;tensor2);
<span class="boring">}</span></code></pre></pre>
<p>The <code>check_closeness</code> function compares the two input tensors element-wise, checking their
absolute differences against a range of epsilon values. It then prints a detailed report showing
the percentage of elements that are within each tolerance level.</p>
<p>The output provides a breakdown for different epsilon values, allowing you to assess the closeness
of the tensors at various precision levels. This is particularly helpful when dealing with
operations that may introduce small numerical discrepancies.</p>
<p>The function uses color-coded output to highlight the results:</p>
<ul>
<li>Green [PASS]: All elements are within the specified tolerance.</li>
<li>Yellow [WARN]: Most elements (90% or more) are within tolerance.</li>
<li>Red [FAIL]: Significant differences are detected.</li>
</ul>
<p>This utility can be invaluable when implementing or debugging tensor operations, especially those
involving complex mathematical computations or when porting algorithms from other frameworks. It's
also an essential tool when verifying the accuracy of imported models, ensuring that the Mabor
implementation produces results that closely match those of the original model.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="autodiff"><a class="header" href="#autodiff">Autodiff</a></h1>
<p>Mabor's tensor also supports auto-differentiation, which is an essential part of any deep learning
framework. We introduced the <code>Backend</code> trait in the <a href="building-blocks/backend.html">previous section</a>, but Mabor also
has another trait for autodiff: <code>AutodiffBackend</code>.</p>
<p>However, not all tensors support auto-differentiation; you need a backend that implements both the
<code>Backend</code> and <code>AutodiffBackend</code> traits. Fortunately, you can add auto-differentiation capabilities to any
backend using a backend decorator: <code>type MyAutodiffBackend = Autodiff&lt;MyBackend&gt;</code>. This
decorator implements both the <code>AutodiffBackend</code> and <code>Backend</code> traits by maintaining a dynamic
computational graph and utilizing the inner backend to execute tensor operations.</p>
<p>The <code>AutodiffBackend</code> trait adds new operations on float tensors that can't be called otherwise. It also
provides a new associated type, <code>B::Gradients</code>, where each calculated gradient resides.</p>
<pre><code class="language-rust  ignore">fn calculate_gradients&lt;B: AutodiffBackend&gt;(tensor: Tensor&lt;B, 2&gt;) -&gt; B::Gradients {
    let mut gradients = tensor.clone().backward();

    let tensor_grad = tensor.grad(&amp;gradients);        // get
    let tensor_grad = tensor.grad_remove(&amp;mut gradients); // pop

    gradients
}</code></pre>
<p>Note that some functions will always be available even if the backend doesn't implement the
<code>AutodiffBackend</code> trait. In such cases, those functions will do nothing.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>tensor.detach()</code></td><td><code>tensor.detach()</code></td></tr>
<tr><td><code>tensor.require_grad()</code></td><td><code>tensor.requires_grad()</code></td></tr>
<tr><td><code>tensor.is_require_grad()</code></td><td><code>tensor.requires_grad</code></td></tr>
<tr><td><code>tensor.set_require_grad(require_grad)</code></td><td><code>tensor.requires_grad(False)</code></td></tr>
</tbody></table>
</div>
<p>However, you're unlikely to make any mistakes since you can't call <code>backward</code> on a tensor that is on
a backend that doesn't implement <code>AutodiffBackend</code>. Additionally, you can't retrieve the gradient of a
tensor without an autodiff backend.</p>
<h2 id="difference-with-pytorch"><a class="header" href="#difference-with-pytorch">Difference with PyTorch</a></h2>
<p>The way Mabor handles gradients is different from PyTorch. First, when calling <code>backward</code>, each
parameter doesn't have its <code>grad</code> field updated. Instead, the backward pass returns all the
calculated gradients in a container. This approach offers numerous benefits, such as the ability to
easily send gradients to other threads.</p>
<p>You can also retrieve the gradient for a specific parameter using the <code>grad</code> method on a tensor.
Since this method takes the gradients as input, it's hard to forget to call <code>backward</code> beforehand.
Note that sometimes, using <code>grad_remove</code> can improve performance by allowing inplace operations.</p>
<p>In PyTorch, when you don't need gradients for inference or validation, you typically need to scope
your code using a block.</p>
<pre><code class="language-python"># Inference mode
torch.inference():
   # your code
   ...

# Or no grad
torch.no_grad():
   # your code
   ...
</code></pre>
<p>With Mabor, you don't need to wrap the backend with the <code>Autodiff</code> for inference, and you
can call <code>inner()</code> to obtain the inner tensor, which is useful for validation.</p>
<pre><code class="language-rust  ignore">/// Use `B: AutodiffBackend`
fn example_validation&lt;B: AutodiffBackend&gt;(tensor: Tensor&lt;B, 2&gt;) {
    let inner_tensor: Tensor&lt;B::InnerBackend, 2&gt; = tensor.inner();
    let _ = inner_tensor + 5;
}

/// Use `B: Backend`
fn example_inference&lt;B: Backend&gt;(tensor: Tensor&lt;B, 2&gt;) {
    let _ = tensor + 5;
    ...
}</code></pre>
<p><strong>Gradients with Optimizers</strong></p>
<p>We've seen how gradients can be used with tensors, but the process is a bit different when working
with optimizers from <code>mabor-core</code>. To work with the <code>Module</code> trait, a translation step is required to
link tensor parameters with their gradients. This step is necessary to easily support gradient
accumulation and training on multiple devices, where each module can be forked and run on different
devices in parallel. We'll explore deeper into this topic in the <a href="building-blocks/module.html">Module</a> section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="module"><a class="header" href="#module">Module</a></h1>
<p>The <code>Module</code> derive allows you to create your own neural network modules, similar to PyTorch. The
derive function only generates the necessary methods to essentially act as a parameter container for
your type, it makes no assumptions about how the forward pass is declared.</p>
<pre><code class="language-rust  ignore">use mabor::module::Module;
use mabor::tensor::backend::Backend;

#[derive(Module, Debug)]
pub struct PositionWiseFeedForward&lt;B: Backend&gt; {
    linear_inner: Linear&lt;B&gt;,
    linear_outer: Linear&lt;B&gt;,
    dropout: Dropout,
    gelu: Gelu,
}

impl&lt;B: Backend&gt; PositionWiseFeedForward&lt;B&gt; {
    /// Normal method added to a struct.
    pub fn forward&lt;const D: usize&gt;(&amp;self, input: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt; {
        let x = self.linear_inner.forward(input);
        let x = self.gelu.forward(x);
        let x = self.dropout.forward(x);

        self.linear_outer.forward(x)
    }
}</code></pre>
<p>Note that all fields declared in the struct must also implement the <code>Module</code> trait.</p>
<h2 id="tensor-1"><a class="header" href="#tensor-1">Tensor</a></h2>
<p>If you want to create your own module that contains tensors, and not just other modules defined with
the <code>Module</code> derive, you need to be careful to achieve the behavior you want.</p>
<ul>
<li>
<p><code>Param&lt;Tensor&lt;B, D&gt;&gt;</code>: If you want the tensor to be included as a parameter of your modules, you
need to wrap the tensor in a <code>Param</code> struct. This will create an ID that will be used to identify
this parameter. This is essential when performing module optimization and when saving states such
as optimizer and module checkpoints. Note that a module's record only contains parameters.</p>
</li>
<li>
<p><code>Param&lt;Tensor&lt;B, D&gt;&gt;.set_require_grad(false)</code>: If you want the tensor to be included as a
parameter of your modules, and therefore saved with the module's weights, but you don't want it to
be updated by the optimizer.</p>
</li>
<li>
<p><code>Tensor&lt;B, D&gt;</code>: If you want the tensor to act as a constant that can be recreated when
instantiating a module. This can be useful when generating sinusoidal embeddings, for example.</p>
</li>
</ul>
<h2 id="methods"><a class="header" href="#methods">Methods</a></h2>
<p>These methods are available for all modules.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>module.devices()</code></td><td>N/A</td></tr>
<tr><td><code>module.fork(device)</code></td><td>Similar to <code>module.to(device).detach()</code></td></tr>
<tr><td><code>module.to_device(device)</code></td><td><code>module.to(device)</code></td></tr>
<tr><td><code>module.no_grad()</code></td><td><code>module.require_grad_(False)</code></td></tr>
<tr><td><code>module.num_params()</code></td><td>N/A</td></tr>
<tr><td><code>module.visit(visitor)</code></td><td>N/A</td></tr>
<tr><td><code>module.map(mapper)</code></td><td>N/A</td></tr>
<tr><td><code>module.into_record()</code></td><td>Similar to <code>state_dict</code></td></tr>
<tr><td><code>module.load_record(record)</code></td><td>Similar to <code>load_state_dict(state_dict)</code></td></tr>
<tr><td><code>module.save_file(file_path, recorder)</code></td><td>N/A</td></tr>
<tr><td><code>module.load_file(file_path, recorder)</code></td><td>N/A</td></tr>
</tbody></table>
</div>
<p>Similar to the backend trait, there is also the <code>AutodiffModule</code> trait to signify a module with
autodiff support.</p>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>module.valid()</code></td><td><code>module.eval()</code></td></tr>
</tbody></table>
</div>
<h2 id="visitor--mapper"><a class="header" href="#visitor--mapper">Visitor &amp; Mapper</a></h2>
<p>As mentioned earlier, modules primarily function as parameter containers. Therefore, we naturally
offer several ways to perform functions on each parameter. This is distinct from PyTorch, where
extending module functionalities is not as straightforward.</p>
<p>The <code>map</code> and <code>visitor</code> methods are quite similar but serve different purposes. Mapping is used for
potentially mutable operations where each parameter of a module can be updated to a new value. In
Mabor, optimizers are essentially just sophisticated module mappers. Visitors, on the other hand, are
used when you don't intend to modify the module but need to retrieve specific information from it,
such as the number of parameters or a list of devices in use.</p>
<p>You can implement your own mapper or visitor by implementing these simple traits:</p>
<pre><code class="language-rust  ignore">/// Module visitor trait.
pub trait ModuleVisitor&lt;B: Backend&gt; {
    /// Visit a float tensor in the module.
    fn visit_float&lt;const D: usize&gt;(&amp;mut self, id: ParamId, tensor: &amp;Tensor&lt;B, D&gt;);
    /// Visit an int tensor in the module.
    fn visit_int&lt;const D: usize&gt;(&amp;mut self, id: ParamId, tensor: &amp;Tensor&lt;B, D, Int&gt;);
    /// Visit a bool tensor in the module.
    fn visit_bool&lt;const D: usize&gt;(&amp;mut self, id: ParamId, tensor: &amp;Tensor&lt;B, D, Bool&gt;);
}

/// Module mapper trait.
pub trait ModuleMapper&lt;B: Backend&gt; {
    /// Map a float tensor in the module.
    fn map_float&lt;const D: usize&gt;(&amp;mut self, id: ParamId, tensor: Tensor&lt;B, D&gt;) -&gt; Tensor&lt;B, D&gt;;
    /// Map an int tensor in the module.
    fn map_int&lt;const D: usize&gt;(&amp;mut self, id: ParamId, tensor: Tensor&lt;B, D, Int&gt;) -&gt; Tensor&lt;B, D, Int&gt;;
    /// Map a bool tensor in the module.
    fn map_bool&lt;const D: usize&gt;(&amp;mut self, id: ParamId, tensor: Tensor&lt;B, D, Bool&gt;) -&gt; Tensor&lt;B, D, Bool&gt;;
}</code></pre>
<p>Note that the trait doesn't require all methods to be implemented as they are already defined to
perform no operation. If you're only interested in float tensors (like the majority of use cases),
then you can simply implement <code>map_float</code> or <code>visit_float</code>.</p>
<p>For example, the <code>ModuleMapper</code> trait could be implemented to clamp all parameters into the range
<code>[min, max]</code>.</p>
<pre><code class="language-rust  ignore">/// Clamp parameters into the range `[min, max]`.
pub struct Clamp {
    /// Lower-bound of the range.
    pub min: f32,
    /// Upper-bound of the range.
    pub max: f32,
}

// Clamp all floating-point parameter tensors between `[min, max]`.
impl&lt;B: Backend&gt; ModuleMapper&lt;B&gt; for Clamp {
    fn map_float&lt;const D: usize&gt;(
        &amp;mut self,
        _id: mabor::module::ParamId,
        tensor: mabor::prelude::Tensor&lt;B, D&gt;,
    ) -&gt; mabor::prelude::Tensor&lt;B, D&gt; {
        tensor.clamp(self.min, self.max)
    }
}

// Clamp module mapper into the range `[-0.5, 0.5]`
let mut clamp = Clamp {
    min: -0.5,
    max: 0.5,
};
let model = model.map(&amp;mut clamp);</code></pre>
<p>If you want to use this during training to constrain your model parameters, make sure that the
parameter tensors are still tracked for autodiff. This can be done with a simple adjustment to the
implementation.</p>
<pre><code class="language-rust  ignore">impl&lt;B: AutodiffBackend&gt; ModuleMapper&lt;B&gt; for Clamp {
    fn map_float&lt;const D: usize&gt;(
        &amp;mut self,
        _id: mabor::module::ParamId,
        tensor: mabor::prelude::Tensor&lt;B, D&gt;,
    ) -&gt; mabor::prelude::Tensor&lt;B, D&gt; {
        let is_require_grad = tensor.is_require_grad();

        let mut tensor = Tensor::from_inner(tensor.inner().clamp(self.min, self.max));

        if is_require_grad {
            tensor = tensor.require_grad();
        }

        tensor
    }
}</code></pre>
<h2 id="module-display"><a class="header" href="#module-display">Module Display</a></h2>
<p>Mabor provides a simple way to display the structure of a module and its configuration at a glance.
You can print the module to see its structure, which is useful for debugging and tracking changes
across different versions of a module. (See the print output of the
<a href="basic-workflow/model.html">Basic Workflow Model</a> example.)</p>
<p>To customize the display of a module, you can implement the <code>ModuleDisplay</code> trait for your module.
This will change the default display settings for the module and its children. Note that
<code>ModuleDisplay</code> is automatically implemented for all modules, but you can override it to customize
the display by annotating the module with <code>#[module(custom_display)]</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Module, Debug)]
#[module(custom_display)]
pub struct PositionWiseFeedForward&lt;B: Backend&gt; {
    linear_inner: Linear&lt;B&gt;,
    linear_outer: Linear&lt;B&gt;,
    dropout: Dropout,
    gelu: Gelu,
}

impl&lt;B: Backend&gt; ModuleDisplay for PositionWiseFeedForward&lt;B&gt; {
    /// Custom settings for the display of the module.
    /// If `None` is returned, the default settings will be used.
    fn custom_settings(&amp;self) -&gt; Option&lt;mabor::module::DisplaySettings&gt; {
        DisplaySettings::new()
            // Will show all attributes (default is false)
            .with_show_all_attributes(false)
            // Will show each attribute on a new line (default is true)
            .with_new_line_after_attribute(true)
            // Will show the number of parameters (default is true)
            .with_show_num_parameters(true)
            // Will indent by 2 spaces (default is 2)
            .with_indentation_size(2)
            // Will show the parameter ID (default is false)
            .with_show_param_id(false)
            // Convenience method to wrap settings in Some()
            .optional()
    }

    /// Custom content to be displayed.
    /// If `None` is returned, the default content will be used
    /// (all attributes of the module)
    fn custom_content(&amp;self, content: Content) -&gt; Option&lt;Content&gt; {
        content
            .add("linear_inner", &amp;self.linear_inner)
            .add("linear_outer", &amp;self.linear_outer)
            .add("anything", "anything_else")
            .optional()
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="built-in-modules"><a class="header" href="#built-in-modules">Built-in Modules</a></h2>
<p>Mabor comes with built-in modules that you can use to build your own modules.</p>
<h3 id="general"><a class="header" href="#general">General</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>BatchNorm</code></td><td><code>nn.BatchNorm1d</code>, <code>nn.BatchNorm2d</code> etc.</td></tr>
<tr><td><code>Dropout</code></td><td><code>nn.Dropout</code></td></tr>
<tr><td><code>Embedding</code></td><td><code>nn.Embedding</code></td></tr>
<tr><td><code>Gelu</code></td><td><code>nn.Gelu</code></td></tr>
<tr><td><code>GroupNorm</code></td><td><code>nn.GroupNorm</code></td></tr>
<tr><td><code>HardSigmoid</code></td><td><code>nn.Hardsigmoid</code></td></tr>
<tr><td><code>InstanceNorm</code></td><td><code>nn.InstanceNorm1d</code>, <code>nn.InstanceNorm2d</code> etc.</td></tr>
<tr><td><code>LayerNorm</code></td><td><code>nn.LayerNorm</code></td></tr>
<tr><td><code>LeakyRelu</code></td><td><code>nn.LeakyReLU</code></td></tr>
<tr><td><code>Linear</code></td><td><code>nn.Linear</code></td></tr>
<tr><td><code>Prelu</code></td><td><code>nn.PReLu</code></td></tr>
<tr><td><code>Relu</code></td><td><code>nn.ReLU</code></td></tr>
<tr><td><code>RmsNorm</code></td><td><em>No direct equivalent</em></td></tr>
<tr><td><code>SwiGlu</code></td><td><em>No direct equivalent</em></td></tr>
<tr><td><code>Interpolate1d</code></td><td><em>No direct equivalent</em></td></tr>
<tr><td><code>Interpolate2d</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div>
<h3 id="convolutions"><a class="header" href="#convolutions">Convolutions</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Conv1d</code></td><td><code>nn.Conv1d</code></td></tr>
<tr><td><code>Conv2d</code></td><td><code>nn.Conv2d</code></td></tr>
<tr><td><code>Conv3d</code></td><td><code>nn.Conv3d</code></td></tr>
<tr><td><code>ConvTranspose1d</code></td><td><code>nn.ConvTranspose1d</code></td></tr>
<tr><td><code>ConvTranspose2d</code></td><td><code>nn.ConvTranspose2d</code></td></tr>
<tr><td><code>ConvTranspose3d</code></td><td><code>nn.ConvTranspose3d</code></td></tr>
<tr><td><code>DeformConv2d</code></td><td><code>torchvision.ops.DeformConv2d</code></td></tr>
</tbody></table>
</div>
<h3 id="pooling"><a class="header" href="#pooling">Pooling</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>AdaptiveAvgPool1d</code></td><td><code>nn.AdaptiveAvgPool1d</code></td></tr>
<tr><td><code>AdaptiveAvgPool2d</code></td><td><code>nn.AdaptiveAvgPool2d</code></td></tr>
<tr><td><code>AvgPool1d</code></td><td><code>nn.AvgPool1d</code></td></tr>
<tr><td><code>AvgPool2d</code></td><td><code>nn.AvgPool2d</code></td></tr>
<tr><td><code>MaxPool1d</code></td><td><code>nn.MaxPool1d</code></td></tr>
<tr><td><code>MaxPool2d</code></td><td><code>nn.MaxPool2d</code></td></tr>
</tbody></table>
</div>
<h3 id="rnns"><a class="header" href="#rnns">RNNs</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>Gru</code></td><td><code>nn.GRU</code></td></tr>
<tr><td><code>Lstm</code>/<code>BiLstm</code></td><td><code>nn.LSTM</code></td></tr>
<tr><td><code>GateController</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div>
<h3 id="transformer"><a class="header" href="#transformer">Transformer</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>MultiHeadAttention</code></td><td><code>nn.MultiheadAttention</code></td></tr>
<tr><td><code>TransformerDecoder</code></td><td><code>nn.TransformerDecoder</code></td></tr>
<tr><td><code>TransformerEncoder</code></td><td><code>nn.TransformerEncoder</code></td></tr>
<tr><td><code>PositionalEncoding</code></td><td><em>No direct equivalent</em></td></tr>
<tr><td><code>RotaryEncoding</code></td><td><em>No direct equivalent</em></td></tr>
</tbody></table>
</div>
<h3 id="loss"><a class="header" href="#loss">Loss</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Mabor API</th><th>PyTorch Equivalent</th></tr></thead><tbody>
<tr><td><code>BinaryCrossEntropyLoss</code></td><td><code>nn.BCELoss</code></td></tr>
<tr><td><code>CosineEmbeddingLoss</code></td><td><code>nn.CosineEmbeddingLoss</code></td></tr>
<tr><td><code>CrossEntropyLoss</code></td><td><code>nn.CrossEntropyLoss</code></td></tr>
<tr><td><code>HuberLoss</code></td><td><code>nn.HuberLoss</code></td></tr>
<tr><td><code>MseLoss</code></td><td><code>nn.MSELoss</code></td></tr>
<tr><td><code>PoissonNllLoss</code></td><td><code>nn.PoissonNLLLoss</code></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="learner"><a class="header" href="#learner">Learner</a></h1>
<p>The <a href="https://github.com/tracel-ai/burn/tree/main/crates/burn-train">mabor-train</a> crate encapsulates
multiple utilities for training deep learning models. The goal of the crate is to provide users with
a well-crafted and flexible training loop, so that projects do not have to write such components
from the ground up. Most of the interactions with <code>mabor-train</code> will be with the <code>LearnerBuilder</code>
struct, briefly presented in the previous <a href="basic-workflow/training.html">training section</a>. This
struct enables you to configure the training loop, offering support for registering metrics,
enabling logging, checkpointing states, using multiple devices, and so on.</p>
<p>There are still some assumptions in the current provided APIs, which may make them inappropriate for
your learning requirements. Indeed, they assume your model will learn from a training dataset and be
validated against another dataset. This is the most common paradigm, allowing users to do both
supervised and unsupervised learning as well as fine-tuning. However, for more complex requirements,
creating a <a href="custom-training-loop.html">custom training loop</a> might be what you need.</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>The learner builder provides numerous options when it comes to configurations.</p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Description</th></tr></thead><tbody>
<tr><td>Training Metric</td><td>Register a training metric</td></tr>
<tr><td>Validation Metric</td><td>Register a validation metric</td></tr>
<tr><td>Training Metric Plot</td><td>Register a training metric with plotting (requires the metric to be numeric)</td></tr>
<tr><td>Validation Metric Plot</td><td>Register a validation metric with plotting (requires the metric to be numeric)</td></tr>
<tr><td>Metric Logger</td><td>Configure the metric loggers (default is saving them to files)</td></tr>
<tr><td>Renderer</td><td>Configure how to render metrics (default is CLI)</td></tr>
<tr><td>Grad Accumulation</td><td>Configure the number of steps before applying gradients</td></tr>
<tr><td>File Checkpointer</td><td>Configure how the model, optimizer and scheduler states are saved</td></tr>
<tr><td>Num Epochs</td><td>Set the number of epochs</td></tr>
<tr><td>Devices</td><td>Set the devices to be used</td></tr>
<tr><td>Checkpoint</td><td>Restart training from a checkpoint</td></tr>
<tr><td>Application logging</td><td>Configure the application logging installer (default is writing to <code>experiment.log</code>)</td></tr>
</tbody></table>
</div>
<p>When the builder is configured at your liking, you can then move forward to build the learner. The
build method requires three inputs: the model, the optimizer and the learning rate scheduler. Note
that the latter can be a simple float if you want it to be constant during training.</p>
<p>The result will be a newly created Learner struct, which has only one method, the <code>fit</code> function
which must be called with the training and validation dataloaders. This will start the training and
return the trained model once finished.</p>
<p>Again, please refer to the <a href="basic-workflow/training.html">training section</a> for a relevant code
snippet.</p>
<h2 id="artifacts"><a class="header" href="#artifacts">Artifacts</a></h2>
<p>When creating a new builder, all the collected data will be saved under the directory provided as
the argument to the <code>new</code> method. Here is an example of the data layout for a model recorded using
the compressed message pack format, with the accuracy and loss metrics registered:</p>
<pre><code>├── experiment.log
├── checkpoint
│   ├── model-1.mpk.gz
│   ├── optim-1.mpk.gz
│   └── scheduler-1.mpk.gz
│   ├── model-2.mpk.gz
│   ├── optim-2.mpk.gz
│   └── scheduler-2.mpk.gz
├── train
│   ├── epoch-1
│   │   ├── Accuracy.log
│   │   └── Loss.log
│   └── epoch-2
│       ├── Accuracy.log
│       └── Loss.log
└── valid
    ├── epoch-1
    │   ├── Accuracy.log
    │   └── Loss.log
    └── epoch-2
        ├── Accuracy.log
        └── Loss.log
</code></pre>
<p>You can choose to save or synchronize that local directory with a remote file system, if desired.
The file checkpointer is capable of automatically deleting old checkpoints according to a specified
configuration.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metric"><a class="header" href="#metric">Metric</a></h1>
<p>When working with the learner, you have the option to record metrics that will be monitored
throughout the training process. We currently offer a restricted range of metrics.</p>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Description</th></tr></thead><tbody>
<tr><td>Accuracy</td><td>Calculate the accuracy in percentage</td></tr>
<tr><td>TopKAccuracy</td><td>Calculate the top-k accuracy in percentage</td></tr>
<tr><td>Precision</td><td>Calculate precision in percentage</td></tr>
<tr><td>Recall</td><td>Calculate recall in percentage</td></tr>
<tr><td>FBetaScore</td><td>Calculate F<sub>β </sub>score in percentage</td></tr>
<tr><td>AUROC</td><td>Calculate the area under curve of ROC in percentage</td></tr>
<tr><td>Loss</td><td>Output the loss used for the backward pass</td></tr>
<tr><td>CPU Temperature</td><td>Fetch the temperature of CPUs</td></tr>
<tr><td>CPU Usage</td><td>Fetch the CPU utilization</td></tr>
<tr><td>CPU Memory Usage</td><td>Fetch the CPU RAM usage</td></tr>
<tr><td>GPU Temperature</td><td>Fetch the GPU temperature</td></tr>
<tr><td>Learning Rate</td><td>Fetch the current learning rate for each optimizer step</td></tr>
<tr><td>CUDA</td><td>Fetch general CUDA metrics such as utilization</td></tr>
</tbody></table>
</div>
<p>In order to use a metric, the output of your training step has to implement the <code>Adaptor</code> trait from
<code>mabor-train::metric</code>. Here is an example for the classification output, already provided with the
crate.</p>
<pre><code class="language-rust   ignore">/// Simple classification output adapted for multiple metrics.
#[derive(new)]
pub struct ClassificationOutput&lt;B: Backend&gt; {
    /// The loss.
    pub loss: Tensor&lt;B, 1&gt;,

    /// The output.
    pub output: Tensor&lt;B, 2&gt;,

    /// The targets.
    pub targets: Tensor&lt;B, 1, Int&gt;,
}

impl&lt;B: Backend&gt; Adaptor&lt;AccuracyInput&lt;B&gt;&gt; for ClassificationOutput&lt;B&gt; {
    fn adapt(&amp;self) -&gt; AccuracyInput&lt;B&gt; {
        AccuracyInput::new(self.output.clone(), self.targets.clone())
    }
}

impl&lt;B: Backend&gt; Adaptor&lt;LossInput&lt;B&gt;&gt; for ClassificationOutput&lt;B&gt; {
    fn adapt(&amp;self) -&gt; LossInput&lt;B&gt; {
        LossInput::new(self.loss.clone())
    }
}</code></pre>
<h1 id="custom-metric"><a class="header" href="#custom-metric">Custom Metric</a></h1>
<p>Generating your own custom metrics is done by implementing the <code>Metric</code> trait.</p>
<pre><code class="language-rust   ignore">
/// Metric trait.
///
/// # Notes
///
/// Implementations should define their own input type only used by the metric.
/// This is important since some conflict may happen when the model output is adapted for each
/// metric's input type.
pub trait Metric: Send + Sync {
    /// The input type of the metric.
    type Input;

    /// The parameterized name of the metric.
    ///
    /// This should be unique, so avoid using short generic names, prefer using the long name.
    ///
    /// For a metric that can exist at different parameters (e.g., top-k accuracy for different
    /// values of k), the name should be unique for each instance.
    fn name(&amp;self) -&gt; String;

    /// Update the metric state and returns the current metric entry.
    fn update(&amp;mut self, item: &amp;Self::Input, metadata: &amp;MetricMetadata) -&gt; MetricEntry;
    /// Clear the metric state.
    fn clear(&amp;mut self);
}</code></pre>
<p>As an example, let's see how the loss metric is implemented.</p>
<pre><code class="language-rust  ignore">/// The loss metric.
#[derive(Default)]
pub struct LossMetric&lt;B: Backend&gt; {
    state: NumericMetricState,
    _b: B,
}

/// The loss metric input type.
#[derive(new)]
pub struct LossInput&lt;B: Backend&gt; {
    tensor: Tensor&lt;B, 1&gt;,
}


impl&lt;B: Backend&gt; Metric for LossMetric&lt;B&gt; {
    type Input = LossInput&lt;B&gt;;

    fn update(&amp;mut self, loss: &amp;Self::Input, _metadata: &amp;MetricMetadata) -&gt; MetricEntry {
        let [batch_size] = loss.tensor.dims();
        let loss = loss
            .tensor
            .clone()
            .mean()
            .into_data()
            .iter::&lt;f64&gt;()
            .next()
            .unwrap();

        self.state.update(
            loss,
            batch_size,
            FormatOptions::new(self.name()).precision(2),
        )
    }

    fn clear(&amp;mut self) {
        self.state.reset()
    }

    fn name(&amp;self) -&gt; String {
        "Loss".to_string()
    }
}</code></pre>
<p>When the metric you are implementing is numeric in nature, you may want to also implement the
<code>Numeric</code> trait. This will allow your metric to be plotted.</p>
<pre><code class="language-rust  ignore">impl&lt;B: Backend&gt; Numeric for LossMetric&lt;B&gt; {
    fn value(&amp;self) -&gt; f64 {
        self.state.value()
    }
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="config"><a class="header" href="#config">Config</a></h1>
<p>When writing scientific code, you normally have a lot of values that are set, and Deep Learning is
no exception. Python has the possibility to define default parameters for functions, which helps
improve the developer experience. However, this has the downside of potentially breaking your code
when upgrading to a new version, as the default values might change without your knowledge, making
debugging very challenging.</p>
<p>With that in mind, we came up with the Config system. It's a simple Rust derive that you can apply
to your types, allowing you to define default values with ease. Additionally, all configs can be
serialized, reducing potential bugs when upgrading versions and improving reproducibility.</p>
<pre><code class="language-rust   ignore">use mabor::config::Config;

#[derive(Config)]
pub struct MyModuleConfig {
    d_model: usize,
    d_ff: usize,
    #[config(default = 0.1)]
    dropout: f64,
}</code></pre>
<p>The derive also adds useful <code>with_</code> methods for every attribute of your config, similar to a builder
pattern, along with a <code>save</code> method.</p>
<pre><code class="language-rust  ignore">fn main() {
    let config = MyModuleConfig::new(512, 2048);
    println!("{}", config.d_model); // 512
    println!("{}", config.d_ff); // 2048
    println!("{}", config.dropout); // 0.1
    let config =  config.with_dropout(0.2);
    println!("{}", config.dropout); // 0.2

    config.save("config.json").unwrap();
}</code></pre>
<h2 id="good-practices"><a class="header" href="#good-practices">Good practices</a></h2>
<p>By using the config type it is easy to create new module instances. The initialization method should
be implemented on the config type with the device as argument.</p>
<pre><code class="language-rust  ignore">impl MyModuleConfig {
    /// Create a module on the given device.
    pub fn init&lt;B: Backend&gt;(&amp;self, device: &amp;B::Device) -&gt; MyModule {
        MyModule {
            linear: LinearConfig::new(self.d_model, self.d_ff).init(device),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}</code></pre>
<p>Then we could add this line to the above <code>main</code>:</p>
<pre><code class="language-rust  ignore">use mabor::backend::Wgpu;
let device = Default::default();
let my_module = config.init::&lt;Wgpu&gt;(&amp;device);</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="record"><a class="header" href="#record">Record</a></h1>
<p>Records are how states are saved with Mabor. Compared to most other frameworks, Mabor has its own
advanced saving mechanism that allows interoperability between backends with minimal possible
runtime errors. There are multiple reasons why Mabor decided to create its own saving formats.</p>
<p>First, Rust has <a href="https://serde.rs/">serde</a>, which is an extremely well-developed serialization and
deserialization library that also powers the <code>safetensors</code> format developed by Hugging Face. If used
properly, all the validations are done when deserializing, which removes the need to write
validation code. Since modules in Mabor are created with configurations, they can't implement
serialization and deserialization. That's why the record system was created: allowing you to save
the state of modules independently of the backend in use extremely fast while still giving you all
the flexibility possible to include any non-serializable field within your module.</p>
<p><strong>Why not use safetensors?</strong></p>
<p><a href="https://github.com/huggingface/safetensors"><code>safetensors</code></a> uses serde with the JSON file format and
only supports serializing and deserializing tensors. The record system in Mabor gives you the
possibility to serialize any type, which is very useful for optimizers that save their state, but
also for any non-standard, cutting-edge modeling needs you may have. Additionally, the record system
performs automatic precision conversion by using Rust types, making it more reliable with fewer
manual manipulations.</p>
<p>It is important to note that the <code>safetensors</code> format uses the word <em>safe</em> to distinguish itself
from Pickle, which is vulnerable to Python code injection. On our end, the simple fact that we use
Rust already ensures that no code injection is possible. If your storage mechanism doesn't handle
data corruption, you might prefer a recorder that performs checksum validation (i.e., any recorder
with Gzip compression).</p>
<h2 id="recorder"><a class="header" href="#recorder">Recorder</a></h2>
<p>Recorders are independent of the backend and serialize records with precision and a format. Note
that the format can also be in-memory, allowing you to save the records directly into bytes.</p>
<div class="table-wrapper"><table><thead><tr><th>Recorder</th><th>Format</th><th>Compression</th></tr></thead><tbody>
<tr><td>DefaultFileRecorder</td><td>File - Named MessagePack</td><td>None</td></tr>
<tr><td>NamedMpkFileRecorder</td><td>File - Named MessagePack</td><td>None</td></tr>
<tr><td>NamedMpkGzFileRecorder</td><td>File - Named MessagePack</td><td>Gzip</td></tr>
<tr><td>BinFileRecorder</td><td>File - Binary</td><td>None</td></tr>
<tr><td>BinGzFileRecorder</td><td>File - Binary</td><td>Gzip</td></tr>
<tr><td>JsonGzFileRecorder</td><td>File - Json</td><td>Gzip</td></tr>
<tr><td>PrettyJsonFileRecorder</td><td>File - Pretty Json</td><td>Gzip</td></tr>
<tr><td>BinBytesRecorder</td><td>In Memory - Binary</td><td>None</td></tr>
</tbody></table>
</div>
<p>Each recorder supports precision settings decoupled from the precision used for training or
inference. These settings allow you to define the floating-point and integer types that will be used
for serialization and deserialization.</p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Float Precision</th><th>Integer Precision</th></tr></thead><tbody>
<tr><td><code>DoublePrecisionSettings</code></td><td><code>f64</code></td><td><code>i64</code></td></tr>
<tr><td><code>FullPrecisionSettings</code></td><td><code>f32</code></td><td><code>i32</code></td></tr>
<tr><td><code>HalfPrecisionSettings</code></td><td><code>f16</code></td><td><code>i16</code></td></tr>
</tbody></table>
</div>
<p>Note that when loading a record into a module, the type conversion is automatically handled, so you
can't encounter errors. The only crucial aspect is using the same recorder for both serialization
and deserialization; otherwise, you will encounter loading errors.</p>
<p><strong>Which recorder should you use?</strong></p>
<ul>
<li>If you want fast serialization and deserialization, choose a recorder without compression. The one
with the lowest file size without compression is the binary format; otherwise, the named
MessagePack could be used.</li>
<li>If you want to save models for storage, you can use compression, but avoid using the binary
format, as it may not be backward compatible.</li>
<li>If you want to debug your model's weights, you can use the pretty JSON format.</li>
<li>If you want to deploy with <code>no-std</code>, use the in-memory binary format and include the bytes with
the compiled code.</li>
</ul>
<p>For examples on saving and loading records, take a look at
<a href="saving-and-loading.html">Saving and Loading Models</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dataset"><a class="header" href="#dataset">Dataset</a></h1>
<p>At its core, a dataset is a collection of data typically related to a specific analysis or
processing task. The data modality can vary depending on the task, but most datasets primarily
consist of images, texts, audio or videos.</p>
<p>This data source represents an integral part of machine learning to successfully train a model. Thus,
it is essential to provide a convenient and performant API to handle your data. Since this process
varies wildly from one problem to another, it is defined as a trait that should be implemented on
your type. The dataset trait is quite similar to the dataset abstract class in PyTorch:</p>
<pre><code class="language-rust  ignore">pub trait Dataset&lt;I&gt;: Send + Sync {
    fn get(&amp;self, index: usize) -&gt; Option&lt;I&gt;;
    fn len(&amp;self) -&gt; usize;
}</code></pre>
<p>The dataset trait assumes a fixed-length set of items that can be randomly accessed in constant
time. This is a major difference from datasets that use Apache Arrow underneath to improve streaming
performance. Datasets in Mabor don't assume <em>how</em> they are going to be accessed; it's just a
collection of items.</p>
<p>However, you can compose multiple dataset transformations to lazily obtain what you want with zero
pre-processing, so that your training can start instantly!</p>
<h2 id="transformation"><a class="header" href="#transformation">Transformation</a></h2>
<p>Transformations in Mabor are all lazy and modify one or multiple input datasets. The goal of these
transformations is to provide you with the necessary tools so that you can model complex data
distributions.</p>
<div class="table-wrapper"><table><thead><tr><th>Transformation</th><th>Description</th></tr></thead><tbody>
<tr><td><code>SamplerDataset</code></td><td>Samples items from a dataset. This is a convenient way to model a dataset as a probability distribution of a fixed size.</td></tr>
<tr><td><code>ShuffledDataset</code></td><td>Maps each input index to a random index, similar to a dataset sampled without replacement.</td></tr>
<tr><td><code>PartialDataset</code></td><td>Returns a view of the input dataset with a specified range.</td></tr>
<tr><td><code>MapperDataset</code></td><td>Computes a transformation lazily on the input dataset.</td></tr>
<tr><td><code>ComposedDataset</code></td><td>Composes multiple datasets together to create a larger one without copying any data.</td></tr>
<tr><td><code>WindowsDataset</code></td><td>Dataset designed to work with overlapping windows of data extracted from an input dataset.</td></tr>
</tbody></table>
</div>
<p>Let us look at the basic usages of each dataset transform and how they can be composed together.
These transforms are lazy by default except when specified, reducing the need for unnecessary
intermediate allocations and improving performance. The full documentation of each transform can be
found at the <a href="../../docs/burn/data/dataset/transform/index.html">API reference</a>.</p>
<ul>
<li><strong>SamplerDataset</strong>: This transform can be used to sample items from a dataset with (default) or
without replacement. Transform is initialized with a sampling size which can be bigger or smaller
than the input dataset size. This is particularly useful in cases where we want to checkpoint
larger datasets more often during training and smaller datasets less often as the size of an epoch
is now controlled by the sampling size. Sample usage:</li>
</ul>
<pre><code class="language-rust  ignore">type DbPedia = SqliteDataset&lt;DbPediaItem&gt;;
let dataset: DbPedia = HuggingfaceDatasetLoader::new("dbpedia_14")
        .dataset("train").
        .unwrap();

let dataset = SamplerDataset&lt;DbPedia, DbPediaItem&gt;::new(dataset, 10000);</code></pre>
<ul>
<li><strong>ShuffledDataset</strong>: This transform can be used to shuffle the items of a dataset. Particularly
useful before splitting the raw dataset into train/test splits. Can be initialized with a seed to
ensure reproducibility.</li>
</ul>
<pre><code class="language-rust  ignore">let dataset = ShuffledDataset&lt;DbPedia, DbPediaItem&gt;::with_seed(dataset, 42);</code></pre>
<ul>
<li><strong>PartialDataset</strong>: This transform is useful to return a view of the dataset with specified start
and end indices. Used to create train/val/test splits. In the example below, we show how to chain
ShuffledDataset and PartialDataset to create splits.</li>
</ul>
<pre><code class="language-rust  ignore">// define chained dataset type here for brevity
type PartialData = PartialDataset&lt;ShuffledDataset&lt;DbPedia, DbPediaItem&gt;&gt;;
let len = dataset.len();
let split == "train"; // or "val"/"test"

let data_split = match split {
            "train" =&gt; PartialData::new(dataset, 0, len * 8 / 10), // Get first 80% dataset
            "test" =&gt; PartialData::new(dataset, len * 8 / 10, len), // Take remaining 20%
            _ =&gt; panic!("Invalid split type"),                     // Handle unexpected split types
        };</code></pre>
<ul>
<li>
<p><strong>MapperDataset</strong>: This transform is useful to apply a transformation on each of the items of a
dataset. Particularly useful for normalization of image data when channel means are known.</p>
</li>
<li>
<p><strong>ComposedDataset</strong>: This transform is useful to compose multiple datasets downloaded from
multiple sources (say different HuggingfaceDatasetLoader sources) into a single bigger dataset
which can be sampled from one source.</p>
</li>
<li>
<p><strong>WindowsDataset</strong>: This transform is useful to create overlapping windows of a dataset.
Particularly useful for sequential Time series Data, for example when working with an LSTM.</p>
</li>
</ul>
<h2 id="storage"><a class="header" href="#storage">Storage</a></h2>
<p>There are multiple dataset storage options available for you to choose from. The choice of the
dataset to use should be based on the dataset's size as well as its intended purpose.</p>
<div class="table-wrapper"><table><thead><tr><th>Storage</th><th>Description</th></tr></thead><tbody>
<tr><td><code>InMemDataset</code></td><td>In-memory dataset that uses a vector to store items. Well-suited for smaller datasets.</td></tr>
<tr><td><code>SqliteDataset</code></td><td>Dataset that uses <a href="https://www.sqlite.org/">SQLite</a> to index items that can be saved in a simple SQL database file. Well-suited for larger datasets.</td></tr>
<tr><td><code>DataframeDataset</code></td><td>Dataset that uses <a href="https://www.pola.rs/">Polars</a> dataframe to store and manage data. Well-suited for efficient data manipulation and analysis.</td></tr>
</tbody></table>
</div>
<h2 id="sources"><a class="header" href="#sources">Sources</a></h2>
<p>For now, there are only a couple of dataset sources available with Mabor, but more to come!</p>
<h3 id="hugging-face"><a class="header" href="#hugging-face">Hugging Face</a></h3>
<p>You can easily import any Hugging Face dataset with Mabor. We use SQLite as the storage to avoid
downloading the model each time or starting a Python process. You need to know the format of each
item in the dataset beforehand. Here's an example with the
<a href="https://huggingface.co/datasets/dbpedia_14">dbpedia dataset</a>.</p>
<pre><code class="language-rust  ignore">#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct DbPediaItem {
    pub title: String,
    pub content: String,
    pub label: usize,
}

fn main() {
    let dataset: SqliteDataset&lt;DbPediaItem&gt; = HuggingfaceDatasetLoader::new("dbpedia_14")
        .dataset("train") // The training split.
        .unwrap();
}</code></pre>
<p>We see that items must derive <code>serde::Serialize</code>, <code>serde::Deserialize</code>, <code>Clone</code>, and <code>Debug</code>, but
those are the only requirements.</p>
<h3 id="images"><a class="header" href="#images">Images</a></h3>
<p><code>ImageFolderDataset</code> is a generic vision dataset used to load images from disk. It is currently
available for multi-class and multi-label classification tasks as well as semantic segmentation and object detection tasks.</p>
<pre><code class="language-rust  ignore">// Create an image classification dataset from the root folder,
// where images for each class are stored in their respective folder.
//
// For example:
// root/dog/dog1.png
// root/dog/dog2.png
// ...
// root/cat/cat1.png
let dataset = ImageFolderDataset::new_classification("path/to/dataset/root").unwrap();</code></pre>
<pre><code class="language-rust  ignore">// Create a multi-label image classification dataset from a list of items,
// where each item is a tuple `(image path, labels)`, and a list of classes
// in the dataset.
//
// For example:
let items = vec![
    ("root/dog/dog1.png", vec!["animal".to_string(), "dog".to_string()]),
    ("root/cat/cat1.png", vec!["animal".to_string(), "cat".to_string()]),
];
let dataset = ImageFolderDataset::new_multilabel_classification_with_items(
    items,
    &amp;["animal", "cat", "dog"],
)
.unwrap();</code></pre>
<pre><code class="language-rust  ignore">// Create a segmentation mask dataset from a list of items, where each
// item is a tuple `(image path, mask path)` and a list of classes
// corresponding to the integer values in the mask.
let items = vec![
    (
        "path/to/images/image0.png",
        "path/to/annotations/mask0.png",
    ),
    (
        "path/to/images/image1.png",
        "path/to/annotations/mask1.png",
    ),
    (
        "path/to/images/image2.png",
        "path/to/annotations/mask2.png",
    ),
];
let dataset = ImageFolderDataset::new_segmentation_with_items(
    items,
    &amp;[
        "cat", // 0
        "dog", // 1
        "background", // 2
    ],
)
.unwrap();</code></pre>
<pre><code class="language-rust  ignore">// Create an object detection dataset from a COCO dataset. Currently only
// the import of object detection data (bounding boxes) is supported.
//
// COCO offers separate annotation and image archives for training and
// validation, paths to the unpacked files need to be passed as parameters:

let dataset = ImageFolderDataset::new_coco_detection(
    "/path/to/coco/instances_train2017.json",
    "/path/to/coco/images/train2017"
)
.unwrap();
</code></pre>
<h3 id="comma-separated-values-csv"><a class="header" href="#comma-separated-values-csv">Comma-Separated Values (CSV)</a></h3>
<p>Loading records from a simple CSV file in-memory is simple with the <code>InMemDataset</code>:</p>
<pre><code class="language-rust  ignore">// Build dataset from csv with tab ('\t') delimiter.
// The reader can be configured for your particular file.
let mut rdr = csv::ReaderBuilder::new();
let rdr = rdr.delimiter(b'\t');

let dataset = InMemDataset::from_csv("path/to/csv", rdr).unwrap();</code></pre>
<p>Note that this requires the <code>csv</code> crate.</p>
<p><strong>What about streaming datasets?</strong></p>
<p>There is no streaming dataset API with Mabor, and this is by design! The learner struct will iterate
multiple times over the dataset and only checkpoint when done. You can consider the length of the
dataset as the number of iterations before performing checkpointing and running the validation.
There is nothing stopping you from returning different items even when called with the same <code>index</code>
multiple times.</p>
<h2 id="how-is-the-dataset-used"><a class="header" href="#how-is-the-dataset-used">How Is The Dataset Used?</a></h2>
<p>During training, the dataset is used to access the data samples and, for most use cases in
supervised learning, their corresponding ground-truth labels. Remember that the <code>Dataset</code> trait
implementation is responsible to retrieve the data from its source, usually some sort of data
storage. At this point, the dataset could be naively iterated over to provide the model a single
sample to process at a time, but this is not very efficient.</p>
<p>Instead, we collect multiple samples that the model can process as a <em>batch</em> to fully leverage
modern hardware (e.g., GPUs - which have impressing parallel processing capabilities). Since each
data sample in the dataset can be collected independently, the data loading is typically done in
parallel to further speed things up. In this case, we parallelize the data loading using a
multi-threaded <code>BatchDataLoader</code> to obtain a sequence of items from the <code>Dataset</code> implementation.
Finally, the sequence of items is combined into a batched tensor that can be used as input to a
model with the <code>Batcher</code> trait implementation. Other tensor operations can be performed during this
step to prepare the batch data, as is done <a href="basic-workflow/data.html">in the basic workflow guide</a>.
The process is illustrated in the figure below for the MNIST dataset.</p>
<img title="Burn Data Loading Pipeline" alt="Burn Data Loading Pipeline" src="building-blocks/dataset.png">
<p>Although we have conveniently implemented the
<a href="https://github.com/tracel-ai/burn/blob/main/crates/burn-dataset/src/vision/mnist.rs"><code>MnistDataset</code></a>
used in the guide, we'll go over its implementation to demonstrate how the <code>Dataset</code> and <code>Batcher</code>
traits are used.</p>
<p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> of handwritten digits has a training set of
60,000 examples and a test set of 10,000 examples. A single item in the dataset is represented by a
\(28 \times 28\) pixels black-and-white image (stored as raw bytes) with its corresponding label
(a digit between \(0\) and \(9\)). This is defined by the <code>MnistItemRaw</code> struct.</p>
<pre><code class="language-rust  ignore"><span class="boring">#[derive(Deserialize, Debug, Clone)]
</span>struct MnistItemRaw {
    pub image_bytes: Vec&lt;u8&gt;,
    pub label: u8,
}</code></pre>
<p>With single-channel images of such low resolution, the entire training and test sets can be loaded
in memory at once. Therefore, we leverage the already existing <code>InMemDataset</code> to retrieve the raw
images and labels data. At this point, the image data is still just a bunch of bytes, but we want to
retrieve the <em>structured</em> image data in its intended form. For that, we can define a <code>MapperDataset</code>
that transforms the raw image bytes to a 2D array image (which we convert to float while we're at
it).</p>
<pre><code class="language-rust  ignore">const WIDTH: usize = 28;
const HEIGHT: usize = 28;

<span class="boring">/// MNIST item.
</span><span class="boring">#[derive(Deserialize, Serialize, Debug, Clone)]
</span>pub struct MnistItem {
    /// Image as a 2D array of floats.
    pub image: [[f32; WIDTH]; HEIGHT],

    /// Label of the image.
    pub label: u8,
}

struct BytesToImage;

impl Mapper&lt;MnistItemRaw, MnistItem&gt; for BytesToImage {
    /// Convert a raw MNIST item (image bytes) to a MNIST item (2D array image).
    fn map(&amp;self, item: &amp;MnistItemRaw) -&gt; MnistItem {
        // Ensure the image dimensions are correct.
        debug_assert_eq!(item.image_bytes.len(), WIDTH * HEIGHT);

        // Convert the image to a 2D array of floats.
        let mut image_array = [[0f32; WIDTH]; HEIGHT];
        for (i, pixel) in item.image_bytes.iter().enumerate() {
            let x = i % WIDTH;
            let y = i / HEIGHT;
            image_array[y][x] = *pixel as f32;
        }

        MnistItem {
            image: image_array,
            label: item.label,
        }
    }
}

type MappedDataset = MapperDataset&lt;InMemDataset&lt;MnistItemRaw&gt;, BytesToImage, MnistItemRaw&gt;;

<span class="boring">/// The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000
</span><span class="boring">/// images per class. There are 60,000 training images and 10,000 test images.
</span><span class="boring">///
</span><span class="boring">/// The data is downloaded from the web from the [CVDF mirror](https://github.com/cvdfoundation/mnist).
</span>pub struct MnistDataset {
    dataset: MappedDataset,
}</code></pre>
<p>To construct the <code>MnistDataset</code>, the data source must be parsed into the expected <code>MappedDataset</code>
type. Since both the train and test sets use the same file format, we can separate the functionality
to load the <code>train()</code> and <code>test()</code> dataset.</p>
<pre><code class="language-rust  ignore">
impl MnistDataset {
    /// Creates a new train dataset.
    pub fn train() -&gt; Self {
        Self::new("train")
    }

    /// Creates a new test dataset.
    pub fn test() -&gt; Self {
        Self::new("test")
    }

    fn new(split: &amp;str) -&gt; Self {
        // Download dataset
        let root = MnistDataset::download(split);

        // Parse data as vector of images bytes and vector of labels
        let images: Vec&lt;Vec&lt;u8&gt;&gt; = MnistDataset::read_images(&amp;root, split);
        let labels: Vec&lt;u8&gt; = MnistDataset::read_labels(&amp;root, split);

        // Collect as vector of MnistItemRaw
        let items: Vec&lt;_&gt; = images
            .into_iter()
            .zip(labels)
            .map(|(image_bytes, label)| MnistItemRaw { image_bytes, label })
            .collect();

        // Create the MapperDataset for InMemDataset&lt;MnistItemRaw&gt; to transform
        // items (MnistItemRaw -&gt; MnistItem)
        let dataset = InMemDataset::new(items);
        let dataset = MapperDataset::new(dataset, BytesToImage);

        Self { dataset }
    }

<span class="boring">   /// Download the MNIST dataset files from the web.
</span><span class="boring">   /// Panics if the download cannot be completed or the content of the file cannot be written to disk.
</span><span class="boring">   fn download(split: &amp;str) -&gt; PathBuf {
</span><span class="boring">       // Dataset files are stored un the mabor-dataset cache directory
</span><span class="boring">       let cache_dir = dirs::home_dir()
</span><span class="boring">           .expect("Could not get home directory")
</span><span class="boring">           .join(".cache")
</span><span class="boring">           .join("mabor-dataset");
</span><span class="boring">       let split_dir = cache_dir.join("mnist").join(split);
</span><span class="boring">
</span><span class="boring">       if !split_dir.exists() {
</span><span class="boring">           create_dir_all(&amp;split_dir).expect("Failed to create base directory");
</span><span class="boring">       }
</span><span class="boring">
</span><span class="boring">       // Download split files
</span><span class="boring">       match split {
</span><span class="boring">           "train" =&gt; {
</span><span class="boring">               MnistDataset::download_file(TRAIN_IMAGES, &amp;split_dir);
</span><span class="boring">               MnistDataset::download_file(TRAIN_LABELS, &amp;split_dir);
</span><span class="boring">           }
</span><span class="boring">           "test" =&gt; {
</span><span class="boring">               MnistDataset::download_file(TEST_IMAGES, &amp;split_dir);
</span><span class="boring">               MnistDataset::download_file(TEST_LABELS, &amp;split_dir);
</span><span class="boring">           }
</span><span class="boring">           _ =&gt; panic!("Invalid split specified {}", split),
</span><span class="boring">       };
</span><span class="boring">
</span><span class="boring">       split_dir
</span><span class="boring">   }
</span><span class="boring">
</span><span class="boring">   /// Download a file from the MNIST dataset URL to the destination directory.
</span><span class="boring">   /// File download progress is reported with the help of a [progress bar](indicatif).
</span><span class="boring">   fn download_file&lt;P: AsRef&lt;Path&gt;&gt;(name: &amp;str, dest_dir: &amp;P) -&gt; PathBuf {
</span><span class="boring">       // Output file name
</span><span class="boring">       let file_name = dest_dir.as_ref().join(name);
</span><span class="boring">
</span><span class="boring">       if !file_name.exists() {
</span><span class="boring">           // Download gzip file
</span><span class="boring">           let bytes = download_file_as_bytes(&amp;format!("{URL}{name}.gz"), name);
</span><span class="boring">
</span><span class="boring">           // Create file to write the downloaded content to
</span><span class="boring">           let mut output_file = File::create(&amp;file_name).unwrap();
</span><span class="boring">
</span><span class="boring">           // Decode gzip file content and write to disk
</span><span class="boring">           let mut gz_buffer = GzDecoder::new(&amp;bytes[..]);
</span><span class="boring">           std::io::copy(&amp;mut gz_buffer, &amp;mut output_file).unwrap();
</span><span class="boring">       }
</span><span class="boring">
</span><span class="boring">       file_name
</span><span class="boring">   }
</span><span class="boring">
</span><span class="boring">   /// Read images at the provided path for the specified split.
</span><span class="boring">   /// Each image is a vector of bytes.
</span><span class="boring">   fn read_images&lt;P: AsRef&lt;Path&gt;&gt;(root: &amp;P, split: &amp;str) -&gt; Vec&lt;Vec&lt;u8&gt;&gt; {
</span><span class="boring">       let file_name = if split == "train" {
</span><span class="boring">           TRAIN_IMAGES
</span><span class="boring">       } else {
</span><span class="boring">           TEST_IMAGES
</span><span class="boring">       };
</span><span class="boring">       let file_name = root.as_ref().join(file_name);
</span><span class="boring">
</span><span class="boring">       // Read number of images from 16-byte header metadata
</span><span class="boring">       let mut f = File::open(file_name).unwrap();
</span><span class="boring">       let mut buf = [0u8; 4];
</span><span class="boring">       let _ = f.seek(SeekFrom::Start(4)).unwrap();
</span><span class="boring">       f.read_exact(&amp;mut buf)
</span><span class="boring">           .expect("Should be able to read image file header");
</span><span class="boring">       let size = u32::from_be_bytes(buf);
</span><span class="boring">
</span><span class="boring">       let mut buf_images: Vec&lt;u8&gt; = vec![0u8; WIDTH * HEIGHT * (size as usize)];
</span><span class="boring">       let _ = f.seek(SeekFrom::Start(16)).unwrap();
</span><span class="boring">       f.read_exact(&amp;mut buf_images)
</span><span class="boring">           .expect("Should be able to read image file header");
</span><span class="boring">
</span><span class="boring">       buf_images
</span><span class="boring">           .chunks(WIDTH * HEIGHT)
</span><span class="boring">           .map(|chunk| chunk.to_vec())
</span><span class="boring">           .collect()
</span><span class="boring">   }
</span><span class="boring">
</span><span class="boring">   /// Read labels at the provided path for the specified split.
</span><span class="boring">   fn read_labels&lt;P: AsRef&lt;Path&gt;&gt;(root: &amp;P, split: &amp;str) -&gt; Vec&lt;u8&gt; {
</span><span class="boring">       let file_name = if split == "train" {
</span><span class="boring">           TRAIN_LABELS
</span><span class="boring">       } else {
</span><span class="boring">           TEST_LABELS
</span><span class="boring">       };
</span><span class="boring">       let file_name = root.as_ref().join(file_name);
</span><span class="boring">
</span><span class="boring">       // Read number of labels from 8-byte header metadata
</span><span class="boring">       let mut f = File::open(file_name).unwrap();
</span><span class="boring">       let mut buf = [0u8; 4];
</span><span class="boring">       let _ = f.seek(SeekFrom::Start(4)).unwrap();
</span><span class="boring">       f.read_exact(&amp;mut buf)
</span><span class="boring">           .expect("Should be able to read label file header");
</span><span class="boring">       let size = u32::from_be_bytes(buf);
</span><span class="boring">
</span><span class="boring">       let mut buf_labels: Vec&lt;u8&gt; = vec![0u8; size as usize];
</span><span class="boring">       let _ = f.seek(SeekFrom::Start(8)).unwrap();
</span><span class="boring">       f.read_exact(&amp;mut buf_labels)
</span><span class="boring">           .expect("Should be able to read labels from file");
</span><span class="boring">
</span><span class="boring">       buf_labels
</span><span class="boring">   }
</span>}</code></pre>
<p>Since the <code>MnistDataset</code> simply wraps a <code>MapperDataset</code> instance with <code>InMemDataset</code>, we can easily
implement the <code>Dataset</code> trait.</p>
<pre><code class="language-rust  ignore">impl Dataset&lt;MnistItem&gt; for MnistDataset {
    fn get(&amp;self, index: usize) -&gt; Option&lt;MnistItem&gt; {
        self.dataset.get(index)
    }

    fn len(&amp;self) -&gt; usize {
        self.dataset.len()
    }
}</code></pre>
<p>The only thing missing now is the <code>Batcher</code>, which we already went over
<a href="basic-workflow/data.html">in the basic workflow guide</a>. The <code>Batcher</code> takes a list of <code>MnistItem</code>
retrieved by the dataloader as input and returns a batch of images as a 3D tensor along with their
targets.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-training-loops"><a class="header" href="#custom-training-loops">Custom Training Loops</a></h1>
<p>Even though Mabor comes with a project dedicated to simplifying training, it doesn't mean that you
have to use it. Sometimes you may have special needs for your training, and it might be faster to
just reimplement the training loop yourself. Also, you may just prefer implementing your own
training loop instead of using a pre-built one in general.</p>
<p>Mabor's got you covered!</p>
<p>We will start from the same example shown in the <a href="basic-workflow/index.htm">basic workflow</a> section, but
without using the <code>Learner</code> struct.</p>
<pre><code class="language-rust  ignore">#[derive(Config)]
pub struct MnistTrainingConfig {
    #[config(default = 10)]
    pub num_epochs: usize,
    #[config(default = 64)]
    pub batch_size: usize,
    #[config(default = 4)]
    pub num_workers: usize,
    #[config(default = 42)]
    pub seed: u64,
    #[config(default = 1e-4)]
    pub lr: f64,
    pub model: ModelConfig,
    pub optimizer: AdamConfig,
}

pub fn run&lt;B: AutodiffBackend&gt;(device: &amp;B::Device) {
    // Create the configuration.
    let config_model = ModelConfig::new(10, 1024);
    let config_optimizer = AdamConfig::new();
    let config = MnistTrainingConfig::new(config_model, config_optimizer);

    B::seed(config.seed);

    // Create the model and optimizer.
    let mut model = config.model.init::&lt;B&gt;(&amp;device);
    let mut optim = config.optimizer.init();

    // Create the batcher.
    let batcher = MnistBatcher::default();

    // Create the dataloaders.
    let dataloader_train = DataLoaderBuilder::new(batcher.clone())
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MnistDataset::train());

    let dataloader_test = DataLoaderBuilder::new(batcher)
        .batch_size(config.batch_size)
        .shuffle(config.seed)
        .num_workers(config.num_workers)
        .build(MnistDataset::test());

    ...
}</code></pre>
<p>As seen with the previous example, setting up the configurations and the dataloader hasn't changed.
Now, let's move forward and write our own training loop:</p>
<pre><code class="language-rust  ignore">pub fn run&lt;B: AutodiffBackend&gt;(device: B::Device) {
    ...

    // Iterate over our training and validation loop for X epochs.
    for epoch in 1..config.num_epochs + 1 {
        // Implement our training loop.
        for (iteration, batch) in dataloader_train.iter().enumerate() {
            let output = model.forward(batch.images);
            let loss = CrossEntropyLoss::new(None, &amp;output.device())
                .forward(output.clone(), batch.targets.clone());
            let accuracy = accuracy(output, batch.targets);

            println!(
                "[Train - Epoch {} - Iteration {}] Loss {:.3} | Accuracy {:.3} %",
                epoch,
                iteration,
                loss.clone().into_scalar(),
                accuracy,
            );

            // Gradients for the current backward pass
            let grads = loss.backward();
            // Gradients linked to each parameter of the model.
            let grads = GradientsParams::from_grads(grads, &amp;model);
            // Update the model using the optimizer.
            model = optim.step(config.lr, model, grads);
        }

        // Get the model without autodiff.
        let model_valid = model.valid();

        // Implement our validation loop.
        for (iteration, batch) in dataloader_test.iter().enumerate() {
            let output = model_valid.forward(batch.images);
            let loss = CrossEntropyLoss::new(None, &amp;output.device())
                .forward(output.clone(), batch.targets.clone());
            let accuracy = accuracy(output, batch.targets);

            println!(
                "[Valid - Epoch {} - Iteration {}] Loss {} | Accuracy {}",
                epoch,
                iteration,
                loss.clone().into_scalar(),
                accuracy,
            );
        }
    }
}</code></pre>
<p>In the previous code snippet, we can observe that the loop starts from epoch <code>1</code> and goes up to
<code>num_epochs</code>. Within each epoch, we iterate over the training dataloader. During this process, we
execute the forward pass, which is necessary for computing both the loss and accuracy. To maintain
simplicity, we print the results to stdout.</p>
<p>Upon obtaining the loss, we can invoke the <code>backward()</code> function, which returns the gradients
specific to each variable. It's important to note that we need to map these gradients to their
corresponding parameters using the <code>GradientsParams</code> type. This step is essential because you might
run multiple different autodiff graphs and accumulate gradients for each parameter id.</p>
<p>Finally, we can perform the optimization step using the learning rate, the model, and the computed
gradients. It's worth mentioning that, unlike PyTorch, there's no need to register the gradients
with the optimizer, nor do you have to call <code>zero_grad</code>. The gradients are automatically consumed
during the optimization step. If you're interested in gradient accumulation, you can easily achieve
this by using the <code>GradientsAccumulator</code>.</p>
<pre><code class="language-rust  ignore">let mut accumulator = GradientsAccumulator::new();
let grads = model.backward();
let grads = GradientsParams::from_grads(grads, &amp;model);
accumulator.accumulate(&amp;model, grads); ...
let grads = accumulator.grads(); // Pop the accumulated gradients.</code></pre>
<p>Note that after each epoch, we include a validation loop to assess our model's performance on
previously unseen data. To disable gradient tracking during this validation step, we can invoke
<code>model.valid()</code>, which provides a model on the inner backend without autodiff capabilities. It's
important to emphasize that we've declared our validation batcher to be on the inner backend,
specifically <code>MnistBatcher&lt;B::InnerBackend&gt;</code>; not using <code>model.valid()</code> will result in a compilation
error.</p>
<p>You can find the code above available as an
<a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-training-loop">example</a> for you to
test.</p>
<h2 id="multiple-optimizers"><a class="header" href="#multiple-optimizers">Multiple optimizers</a></h2>
<p>It's common practice to set different learning rates, optimizer parameters, or use different optimizers entirely, for different parts
of a model. In Mabor, each <code>GradientParams</code> can contain only a subset of gradients to actually apply with an optimizer.
This allows you to flexibly mix and match optimizers!</p>
<pre><code class="language-rust ignore">// Start with calculating all gradients
let grads = loss.backward();

// Now split the gradients into various parts.
let grads_conv1 = GradientParams::from_module(&amp;mut grads, &amp;model.conv1);
let grads_conv2 = GradientParams::from_module(&amp;mut grads, &amp;model.conv2);

// You can step the model with these gradients, using different learning
// rates for each param. You could also use an entirely different optimizer here!
model = optim.step(config.lr * 2.0, model, grads_conv1);
model = optim.step(config.lr * 4.0, model, grads_conv2);

// For even more granular control you can split off individual parameter
// eg. a linear bias usually needs a smaller learning rate.
if let Some(bias) == model.linear1.bias {
    let grads_bias = GradientParams::from_params(&amp;mut grads, &amp;model.linear1, &amp;[bias.id]);
    model = optim.step(config.lr * 0.1, model, grads_bias);
}

// Note that above calls remove gradients, so we can just get all "remaining" gradients.
let grads = GradientsParams::from_grads(grads, &amp;model);
model = optim.step(config.lr, model, grads);</code></pre>
<h2 id="custom-type"><a class="header" href="#custom-type">Custom Type</a></h2>
<p>The explanations above demonstrate how to create a basic training loop. However, you may find it
beneficial to organize your program using intermediary types. There are various ways to do this, but
it requires getting comfortable with generics.</p>
<p>If you wish to group the optimizer and the model into the same structure, you have several options.
It's important to note that the optimizer trait depends on both the <code>AutodiffModule</code> trait and the
<code>AutodiffBackend</code> trait, while the module only depends on the <code>AutodiffBackend</code> trait.</p>
<p>Here's a closer look at how you can create your types:</p>
<p><strong>Create a struct that is generic over the backend and the optimizer, with a predefined model.</strong></p>
<pre><code class="language-rust  ignore">struct Learner&lt;B, O&gt;
where
    B: AutodiffBackend,
{
    model: Model&lt;B&gt;,
    optim: O,
}</code></pre>
<p>This is quite straightforward. You can be generic over the backend since it's used with the concrete
type <code>Model</code> in this case.</p>
<p><strong>Create a struct that is generic over the model and the optimizer.</strong></p>
<pre><code class="language-rust  ignore">struct Learner&lt;M, O&gt; {
    model: M,
    optim: O,
}</code></pre>
<p>This option is a quite intuitive way to declare the struct. You don't need to write type constraints
with a <code>where</code> statement when defining a struct; you can wait until you implement the actual
function. However, with this struct, you may encounter some issues when trying to implement code
blocks to your struct.</p>
<pre><code class="language-rust  ignore">impl&lt;B, M, O&gt; Learner&lt;M, O&gt;
where
    B: AutodiffBackend,
    M: AutodiffModule&lt;B&gt;,
    O: Optimizer&lt;M, B&gt;,
{
    pub fn step(&amp;mut self, _batch: MnistBatch&lt;B&gt;) {
        //
    }
}</code></pre>
<p>This will result in the following compilation error:</p>
<pre><code class="language-console">1. the type parameter `B` is not constrained by the impl trait, self type, or predicates
   unconstrained type parameter [E0207]
</code></pre>
<p>To resolve this issue, you have two options. The first one is to make your function generic over
the backend and add your trait constraint within its definition:</p>
<pre><code class="language-rust  ignore">#[allow(dead_code)]
impl&lt;M, O&gt; Learner2&lt;M, O&gt; {
    pub fn step&lt;B: AutodiffBackend&gt;(&amp;mut self, _batch: MnistBatch&lt;B&gt;)
    where
        B: AutodiffBackend,
        M: AutodiffModule&lt;B&gt;,
        O: Optimizer&lt;M, B&gt;,
    {
        //
    }
}</code></pre>
<p>However, some people may prefer to have the constraints on the implementation block itself. In that
case, you can make your struct generic over the backend using <code>PhantomData&lt;B&gt;</code>.</p>
<p><strong>Create a struct that is generic over the backend, the model, and the optimizer.</strong></p>
<pre><code class="language-rust  ignore">struct Learner3&lt;B, M, O&gt; {
    model: M,
    optim: O,
    _b: PhantomData&lt;B&gt;,
}</code></pre>
<p>You might wonder why <code>PhantomData</code> is required. Each generic argument must be used as a field when
declaring a struct. When you don't need the generic argument, you can use <code>PhantomData</code> to mark it
as a zero sized type.</p>
<p>These are just some suggestions on how to define your own types, but you are free to use any pattern
that you prefer.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="saving-and-loading-models"><a class="header" href="#saving-and-loading-models">Saving and Loading Models</a></h1>
<p>Saving your trained machine learning model is quite easy, no matter the output format you choose. As
mentioned in the <a href="building-blocks/record.html">Record</a> section, different formats are supported to
serialize/deserialize models. By default, we use the <code>NamedMpkFileRecorder</code> which uses the
<a href="https://msgpack.org/">MessagePack</a> binary serialization format with the help of
<a href="https://docs.rs/rmp-serde/">rmp_serde</a>.</p>
<pre><code class="language-rust  ignore">// Save model in MessagePack format with full precision
let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
    .save_file(model_path, &amp;recorder)
    .expect("Should be able to save the model");</code></pre>
<p>Note that the file extension is automatically handled by the recorder depending on the one you
choose. Therefore, only the file path and base name should be provided.</p>
<p>Now that you have a trained model saved to your disk, you can easily load it in a similar fashion.</p>
<pre><code class="language-rust  ignore">// Load model in full precision from MessagePack file
let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model = model
    .load_file(model_path, &amp;recorder, device)
    .expect("Should be able to load the model weights from the provided file");</code></pre>
<p><strong>Note:</strong> models can be saved in different output formats, just make sure you are using the correct
recorder type when loading the saved model. Type conversion between different precision settings is
automatically handled, but formats are not interchangeable. A model can be loaded from one format
and saved to another format, just as long as you load it back with the new recorder type afterwards.</p>
<h2 id="initialization-from-recorded-weights"><a class="header" href="#initialization-from-recorded-weights">Initialization from Recorded Weights</a></h2>
<p>The most straightforward way to load weights for a module is simply by using the generated method
<a href="../../docs/burn/module/trait.Module.html#tymethod.load_record">load_record</a>. Note that
parameter initialization is lazy, therefore no actual tensor allocation and GPU/CPU kernels are
executed before the module is used. This means that you can use <code>init(device)</code> followed by
<code>load_record(record)</code> without any meaningful performance cost.</p>
<pre><code class="language-rust  ignore">// Create a dummy initialized model to save
let device = Default::default();
let model = Model::&lt;MyBackend&gt;::init(&amp;device);

// Save model in MessagePack format with full precision
let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
    .save_file(model_path, &amp;recorder)
    .expect("Should be able to save the model");</code></pre>
<p>Afterwards, the model can just as easily be loaded from the record saved on disk.</p>
<pre><code class="language-rust  ignore">// Load model record on the backend's default device
let record: ModelRecord&lt;MyBackend&gt; = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::new()
    .load(model_path.into(), &amp;device)
    .expect("Should be able to load the model weights from the provided file");

// Initialize a new model with the loaded record/weights
let model = Model::init(&amp;device).load_record(record);</code></pre>
<h2 id="no-storage-no-problem"><a class="header" href="#no-storage-no-problem">No Storage, No Problem!</a></h2>
<p>For applications where file storage may not be available (or desired) at runtime, you can use the
<code>BinBytesRecorder</code>.</p>
<p>In the previous examples we used a <code>FileRecorder</code> based on the MessagePack format, which could be
replaced with <a href="building-blocks/record.html#recorder">another file recorder</a> of your choice. To embed
a model as part of your runtime application, first save the model to a binary file with
<code>BinFileRecorder</code>.</p>
<pre><code class="language-rust  ignore">// Save model in binary format with full precision
let recorder = BinFileRecorder::&lt;FullPrecisionSettings&gt;::new();
model
    .save_file(model_path, &amp;recorder)
    .expect("Should be able to save the model");</code></pre>
<p>Then, in your final application, include the model and use the <code>BinBytesRecorder</code> to load it.</p>
<p>Embedding the model as part of your application is especially useful for smaller models but not
recommended for very large models as it would significantly increase the binary size as well as
consume a lot more memory at runtime.</p>
<pre><code class="language-rust  ignore">// Include the model file as a reference to a byte array
static MODEL_BYTES: &amp;[u8] = include_bytes!("path/to/model.bin");

// Load model binary record in full precision
let record = BinBytesRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(MODEL_BYTES.to_vec(), device)
    .expect("Should be able to load model the model weights from bytes");

// Load that record with the model
model.load_record(record);</code></pre>
<p>This example assumes that the model was already created before loading the model record. If instead
you want to skip the random initialization and directly initialize the weights with the provided
record, you could adapt this like the <a href="saving-and-loading.html#initialization-from-recorded-weights">previous example</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="importing-models"><a class="header" href="#importing-models">Importing Models</a></h1>
<p>Mabor supports importing models from other frameworks and file formats, enabling you to use pre-trained weights in your Mabor applications.</p>
<h2 id="supported-formats"><a class="header" href="#supported-formats">Supported Formats</a></h2>
<p>Mabor currently supports three primary model import formats:</p>
<div class="table-wrapper"><table><thead><tr><th>Format</th><th>Description</th><th>Use Case</th></tr></thead><tbody>
<tr><td><a href="import/onnx-model.html"><strong>ONNX</strong></a></td><td>Open Neural Network Exchange format</td><td>Direct import of complete model architectures and weights from any framework that supports ONNX export</td></tr>
<tr><td><a href="import/pytorch-model.html"><strong>PyTorch</strong></a></td><td>PyTorch weights (.pt, .pth)</td><td>Loading weights from PyTorch models into a matching Mabor architecture</td></tr>
<tr><td><a href="import/safetensors-model.html"><strong>Safetensors</strong></a></td><td>Hugging Face's model serialization format</td><td>Loading a model's tensor weights into a matching Mabor architecture</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="importing-onnx-models-in-burn"><a class="header" href="#importing-onnx-models-in-burn">Importing ONNX Models in Mabor</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>As deep learning evolves, interoperability between frameworks becomes crucial. Mabor, a modern deep
learning framework in Rust, provides robust support for importing models from other popular
frameworks. This section focuses on importing
<a href="https://onnx.ai/onnx/intro/index.html">ONNX (Open Neural Network Exchange)</a> models into Mabor,
enabling you to leverage pre-trained models in your Rust-based deep learning projects.</p>
<h2 id="why-import-models"><a class="header" href="#why-import-models">Why Import Models?</a></h2>
<p>Importing pre-trained models offers several advantages:</p>
<ol>
<li><strong>Time-saving</strong>: Skip the resource-intensive process of training models from scratch.</li>
<li><strong>Access to state-of-the-art architectures</strong>: Utilize cutting-edge models developed by
researchers and industry leaders.</li>
<li><strong>Transfer learning</strong>: Fine-tune imported models for your specific tasks, benefiting from
knowledge transfer.</li>
<li><strong>Consistency across frameworks</strong>: Maintain consistent performance when moving between
frameworks.</li>
</ol>
<h2 id="understanding-onnx"><a class="header" href="#understanding-onnx">Understanding ONNX</a></h2>
<p>ONNX (Open Neural Network Exchange) is an open format designed to represent machine learning models
with these key features:</p>
<ul>
<li><strong>Framework agnostic</strong>: Provides a common format that works across various deep learning
frameworks.</li>
<li><strong>Comprehensive representation</strong>: Captures both the model architecture and trained weights.</li>
<li><strong>Wide support</strong>: Compatible with popular frameworks like PyTorch, TensorFlow, and scikit-learn.</li>
</ul>
<p>This standardization allows seamless movement of models between different frameworks and deployment
environments.</p>
<h2 id="burns-onnx-support"><a class="header" href="#burns-onnx-support">Mabor's ONNX Support</a></h2>
<p>Mabor's approach to ONNX import offers unique advantages:</p>
<ol>
<li><strong>Native Rust code generation</strong>: Translates ONNX models into Rust source code for deep
integration with Mabor's ecosystem.</li>
<li><strong>Compile-time optimization</strong>: Leverages the Rust compiler to optimize the generated code,
potentially improving performance.</li>
<li><strong>No runtime dependency</strong>: Eliminates the need for an ONNX runtime, unlike many other solutions.</li>
<li><strong>Trainability</strong>: Allows imported models to be further trained or fine-tuned using Mabor.</li>
<li><strong>Portability</strong>: Enables compilation for various targets, including WebAssembly and embedded
devices.</li>
<li><strong>Backend flexibility</strong>: Works with any of Mabor's supported backends.</li>
</ol>
<h2 id="onnx-compatibility"><a class="header" href="#onnx-compatibility">ONNX Compatibility</a></h2>
<p>Mabor requires ONNX models to use <strong>opset version 16 or higher</strong>. If your model uses an older
version, you'll need to upgrade it using the ONNX version converter.</p>
<h3 id="upgrading-onnx-models"><a class="header" href="#upgrading-onnx-models">Upgrading ONNX Models</a></h3>
<p>There are two simple ways to upgrade your ONNX models to the required opset version:</p>
<p>Option 1: Use the provided utility script:</p>
<pre><code>uv run --script https://raw.githubusercontent.com/tracel-ai/mabor/refs/heads/main/crates/mabor-import/onnx_opset_upgrade.py
</code></pre>
<p>Option 2: Use a custom Python script:</p>
<pre><code class="language-python">import onnx
from onnx import version_converter, shape_inference

# Load your ONNX model
model = onnx.load('path/to/your/model.onnx')

# Convert the model to opset version 16
upgraded_model = version_converter.convert_version(model, 16)

# Apply shape inference to the upgraded model
inferred_model = shape_inference.infer_shapes(upgraded_model)

# Save the converted model
onnx.save(inferred_model, 'upgraded_model.onnx')
</code></pre>
<h2 id="step-by-step-guide"><a class="header" href="#step-by-step-guide">Step-by-Step Guide</a></h2>
<p>Follow these steps to import an ONNX model into your Mabor project:</p>
<h3 id="step-1-update-buildrs"><a class="header" href="#step-1-update-buildrs">Step 1: Update <code>build.rs</code></a></h3>
<p>First, add the <code>mabor-import</code> crate to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[build-dependencies]
mabor-import = "~0.18"
</code></pre>
<p>Then, in your <code>build.rs</code> file:</p>
<pre><pre class="playground"><code class="language-rust">use mabor_import::onnx::ModelGen;

fn main() {
    ModelGen::new()
        .input("src/model/my_model.onnx")
        .out_dir("model/")
        .run_from_script();
}</code></pre></pre>
<p>This generates Rust code from your ONNX model during the build process.</p>
<h3 id="step-2-modify-modrs"><a class="header" href="#step-2-modify-modrs">Step 2: Modify <code>mod.rs</code></a></h3>
<p>In your <code>src/model/mod.rs</code> file, include the generated code:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub mod my_model {
    include!(concat!(env!("OUT_DIR"), "/model/my_model.rs"));
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-3-use-the-imported-model"><a class="header" href="#step-3-use-the-imported-model">Step 3: Use the Imported Model</a></h3>
<p>Now you can use the imported model in your code:</p>
<pre><pre class="playground"><code class="language-rust">use mabor::tensor;
use mabor_ndarray::{NdArray, NdArrayDevice};
use model::my_model::Model;

fn main() {
    let device = NdArrayDevice::default();

    // Create model instance and load weights from target dir default device
    let model: Model&lt;NdArray&lt;f32&gt;&gt; = Model::default();

    // Create input tensor (replace with your actual input)
    let input = tensor::Tensor::&lt;NdArray&lt;f32&gt;, 4&gt;::zeros([1, 3, 224, 224], &amp;device);

    // Perform inference
    let output = model.forward(input);

    println!("Model output: {:?}", output);
}</code></pre></pre>
<h2 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h2>
<p>The <code>ModelGen</code> struct provides several configuration options:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ModelGen::new()
    .input("path/to/model.onnx")
    .out_dir("model/")
    .record_type(RecordType::NamedMpk)
    .half_precision(false)
    .embed_states(false)
    .run_from_script();
<span class="boring">}</span></code></pre></pre>
<ul>
<li><code>record_type</code>: Defines the format for storing weights (Bincode, NamedMpk, NamedMpkGz, or
PrettyJson).</li>
<li><code>half_precision</code>: Reduces model size by using half-precision (f16) for weights.</li>
<li><code>embed_states</code>: Embeds model weights directly in the generated Rust code (requires record type
<code>Bincode</code>).</li>
</ul>
<h2 id="loading-and-using-models"><a class="header" href="#loading-and-using-models">Loading and Using Models</a></h2>
<p>Depending on your configuration, you can load models in several ways:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Create a new model instance with device
// (initializes weights randomly and lazily; load weights via `load_record` afterward)
let model = Model::&lt;Backend&gt;::new(&amp;device);

// Load from a file
// (file type should match the record type specified in `ModelGen`)
let model = Model::&lt;Backend&gt;::from_file("path/to/weights", &amp;device);

// Load from embedded weights (if embed_states was true)
let model = Model::&lt;Backend&gt;::from_embedded(&amp;device);

// Load from the output directory with default device (useful for testing)
let model = Model::&lt;Backend&gt;::default();
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<p>Common issues and solutions:</p>
<ol>
<li>
<p><strong>Unsupported ONNX operator</strong>: Check the
<a href="https://github.com/tracel-ai/burn/blob/main/crates/burn-import/SUPPORTED-ONNX-OPS.md">list of supported ONNX operators</a>.
You may need to simplify your model or wait for support.</p>
</li>
<li>
<p><strong>Build errors</strong>: Ensure your <code>mabor-import</code> version matches your Mabor version and verify the ONNX
file path in <code>build.rs</code>.</p>
</li>
<li>
<p><strong>Runtime errors</strong>: Confirm that your input tensors match the expected shape and data type of
your model.</p>
</li>
<li>
<p><strong>Performance issues</strong>: Try using the <code>half_precision</code> option to reduce memory usage or
experiment with different <code>record_type</code> options.</p>
</li>
<li>
<p><strong>Viewing generated files</strong>: Find the generated Rust code and weights in the <code>OUT_DIR</code> directory
(usually <code>target/debug/build/&lt;project&gt;/out</code>).</p>
</li>
</ol>
<h2 id="examples-and-resources"><a class="header" href="#examples-and-resources">Examples and Resources</a></h2>
<p>For practical examples, check out:</p>
<ol>
<li><a href="https://github.com/tracel-ai/burn/tree/main/examples/onnx-inference">MNIST Inference Example</a></li>
<li><a href="https://github.com/tracel-ai/models/tree/main/squeezenet-burn">SqueezeNet Image Classification</a></li>
</ol>
<p>These demonstrate real-world usage of ONNX import in Mabor projects.</p>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>Importing ONNX models into Mabor combines the vast ecosystem of pre-trained models with Mabor's
performance and Rust's safety features. Following this guide, you can seamlessly integrate ONNX
models into your Mabor projects for inference, fine-tuning, or further development.</p>
<p>The <code>mabor-import</code> crate is actively developed, with ongoing work to support more ONNX operators and
improve performance. Stay tuned to the Mabor repository for updates!</p>
<hr>
<blockquote>
<p>🚨<strong>Note</strong>: The <code>mabor-import</code> crate is in active development. For the most up-to-date information
on supported ONNX operators, please refer to the
<a href="https://github.com/tracel-ai/burn/blob/main/crates/burn-import/SUPPORTED-ONNX-OPS.md">official documentation</a>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pytorch-model"><a class="header" href="#pytorch-model">PyTorch Model</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>Mabor supports importing model weights from PyTorch, whether you've trained your model in PyTorch or
want to use a pre-trained model. Mabor supports importing PyTorch model weights with <code>.pt</code> and
<code>.safetensors</code> file extensions. Compared to ONNX models, these files only contain the weights of the
model, so you will need to reconstruct the model architecture in Mabor.</p>
<p>This guide demonstrates the complete workflow for exporting models from PyTorch and importing them
into Mabor. You can also refer to this
<a href="https://dev.to/laggui/transitioning-from-pytorch-to-burn-45m">Transitioning From PyTorch to Mabor</a>
tutorial for importing a more complex model.</p>
<h2 id="exporting-models-to-pytorch-format"><a class="header" href="#exporting-models-to-pytorch-format">Exporting Models to PyTorch Format</a></h2>
<p>To export a PyTorch model correctly, you need to save only the model weights (state_dict) using the
<code>torch.save</code> function, not the entire model.</p>
<h3 id="example-exporting-a-pytorch-model"><a class="header" href="#example-exporting-a-pytorch-model">Example: Exporting a PyTorch Model</a></h3>
<pre><code class="language-python">import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(2, 2, (2,2))
        self.conv2 = nn.Conv2d(2, 2, (2,2), bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

if __name__ == "__main__":
    # Set seed for reproducibility
    torch.manual_seed(42)

    # Initialize model and ensure it's on CPU
    model = Net().to(torch.device("cpu"))

    # Extract model weights dictionary
    model_weights = model.state_dict()

    # Save only the weights, not the entire model
    torch.save(model_weights, "conv2d.pt")
</code></pre>
<p>If you accidentally save the entire model instead of just the weights, you may encounter errors
during import like:</p>
<pre><code>Failed to decode foobar: DeserializeError("Serde error: other error:
Missing source values for the 'foo1' field of type 'BarRecordItem'.
Please verify the source data and ensure the field name is correct")
</code></pre>
<h3 id="verifying-the-export"><a class="header" href="#verifying-the-export">Verifying the Export</a></h3>
<p>You can verify your exported model by viewing the <code>.pt</code> file in
<a href="https://github.com/lutzroeder/netron">Netron</a>, a neural network visualization tool. A properly
exported weights file will show a flat structure of tensors, while an incorrectly exported file will
display nested blocks representing the entire model architecture.</p>
<p>When viewing the exported model in Netron, you should see something like this:</p>
<p><img src="import/conv2d.svg" alt="image alt&gt;"></p>
<h2 id="importing-pytorch-models-into-burn"><a class="header" href="#importing-pytorch-models-into-burn">Importing PyTorch Models into Mabor</a></h2>
<p>Importing a PyTorch model into Mabor involves two main steps:</p>
<ol>
<li>Defining the model architecture in Mabor</li>
<li>Loading the weights from the exported PyTorch model</li>
</ol>
<h3 id="step-1-define-the-model-in-burn"><a class="header" href="#step-1-define-the-model-in-burn">Step 1: Define the Model in Mabor</a></h3>
<p>First, you need to create a Mabor model that matches the architecture of the model you exported:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mabor::{
    nn::conv::{Conv2d, Conv2dConfig},
    prelude::*,
};

#[derive(Module, Debug)]
pub struct Net&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
}

impl&lt;B: Backend&gt; Net&lt;B&gt; {
    /// Create a new model.
    pub fn init(device: &amp;B::Device) -&gt; Self {
        let conv1 = Conv2dConfig::new([2, 2], [2, 2])
            .init(device);
        let conv2 = Conv2dConfig::new([2, 2], [2, 2])
            .with_bias(false)
            .init(device);
        Self { conv1, conv2 }
    }

    /// Forward pass of the model.
    pub fn forward(&amp;self, x: Tensor&lt;B, 4&gt;) -&gt; Tensor&lt;B, 4&gt; {
        let x = self.conv1.forward(x);
        self.conv2.forward(x)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-load-the-weights"><a class="header" href="#step-2-load-the-weights">Step 2: Load the Weights</a></h3>
<p>You have two options for loading the weights:</p>
<h4 id="option-a-load-dynamically-at-runtime"><a class="header" href="#option-a-load-dynamically-at-runtime">Option A: Load Dynamically at Runtime</a></h4>
<p>This approach loads the PyTorch file directly at runtime, requiring the <code>mabor-import</code> dependency:</p>
<pre><pre class="playground"><code class="language-rust">use crate::model;
use mabor::record::{FullPrecisionSettings, Recorder};
use mabor_import::pytorch::PyTorchFileRecorder;

type Backend = mabor_ndarray::NdArray&lt;f32&gt;;

fn main() {
    let device = Default::default();

    // Load weights from PyTorch file
    let record = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
        .load("./conv2d.pt".into(), &amp;device)
        .expect("Should decode state successfully");

    // Initialize model and load weights
    let model = model::Net::&lt;Backend&gt;::init(&amp;device).load_record(record);
}</code></pre></pre>
<h4 id="option-b-pre-convert-to-burns-binary-format"><a class="header" href="#option-b-pre-convert-to-burns-binary-format">Option B: Pre-convert to Mabor's Binary Format</a></h4>
<p>This approach converts the PyTorch file to Mabor's optimized binary format during build time,
removing the runtime dependency on <code>mabor-import</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This code would go in build.rs or a separate tool

use crate::model;
use mabor::record::{FullPrecisionSettings, NamedMpkFileRecorder, Recorder};
use mabor_import::pytorch::PyTorchFileRecorder;

type Backend = mabor_ndarray::NdArray&lt;f32&gt;;

fn convert_model() {
    let device = Default::default();

    // Load from PyTorch
    let recorder = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default();
    let record = recorder
        .load("./conv2d.pt".into(), &amp;device)
        .expect("Should decode state successfully");

    // Save to Mabor's binary format
    let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::default();
    recorder
        .record(record, "model.mpk".into())
        .expect("Failed to save model record");
}

// In your application code
fn load_model() -&gt; Net&lt;Backend&gt; {
    let device = Default::default();

    // Load from Mabor's binary format
    let record = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::default()
        .load("./model.mpk".into(), &amp;device)
        .expect("Should decode state successfully");

    Net::&lt;Backend&gt;::init(&amp;device).load_record(record)
}
<span class="boring">}</span></code></pre></pre>
<blockquote>
<p><strong>Note</strong>: For examples of pre-converting models, see the <code>examples/import-model-weights</code> directory
in the Mabor repository.</p>
</blockquote>
<h2 id="extract-configuration"><a class="header" href="#extract-configuration">Extract Configuration</a></h2>
<p>In some cases, models may require additional configuration settings, which are often included in a
<code>.pt</code> file during export. The <code>config_from_file</code> function from the <code>mabor-import</code> cargo package
allows for the extraction of these configurations directly from the <code>.pt</code> file.</p>
<pre><pre class="playground"><code class="language-rust">use std::collections::HashMap;

use mabor::config::Config;
use mabor_import::pytorch::config_from_file;

#[derive(Debug, Config)]
struct NetConfig {
    n_head: usize,
    n_layer: usize,
    d_model: usize,
    some_float: f64,
    some_int: i32,
    some_bool: bool,
    some_str: String,
    some_list_int: Vec&lt;i32&gt;,
    some_list_str: Vec&lt;String&gt;,
    some_list_float: Vec&lt;f64&gt;,
    some_dict: HashMap&lt;String, String&gt;,
}

fn main() {
    let path = "weights_with_config.pt";
    let top_level_key = Some("my_config");
    let config: NetConfig = config_from_file(path, top_level_key).unwrap();
    println!("{:#?}", config);

    // After extracting, it's recommended you save it as a json file.
    config.save("my_config.json").unwrap();
}</code></pre></pre>
<h2 id="troubleshooting-and-advanced-features"><a class="header" href="#troubleshooting-and-advanced-features">Troubleshooting and Advanced Features</a></h2>
<h3 id="key-remapping-for-different-model-architectures"><a class="header" href="#key-remapping-for-different-model-architectures">Key Remapping for Different Model Architectures</a></h3>
<p>If your Mabor model structure doesn't match the parameter names in the PyTorch file, you can remap
keys using regular expressions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();
let load_args = LoadArgs::new("tests/key_remap/key_remap.pt".into())
    // Remove "conv" prefix, e.g. "conv.conv1" -&gt; "conv1"
    .with_key_remap("conv\\.(.*)", "$1");

let record = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect("Should decode state successfully");

let model = Net::&lt;Backend&gt;::init(&amp;device).load_record(record);
<span class="boring">}</span></code></pre></pre>
<h3 id="debugging-with-key-inspection"><a class="header" href="#debugging-with-key-inspection">Debugging with Key Inspection</a></h3>
<p>To help with troubleshooting import issues, you can enable debugging to print the original and
remapped keys:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();
let load_args = LoadArgs::new("tests/key_remap/key_remap.pt".into())
    // Remove "conv" prefix, e.g. "conv.conv1" -&gt; "conv1"
    .with_key_remap("conv\\.(.*)", "$1")
    .with_debug_print(); // Print the keys and remapped keys

let record = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect("Should decode state successfully");

let model = Net::&lt;Backend&gt;::init(&amp;device).load_record(record);
<span class="boring">}</span></code></pre></pre>
<p>Here is an example of the output:</p>
<pre><code class="language-text">Debug information of keys and tensor shapes:
---
Original Key: conv.conv1.bias
Remapped Key: conv1.bias
Shape: [2]
Dtype: F32
---
Original Key: conv.conv1.weight
Remapped Key: conv1.weight
Shape: [2, 2, 2, 2]
Dtype: F32
---
Original Key: conv.conv2.weight
Remapped Key: conv2.weight
Shape: [2, 2, 2, 2]
Dtype: F32
---
</code></pre>
<h3 id="automatic-handling-of-non-contiguous-indices"><a class="header" href="#automatic-handling-of-non-contiguous-indices">Automatic Handling of Non-Contiguous Indices</a></h3>
<p>The PyTorchFileRecorder automatically handles non-contiguous indices in model layer names. For
example, if the source model contains indices with gaps:</p>
<pre><code>"model.layers.0.weight"
"model.layers.0.bias"
"model.layers.2.weight"  // Note the gap (no index 1)
"model.layers.2.bias"
"model.layers.4.weight"
"model.layers.4.bias"
</code></pre>
<p>The recorder will automatically reindex these to be contiguous while preserving their order:</p>
<pre><code>"model.layers.0.weight"
"model.layers.0.bias"
"model.layers.1.weight"  // Reindexed from 2
"model.layers.1.bias"
"model.layers.2.weight"  // Reindexed from 4
"model.layers.2.bias"
</code></pre>
<h3 id="partial-model-loading"><a class="header" href="#partial-model-loading">Partial Model Loading</a></h3>
<p>You can selectively load weights into a partial model, which is useful for:</p>
<ul>
<li>Loading only the encoder from an encoder-decoder architecture</li>
<li>Fine-tuning specific layers while initializing others randomly</li>
<li>Creating hybrid models combining parts from different sources</li>
</ul>
<h3 id="specifying-the-top-level-key-for-state_dict"><a class="header" href="#specifying-the-top-level-key-for-state_dict">Specifying the Top-Level Key for state_dict</a></h3>
<p>Sometimes the
<a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict"><code>state_dict</code></a>
is nested under a top-level key along with other metadata. In this case, you can specify the
top-level key in <code>LoadArgs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();
let load_args = LoadArgs::new("tiny.en.pt".into())
    .with_top_level_key("my_state_dict");

let record = PyTorchFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect("Should decode state successfully")
<span class="boring">}</span></code></pre></pre>
<h3 id="support-for-enum-modules"><a class="header" href="#support-for-enum-modules">Support for Enum Modules</a></h3>
<p>The PyTorchFileRecorder supports models containing enum modules with new-type variants. The enum
variant is automatically selected based on the enum variant type, allowing for flexible model
architectures.</p>
<h2 id="current-known-issues"><a class="header" href="#current-known-issues">Current Known Issues</a></h2>
<ol>
<li><a href="https://github.com/tracel-ai/burn/issues/1179">Candle's pickle does not currently unpack boolean tensors</a>.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="safetensors-model"><a class="header" href="#safetensors-model">Safetensors Model</a></h1>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>Mabor supports importing model weights from the Safetensors format, a secure and efficient
alternative to pickle-based formats. Whether you've trained your model in PyTorch or you want to use
a pre-trained model that provides weights in Safetensors format, you can easily import them into
Mabor.</p>
<p>This guide demonstrates the complete workflow for exporting models to Safetensors format and
importing them into Mabor.</p>
<h2 id="exporting-models-to-safetensors-format"><a class="header" href="#exporting-models-to-safetensors-format">Exporting Models to Safetensors Format</a></h2>
<p>To export a PyTorch model to Safetensors format, you'll need the <code>safetensors</code> Python library. This
library provides a simple API for saving model weights in the Safetensors format.</p>
<h3 id="example-exporting-a-pytorch-model-1"><a class="header" href="#example-exporting-a-pytorch-model-1">Example: Exporting a PyTorch Model</a></h3>
<pre><code class="language-python">import torch
import torch.nn as nn
from safetensors.torch import save_file

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(2, 2, (2,2))
        self.conv2 = nn.Conv2d(2, 2, (2,2), bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

if __name__ == "__main__":
    # Set seed for reproducibility
    torch.manual_seed(42)

    # Initialize model and ensure it's on CPU
    model = Net().to(torch.device("cpu"))

    # Extract model weights dictionary
    model_weights = model.state_dict()

    # Save to Safetensors format
    save_file(model_weights, "conv2d.safetensors")
</code></pre>
<h3 id="verifying-the-export-1"><a class="header" href="#verifying-the-export-1">Verifying the Export</a></h3>
<p>You can verify your exported model by viewing the <code>.safetensors</code> file in
<a href="https://github.com/lutzroeder/netron">Netron</a>, a neural network visualization tool. A correctly
exported file will display a flat structure of tensors, similar to a PyTorch <code>.pt</code> weights file.</p>
<h2 id="importing-safetensors-models-into-burn"><a class="header" href="#importing-safetensors-models-into-burn">Importing Safetensors Models into Mabor</a></h2>
<p>Importing a Safetensors model into Mabor involves two main steps:</p>
<ol>
<li>Defining the model architecture in Mabor</li>
<li>Loading the weights from the Safetensors file</li>
</ol>
<h3 id="step-1-define-the-model-in-burn-1"><a class="header" href="#step-1-define-the-model-in-burn-1">Step 1: Define the Model in Mabor</a></h3>
<p>First, you need to create a Mabor model that matches the architecture of the model you exported:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mabor::{
    nn::conv::{Conv2d, Conv2dConfig},
    prelude::*,
};

#[derive(Module, Debug)]
pub struct Net&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
}

impl&lt;B: Backend&gt; Net&lt;B&gt; {
    /// Create a new model.
    pub fn init(device: &amp;B::Device) -&gt; Self {
        let conv1 = Conv2dConfig::new([2, 2], [2, 2])
            .init(device);
        let conv2 = Conv2dConfig::new([2, 2], [2, 2])
            .with_bias(false)
            .init(device);
        Self { conv1, conv2 }
    }

    /// Forward pass of the model.
    pub fn forward(&amp;self, x: Tensor&lt;B, 4&gt;) -&gt; Tensor&lt;B, 4&gt; {
        let x = self.conv1.forward(x);
        self.conv2.forward(x)
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="step-2-load-the-weights-1"><a class="header" href="#step-2-load-the-weights-1">Step 2: Load the Weights</a></h3>
<p>You have two options for loading the weights:</p>
<h4 id="option-a-load-dynamically-at-runtime-1"><a class="header" href="#option-a-load-dynamically-at-runtime-1">Option A: Load Dynamically at Runtime</a></h4>
<p>This approach loads the Safetensors file directly at runtime, requiring the <code>mabor-import</code>
dependency:</p>
<pre><pre class="playground"><code class="language-rust">use crate::model;
use mabor::record::{FullPrecisionSettings, Recorder};
use mabor_import::safetensors::SafetensorsFileRecorder;

type Backend = mabor_ndarray::NdArray&lt;f32&gt;;

fn main() {
    let device = Default::default();

    // Load weights from Safetensors file
    let record = SafetensorsFileRecorder::&lt;FullPrecisionSettings&gt;::default()
        .load("./conv2d.safetensors".into(), &amp;device)
        .expect("Should decode state successfully");

    // Initialize model and load weights
    let model = model::Net::&lt;Backend&gt;::init(&amp;device).load_record(record);
}</code></pre></pre>
<h4 id="option-b-pre-convert-to-burns-binary-format-1"><a class="header" href="#option-b-pre-convert-to-burns-binary-format-1">Option B: Pre-convert to Mabor's Binary Format</a></h4>
<p>This approach converts the Safetensors file to Mabor's optimized binary format during build time,
removing the runtime dependency on <code>mabor-import</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This code would go in build.rs or a separate tool

use crate::model;
use mabor::record::{FullPrecisionSettings, NamedMpkFileRecorder, Recorder};
use mabor_import::safetensors::SafetensorsFileRecorder;

type Backend = mabor_ndarray::NdArray&lt;f32&gt;;

fn convert_model() {
    let device = Default::default();

    // Load from Safetensors
    let recorder = SafetensorsFileRecorder::&lt;FullPrecisionSettings&gt;::default();
    let record = recorder
        .load("./conv2d.safetensors".into(), &amp;device)
        .expect("Should decode state successfully");

    // Save to Mabor's binary format
    let recorder = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::default();
    recorder
        .record(record, "model.mpk".into())
        .expect("Failed to save model record");
}

// In your application code
fn load_model() -&gt; Net&lt;Backend&gt; {
    let device = Default::default();

    // Load from Mabor's binary format
    let record = NamedMpkFileRecorder::&lt;FullPrecisionSettings&gt;::default()
        .load("./model.mpk".into(), &amp;device)
        .expect("Should decode state successfully");

    Net::&lt;Backend&gt;::init(&amp;device).load_record(record)
}
<span class="boring">}</span></code></pre></pre>
<blockquote>
<p><strong>Note</strong>: For examples of pre-converting models, see the <code>examples/import-model-weights</code> directory
in the Mabor repository.</p>
</blockquote>
<h2 id="advanced-configuration-options"><a class="header" href="#advanced-configuration-options">Advanced Configuration Options</a></h2>
<h3 id="framework-specific-adapters"><a class="header" href="#framework-specific-adapters">Framework-Specific Adapters</a></h3>
<p>When importing Safetensors models, you can specify an adapter type to handle framework-specific
tensor transformations. This is crucial when importing models from different ML frameworks, as
tensor layouts and naming conventions can vary:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();

// Create load arguments with framework-specific adapter
let load_args = LoadArgs::new("model.safetensors".into())
    .with_adapter_type(AdapterType::PyTorch); // Default adapter

// Load with the specified adapter
let record = SafetensorsFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect("Should decode state successfully");
<span class="boring">}</span></code></pre></pre>
<h4 id="available-adapter-types"><a class="header" href="#available-adapter-types">Available Adapter Types</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Adapter Type</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>PyTorch</strong> (default)</td><td>Automatically applies PyTorch-specific transformations:<br>- Transposes weights for linear layers<br>- Renames normalization parameters (weight→gamma, bias→beta)</td></tr>
<tr><td><strong>NoAdapter</strong></td><td>Loads tensors directly without any transformations<br>- Useful when importing from frameworks that already match Mabor's tensor layout</td></tr>
</tbody></table>
</div>
<h2 id="troubleshooting-and-advanced-features-1"><a class="header" href="#troubleshooting-and-advanced-features-1">Troubleshooting and Advanced Features</a></h2>
<h3 id="key-remapping-for-different-model-architectures-1"><a class="header" href="#key-remapping-for-different-model-architectures-1">Key Remapping for Different Model Architectures</a></h3>
<p>If your Mabor model structure doesn't match the parameter names in the Safetensors file, you can
remap keys using regular expressions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();

// Create load arguments with key remapping
let load_args = LoadArgs::new("model.safetensors".into())
    // Remove "conv" prefix, e.g. "conv.conv1" -&gt; "conv1"
    .with_key_remap("conv\\.(.*)", "$1");

let record = SafetensorsFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect("Should decode state successfully");

let model = Net::&lt;Backend&gt;::init(&amp;device).load_record(record);
<span class="boring">}</span></code></pre></pre>
<h3 id="debugging-with-key-inspection-1"><a class="header" href="#debugging-with-key-inspection-1">Debugging with Key Inspection</a></h3>
<p>To help with troubleshooting import issues, you can enable debugging to print the original and
remapped keys:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let device = Default::default();

// Enable debug printing of keys
let load_args = LoadArgs::new("model.safetensors".into())
    .with_key_remap("conv\\.(.*)", "$1")
    .with_debug_print();  // Print original and remapped keys

let record = SafetensorsFileRecorder::&lt;FullPrecisionSettings&gt;::default()
    .load(load_args, &amp;device)
    .expect("Should decode state successfully");
<span class="boring">}</span></code></pre></pre>
<h3 id="automatic-handling-of-non-contiguous-indices-1"><a class="header" href="#automatic-handling-of-non-contiguous-indices-1">Automatic Handling of Non-Contiguous Indices</a></h3>
<p>The SafetensorsFileRecorder automatically handles non-contiguous indices in model layer names. For
example, if the source model contains indices with gaps:</p>
<pre><code>"model.layers.0.weight"
"model.layers.0.bias"
"model.layers.2.weight"  // Note the gap (no index 1)
"model.layers.2.bias"
"model.layers.4.weight"
"model.layers.4.bias"
</code></pre>
<p>The recorder will automatically reindex these to be contiguous while preserving their order:</p>
<pre><code>"model.layers.0.weight"
"model.layers.0.bias"
"model.layers.1.weight"  // Reindexed from 2
"model.layers.1.bias"
"model.layers.2.weight"  // Reindexed from 4
"model.layers.2.bias"
</code></pre>
<h3 id="partial-model-loading-1"><a class="header" href="#partial-model-loading-1">Partial Model Loading</a></h3>
<p>You can selectively load weights into a partial model, which is useful for:</p>
<ul>
<li>Loading only the encoder from an encoder-decoder architecture</li>
<li>Fine-tuning specific layers while initializing others randomly</li>
<li>Creating hybrid models combining parts from different sources</li>
</ul>
<h3 id="support-for-enum-modules-1"><a class="header" href="#support-for-enum-modules-1">Support for Enum Modules</a></h3>
<p>The SafetensorsFileRecorder supports models containing enum modules with new-type variants. The enum
variant is automatically selected based on the enum variant type, allowing for flexible model
architectures.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="models-and-pre-trained-weights"><a class="header" href="#models-and-pre-trained-weights">Models and Pre-Trained Weights</a></h1>
<p>The <a href="https://github.com/tracel-ai/models"><code>models</code></a> repository contains definitions of different
deep learning models with examples for different domains like computer vision and natural language
processing.</p>
<p>This includes image classification models such as
<a href="https://github.com/tracel-ai/models/tree/main/mobilenetv2-burn"><code>MobileNetV2</code></a>,
<a href="https://github.com/tracel-ai/models/tree/main/squeezenet-burn"><code>SqueezeNet</code></a> and
<a href="https://github.com/tracel-ai/models/tree/main/resnet-burn"><code>ResNet</code></a>, object detection models such
as <a href="https://github.com/tracel-ai/models/tree/main/yolox-burn"><code>YOLOX</code></a> and language models like
<a href="https://github.com/tracel-ai/models/tree/main/bert-burn"><code>BERT</code> and <code>RoBERTa</code></a>.</p>
<p>Be sure to check out the up-to-date
<a href="https://github.com/tracel-ai/models?tab=readme-ov-file#collection-of-official-models">collection of models</a>
to get you started. Pre-trained weights are available for every supported architecture in this
collection. You will also find a spotlight of
<a href="https://github.com/tracel-ai/models?tab=readme-ov-file#community-contributions">community contributed models</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quantization-beta"><a class="header" href="#quantization-beta">Quantization (Beta)</a></h1>
<p>Quantization techniques perform computations and store tensors in lower precision data types like
8-bit integer instead of floating point precision. There are multiple approaches to quantize a deep
learning model categorized as:</p>
<ul>
<li>Post-training quantization (PTQ)</li>
<li>Quantization aware training (QAT)</li>
</ul>
<p>In post-training quantization, the model is trained in floating point precision and later converted
to the lower precision data type.</p>
<p>There are two types of post-training quantization:</p>
<ol>
<li>Static quantization: quantizes the weights and activations of the model. Quantizing the
activations statically requires data to be calibrated (i.e., recording the activation values to
compute the optimal quantization parameters with representative data).</li>
<li>Dynamic quantization: quantized the weights ahead of time (like static quantization) but the
activations are dynamically at runtime.</li>
</ol>
<p>Sometimes post-training quantization is not able to achieve acceptable task accuracy. This is where
quantization aware training comes into play, as it models the effects of quantization during
training. Quantization errors are thus modeled in the forward and backward passes using fake
quantization modules, which helps the model learn representations that are more robust to the
reduction in precision.</p>
<div class="warning">
<p>Quantization support in Mabor is currently in active development.</p>
<p>It supports the following modes on some backends:</p>
<ul>
<li>Static per-tensor quantization to signed 8-bit integer (<code>i8</code>)</li>
</ul>
<p>No integer operations are currently supported, which means tensors are dequantized to perform the
operations in floating point precision.</p>
</div>
<h2 id="module-quantization"><a class="header" href="#module-quantization">Module Quantization</a></h2>
<p>Quantizing the weights of your model after training is quite simple. We have access to the weight
tensors and can collect their statistics, such as the min and max value when using
<code>MinMaxCalibration</code>, to compute the quantization parameters.</p>
<pre><code class="language-rust   ignore"><span class="boring">use mabor::module::Quantizer;
</span><span class="boring">use mabor::tensor::quantization::{Calibration, QuantizationScheme, QuantizationType};
</span><span class="boring">
</span>// Quantization config
let mut quantizer = Quantizer {
    calibration: Calibration::MinMax,
    scheme: QuantizationScheme::PerTensor(QuantizationMode::Symmetric, QuantizationType::QInt8),
};

// Quantize the weights
let model = model.quantize_weights(&amp;mut quantizer);</code></pre>
<blockquote>
<p>Given that all operations are currently performed in floating point precision, it might be wise to
dequantize the module parameters before inference. This allows us to save disk space by storing
the model in reduced precision while preserving the inference speed.</p>
<p>This can easily be implemented with a <code>ModuleMapper</code>.</p>
<pre><code class="language-rust  ignore"><span class="boring">use mabor::module::{ModuleMapper, ParamId};
</span><span class="boring">use mabor::tensor::{backend::Backend, Tensor};
</span><span class="boring">
</span>/// Module mapper used to dequantize the model params being loaded.
pub struct Dequantize {}

impl&lt;B: Backend&gt; ModuleMapper&lt;B&gt; for Dequantize {
    fn map_float&lt;const D: usize&gt;(
        &amp;mut self,
        _id: ParamId,
        tensor: Tensor&lt;B, D&gt;,
    ) -&gt; Tensor&lt;B, D&gt; {
        tensor.dequantize()
    }
}

// Load saved quantized model in floating point precision
model = model
    .load_file(file_path, recorder, &amp;device)
    .expect("Should be able to load the quantized model weights")
    .map(&amp;mut Dequantize {});</code></pre>
</blockquote>
<h3 id="calibration"><a class="header" href="#calibration">Calibration</a></h3>
<p>Calibration is the step during quantization where the range of all floating-point tensors is
computed. This is pretty straightforward for weights since the actual range is known at
<em>quantization-time</em> (weights are static), but activations require more attention.</p>
<p>To compute the quantization parameters, Mabor supports the following <code>Calibration</code> methods.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">Method</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>MinMax</code></td><td style="text-align: left">Computes the quantization range mapping based on the running min and max values.</td></tr>
</tbody></table>
</div>
<h3 id="quantization-scheme"><a class="header" href="#quantization-scheme">Quantization Scheme</a></h3>
<p>A quantization scheme defines the quantized type, quantization granularity and range mapping
technique.</p>
<p>Mabor currently supports the following <code>QuantizationType</code> variants.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">Type</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>QInt8</code></td><td style="text-align: left">8-bit signed integer quantization.</td></tr>
</tbody></table>
</div>
<p>Quantization parameters are defined based on the range of values to represent and can typically be
calculated for the layer's entire weight tensor with per-tensor quantization or separately for each
channel with per-channel quantization (commonly used with CNNs).</p>
<p>Mabor currently supports the following <code>QuantizationScheme</code> variants.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">Variant</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>PerTensor(mode, type)</code></td><td style="text-align: left">Applies a single set of quantization parameters to the entire tensor. The <code>mode</code> defines how values are transformed, and <code>type</code> represents the target quantization type.</td></tr>
</tbody></table>
</div>
<h4 id="quantization-mode"><a class="header" href="#quantization-mode">Quantization Mode</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Mode</th><th>Description</th></tr></thead><tbody>
<tr><td><code>Symmetric</code></td><td>Maps values using a scale factor for a range centered around zero.</td></tr>
</tbody></table>
</div>
<hr>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced"><a class="header" href="#advanced">Advanced</a></h1>
<p>In this section, we will go into advanced topics that extend beyond basic usage. Given Mabor's
exceptional flexibility, a lot of advanced use cases become possible.</p>
<p>Before going through this section, we strongly recommend exploring the
<a href="basic-workflow/index.htm">basic workflow</a> section and the
<a href="building-blocks/index.htm">building blocks</a> section. Establishing a solid understanding of how
the framework operates is crucial to comprehending the advanced concepts presented here. While you
have the freedom to explore the advanced sections in any order you prefer, it's important to note
that this section is not intended to be linear, contrary to preceding sections. Instead, it serves
as a repository of use cases that you can refer to for guidance as needed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backend-extension"><a class="header" href="#backend-extension">Backend Extension</a></h1>
<p>Mabor aims to be the most flexible deep learning framework. While it's crucial to maintain
compatibility with a wide variety of backends, Mabor provides the ability to extend the functionality
of a backend implementation to suit your modeling requirements. This versatility is advantageous in
numerous ways, such as supporting custom operations like flash attention or manually fusing
operations for enhanced performance.</p>
<p>In this section, we will go into the process of extending a backend, providing multiple examples.
But before we proceed, let's establish the fundamental principles that will empower you to craft
your own backend extensions.</p>
<p>As you can observe, most types in Mabor are generic over the Backend trait. This might give the
impression that Mabor operates at a high level over the backend layer. However, making the trait
explicit instead of being chosen via a compilation flag was a thoughtful design decision. This
explicitness does not imply that all backends must be identical; rather, it offers a great deal of
flexibility when composing backends. The autodifferentiation backend trait (see
<a href="building-blocks/autodiff.html">autodiff section</a>) is an example of how the backend trait has
been extended to enable gradient computation with backpropagation. Furthermore, this design allows
you to create your own backend extension. To achieve this, you need to design your own backend trait
specifying which functions should be supported.</p>
<pre><code class="language-rust  ignore">pub trait Backend: mabor::tensor::backend::Backend {
    fn my_new_function(tensor: B::TensorPrimitive&lt;2&gt;) -&gt; B::TensorPrimitive&lt;2&gt; {
        // You can define a basic implementation reusing the Mabor Backend API.
        // This can be useful since all backends will now automatically support
        // your model. But performance can be improved for this new
        // operation by implementing this block in specific backends.
    }
}</code></pre>
<p>You can then implement your new custom backend trait for any backend that you want to support:</p>
<pre><code class="language-rust  ignore">impl&lt;E: TchElement&gt; Backend for mabor_tch::LibTorch&lt;E&gt; {
   fn my_new_function(tensor: TchTensor&lt;E, 2&gt;) -&gt; TchTensor&lt;E, 2&gt; {
      // My Tch implementation
   }
}

impl&lt;E: NdArrayElement&gt; Backend for mabor_ndarray::NdArray&lt;E&gt; {
    // No specific implementation, but the backend can still be used.
}</code></pre>
<p>You can support the backward pass using the same pattern.</p>
<pre><code class="language-rust  ignore">impl&lt;B: Backend&gt; Backend for mabor_autodiff::Autodiff&lt;B&gt; {
    // No specific implementation; autodiff will work with the default
    // implementation. Useful if you still want to train your model, but
    // observe performance gains mostly during inference.
}

impl&lt;B: Backend&gt; Backend for mabor_autodiff::Autodiff&lt;B&gt; {
   fn my_new_function(tensor: AutodiffTensor&lt;E, 2&gt;) -&gt; AutodiffTensor&lt;E, 2&gt; {
      // My own backward implementation, generic over my custom Backend trait.
      //
      // You can add a new method `my_new_function_backward` to your custom backend
      // trait if you want to invoke a custom kernel during the backward pass.
   }
}

impl&lt;E: TchElement&gt; Backend for mabor_autodiff::Autodiff&lt;mabor_tch::LibTorch&lt;E&gt;&gt; {
   fn my_new_function(tensor: AutodiffTensor&lt;E, 2&gt;) -&gt; AutodiffTensor&lt;E, 2&gt; {
      // My own backward implementation, generic over a backend implementation.
      //
      // This is another way to call a custom kernel for the backward pass that
      // doesn't require the addition of a new `backward` function in the custom backend.
      // This is useful if you don't want all backends to support training, reducing
      // the need for extra code when you know your model will only be trained on one
      // specific backend.
   }
}</code></pre>
<p>The specifics of each implementation will be covered by the examples provided in this section. The
<code>cubecl</code> compiler frontend is the recommended method of implementing custom kernels, since it
supports multiple backends, including <code>wgpu</code> and <code>CUDA</code>, and is the way first-party <code>mabor</code> kernels
are written.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-cubecl-kernel"><a class="header" href="#custom-cubecl-kernel">Custom <code>cubecl</code> Kernel</a></h1>
<p>In this section, you will learn how to create your own custom operation by writing your own kernel
with the cubecl compiler frontend. We will take the example of a common workflow in the deep
learning field, where we create a kernel to fuse multiple operations together. Note that <code>mabor</code> does
this automatically, but a manual implementation might be more efficient in some cases. We will fuse
a matmul kernel followed by an addition and the ReLU activation function, which is commonly found in
various models. All the code can be found under the
<a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-cubecl-kernel">examples directory</a>.</p>
<h2 id="custom-backend-trait"><a class="header" href="#custom-backend-trait">Custom Backend Trait</a></h2>
<p>First, we need to determine the type signature of our newly created operation by defining our custom
backend traits. As we will use the associated type <code>TensorPrimitive</code> of the <code>Backend</code> trait, which
encapsulates the underlying tensor implementation of the backend, we will use a type alias to avoid
the ugly disambiguation with associated types.</p>
<pre><code class="language-rust  ignore">/// We create our own Backend trait that extends the Mabor backend trait.
pub trait Backend: mabor::tensor::backend::Backend {
    fn fused_matmul_add_relu(
        lhs: FloatTensor&lt;Self&gt;,
        rhs: FloatTensor&lt;Self&gt;,
        bias: FloatTensor&lt;Self&gt;,
    ) -&gt; FloatTensor&lt;Self&gt;;
}

/// We create our own AutodiffBackend trait that extends the Mabor autodiff backend trait.
pub trait AutodiffBackend: Backend + mabor::tensor::backend::AutodiffBackend {}</code></pre>
<p>In our project, we can use these traits instead of the
<code>mabor::tensor::backend::{Backend, AutodiffBackend}</code> traits provided by Mabor. Mabor's user APIs
typically make use of the <code>Tensor</code> struct rather than dealing directly with primitive tensor types.
Therefore, we can encapsulate our newly defined backend traits with functions that expose new
operations while maintaining a consistent API.</p>
<pre><code class="language-rust  ignore">/// We define our custom implementation using the added function on our custom backend.
pub fn matmul_add_relu_custom&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let output = B::fused_matmul_add_relu(
        lhs.into_primitive().tensor(),
        rhs.into_primitive().tensor(),
        bias.into_primitive().tensor(),
    );

    Tensor::from_primitive(TensorPrimitive::Float(output))
}

/// We define a reference implementation using basic tensor operations.
pub fn matmul_add_relu_reference&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let x = lhs.matmul(rhs) + bias;

    activation::relu(x)
}
</code></pre>
<p>Note that we also provide a reference implementation for testing purposes, which allows us to easily
validate our new implementation. While not mandatory, having a reference implementation can be
valuable, especially in projects where creating a reference implementation solely using basic tensor
operations is feasible.</p>
<h2 id="forward-kernel"><a class="header" href="#forward-kernel">Forward Kernel</a></h2>
<p>Now, let's proceed to write the fused kernel using the <code>cubecl</code> compiler frontend. To keep things
simple, we'll create a straightforward matmul kernel without employing any intricate techniques. We
won't delve into the details of the <code>cube</code> macro, but if you're interested to learn more, please see
<a href="https://github.com/tracel-ai/cubecl/tree/f5b63076a01a5c03ea9ed20799d3eeaf776b45da/cubecl-book"><code>cubecl</code> Book</a>.
The actual matmul, add and relu computations are found at the end, after an extensive prelude
that serves to correctly map each compute unit to the data it is responsible for, with support for
batches.</p>
<pre><code class="language-rust  ignore">use cubecl::{cube, prelude::*};

#[cube(launch)]
pub fn fused_matmul_add_relu_kernel&lt;F: Float&gt;(
    lhs: &amp;Tensor&lt;F&gt;,
    rhs: &amp;Tensor&lt;F&gt;,
    bias: &amp;Tensor&lt;F&gt;,
    output: &amp;mut Tensor&lt;F&gt;,
) {
    let row = ABSOLUTE_POS_X;
    let col = ABSOLUTE_POS_Y;
    let batch = ABSOLUTE_POS_Z;

    let n_rows = output.shape(output.rank() - 2);
    let n_cols = output.shape(output.rank() - 1);
    let dim_k = rhs.shape(rhs.rank() - 1);

    if row &gt;= n_rows || col &gt;= n_cols {
        return;
    }

    let offset_output = batch * n_rows * n_cols;
    let mut offset_lhs = 0;
    let mut offset_rhs = 0;

    let batch_dims = output.rank() - 2;
    for dim in 0..batch_dims {
        offset_lhs += offset_output / output.stride(dim) % lhs.shape(dim) * lhs.stride(dim);
        offset_rhs += offset_output / output.stride(dim) % rhs.shape(dim) * rhs.stride(dim);
    }

    let mut sum = F::new(0.0);
    for k in 0..dim_k {
        let lhs_index = row * dim_k + k;
        let rhs_index = k * n_cols + col;

        sum += lhs[offset_lhs + lhs_index] * rhs[offset_rhs + rhs_index];
    }

    let out_index = row * n_cols + col;
    let index = offset_output + out_index;

    output[index] = F::max(sum + bias[index], F::new(0.0));
}</code></pre>
<p>Now, let's move on to the next step, which involves implementing the remaining code to launch the
kernel. We'll go into implementing our custom backend trait for the generic JIT backend. This
automatically implements the trait for <code>mabor-cuda</code>, <code>mabor-wgpu</code> as well as fusion.</p>
<pre><code class="language-rust  ignore">/// Implement our custom backend trait for the generic `CubeBackend`.
impl&lt;R: CubeRuntime, F: FloatElement, I: IntElement&gt; Backend for CubeBackend&lt;R, F, I&gt; {
    fn fused_matmul_add_relu(
        lhs: FloatTensor&lt;Self&gt;,
        rhs: FloatTensor&lt;Self&gt;,
        bias: FloatTensor&lt;Self&gt;,
    ) -&gt; FloatTensor&lt;Self&gt; {
        // Define cube dim, hardcoded for simplicity.
        let cube_dim = CubeDim { x: 16, y: 16, z: 1 };

        lhs.assert_is_on_same_device(&amp;rhs);
        lhs.assert_is_on_same_device(&amp;bias);

        // For simplicity, make sure each tensor is continuous.
        let lhs = into_contiguous(lhs);
        let rhs = into_contiguous(rhs);
        let bias = into_contiguous(bias);

        // Get the matmul relevant shapes.
        let ndims = lhs.shape.num_dims();
        let num_rows = lhs.shape.dims[ndims - 2];
        let num_cols = rhs.shape.dims[ndims - 1];

        // Compute shape of output, while tracking number of batches.
        let mut num_batches = 1;
        let mut shape_out = vec![0; ndims];
        for i in shape_out.clone().into_iter().take(ndims - 2) {
            shape_out[i] = usize::max(lhs.shape.dims[i], rhs.shape.dims[i]);
            num_batches *= shape_out[i];
        }
        shape_out[ndims - 2] = num_rows;
        shape_out[ndims - 1] = num_cols;
        let shape_out = Shape::from(shape_out);

        // Create a buffer for the output tensor.
        let buffer = lhs
            .client
            .empty(shape_out.num_elements() * core::mem::size_of::&lt;F&gt;());

        // Create the output tensor primitive.
        // Create the output tensor primitive.
        let output = CubeTensor::new_contiguous(
            lhs.client.clone(),
            lhs.device.clone(),
            shape_out,
            buffer,
            F::dtype(),
        );

        // Declare the wgsl workgroup with the number of cubes in x, y and z.
        let cubes_needed_in_x = f32::ceil(num_rows as f32 / cube_dim.x as f32) as u32;
        let cubes_needed_in_y = f32::ceil(num_cols as f32 / cube_dim.y as f32) as u32;
        let cube_count =
            CubeCount::Static(cubes_needed_in_x, cubes_needed_in_y, num_batches as u32);

        // Execute lazily the kernel with the launch information and the given buffers. For
        // simplicity, no vectorization is performed
        fused_matmul_add_relu_kernel::launch::&lt;F, R&gt;(
            &amp;lhs.client,
            cube_count,
            cube_dim,
            lhs.as_tensor_arg::&lt;F&gt;(1),
            rhs.as_tensor_arg::&lt;F&gt;(1),
            bias.as_tensor_arg::&lt;F&gt;(1),
            output.as_tensor_arg::&lt;F&gt;(1),
        );

        // Return the output tensor.
        output
    }
}</code></pre>
<p>In the preceding code block, we demonstrated how to launch the kernel that modifies the correct
buffer. It's important to note that Rust's mutability safety doesn't apply here; the context has the
capability to execute any mutable operation on any buffer. While this isn't a problem in the
previous scenario where we only modify the newly created output buffer, it is wise to keep this in
mind.</p>
<h2 id="backward"><a class="header" href="#backward">Backward</a></h2>
<p>Now that the custom backend trait is implemented for the JIT backend, you can use it to invoke the
<code>matmul_add_relu_custom</code> function. However, calculating gradients is not yet possible at this stage.
If your use case does not extend beyond inference, there is no need to implement any of the
following code.</p>
<p>For the backward pass, we will leverage the backend implementation from <code>mabor-autodiff</code>, which is
actually generic over the backend. Instead of crafting our own <code>cubecl</code> kernel for the backward
pass, we will use our fused kernel only for the forward pass, and compute the gradient using basic
operations.</p>
<pre><code class="language-rust  ignore">// Implement our custom backend trait for any backend that also implements our custom backend trait.
impl&lt;B: Backend, C: CheckpointStrategy&gt; Backend for Autodiff&lt;B, C&gt; {
    fn fused_matmul_add_relu(
        lhs: FloatTensor&lt;Self&gt;,
        rhs: FloatTensor&lt;Self&gt;,
        bias: FloatTensor&lt;Self&gt;,
    ) -&gt; FloatTensor&lt;Self&gt; {
        // Create our zero-sized type that will implement the Backward trait.
        #[derive(Debug)]
        struct FusedMatmulAddReluBackward;

        // Implement the backward trait for the given backend B, the node gradient
        // with three other gradients to calculate (lhs, rhs, and bias).
        impl&lt;B: Backend&gt; Backward&lt;B, 3&gt; for FusedMatmulAddReluBackward {
            // Our state that we must build during the forward pass to compute the backward pass.
            //
            // Note that we could improve the performance further by only keeping the state of
            // tensors that are tracked, improving memory management, but for simplicity, we avoid
            // that part.
            type State = (NodeID, NodeID, FloatTensor&lt;B&gt;, Shape);

            fn backward(
                self,
                ops: Ops&lt;Self::State, 3&gt;,
                grads: &amp;mut Gradients,
                checkpointer: &amp;mut Checkpointer,
            ) {
                // Get the nodes of each variable.
                let [node_lhs, node_rhs, node_bias] = ops.parents;
                // Fetch the gradient for the current node.
                let grad = grads.consume::&lt;B&gt;(&amp;ops.node);

                // Set our state.
                let (lhs_state, rhs_state, output, shape_bias) = ops.state;
                let lhs: FloatTensor&lt;B&gt; = checkpointer.retrieve_node_output(lhs_state);
                let rhs: FloatTensor&lt;B&gt; = checkpointer.retrieve_node_output(rhs_state);

                // Fetch shapes of our tensor to support broadcasting.
                let shape_lhs = lhs.shape();
                let shape_rhs = rhs.shape();

                // Compute the gradient of the output using the already existing `relu_backward`
                // function in the basic Mabor backend trait.
                let grad_output = B::relu_backward(output, grad);

                // Compute the lhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_lhs = broadcast_shape::&lt;B&gt;(
                    B::float_matmul(grad_output.clone(), B::float_transpose(rhs)),
                    &amp;shape_lhs,
                );
                // Compute the rhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_rhs = broadcast_shape::&lt;B&gt;(
                    B::float_matmul(B::float_transpose(lhs), grad_output.clone()),
                    &amp;shape_rhs,
                );
                // The add derivative is only 1, so we just need to support broadcasting to
                // compute the bias gradient.
                let grad_bias = broadcast_shape::&lt;B&gt;(grad_output, &amp;shape_bias);

                // Register the gradient for each variable based on whether they are marked as
                // `tracked`.
                if let Some(node) = node_bias {
                    grads.register::&lt;B&gt;(node.id, grad_bias);
                }
                if let Some(node) = node_lhs {
                    grads.register::&lt;B&gt;(node.id, grad_lhs);
                }
                if let Some(node) = node_rhs {
                    grads.register::&lt;B&gt;(node.id, grad_rhs);
                }
            }
        }

        // Prepare a stateful operation with each variable node and corresponding graph.
        //
        // Each node can be fetched with `ops.parents` in the same order as defined here.
        match FusedMatmulAddReluBackward
            .prepare::&lt;C&gt;([lhs.node.clone(), rhs.node.clone(), bias.node.clone()])
            // Marks the operation as compute bound, meaning it will save its
            // state instead of recomputing itself during checkpointing
            .compute_bound()
            .stateful()
        {
            OpsKind::Tracked(mut prep) =&gt; {
                // When at least one node is tracked, we should register our backward step.

                // The state consists of what will be needed for this operation's backward pass.
                // Since we need the parents' outputs, we must checkpoint their ids to retrieve
                // their node output at the beginning of the backward pass. We can also save
                // utilitary data such as the bias shape. If we also need this operation's output,
                // we can either save it in the state or recompute it.
                // during the backward pass. Here we choose to save it in the state because it's a
                // compute bound operation.
                let lhs_state = prep.checkpoint(&amp;lhs);
                let rhs_state = prep.checkpoint(&amp;rhs);
                let bias_shape = bias.primitive.shape();

                let output = B::fused_matmul_add_relu(
                    lhs.primitive.clone(),
                    rhs.primitive.clone(),
                    bias.primitive,
                );

                let state = (lhs_state, rhs_state, output.clone(), bias_shape);

                prep.finish(state, output)
            }
            OpsKind::UnTracked(prep) =&gt; {
                // When no node is tracked, we can just compute the original operation without
                // keeping any state.
                let output = B::fused_matmul_add_relu(lhs.primitive, rhs.primitive, bias.primitive);
                prep.finish(output)
            }
        }
    }
}</code></pre>
<p>The previous code is self-documented to make it clearer, but here is what it does in summary:</p>
<p>We define <code>fused_matmul_add_relu</code> within <code>Autodiff&lt;B&gt;</code>, allowing any autodiff-decorated backend to
benefit from our implementation. In an autodiff-decorated backend, the forward pass must still be
implemented. This is achieved using a comprehensive match statement block where computation is
delegated to the inner backend, while keeping track of a state. The state comprises any information
relevant to the backward pass, such as input and output tensors, along with the bias shape. When an
operation isn't tracked (meaning there won't be a backward pass for this specific operation in the
graph), storing a state becomes unnecessary, and we simply perform the forward computation.</p>
<p>The backward pass uses the gradient obtained from the preceding node in the computation graph. It
calculates the derivatives for <code>relu</code> (<code>relu_backward</code>), add (no operation is required here, as the
derivative is one), and <code>matmul</code> (another <code>matmul</code> with transposed inputs). This results in
gradients for both input tensors and the bias, which are registered for consumption by subsequent
operation nodes.</p>
<p>The only remaining part is to implement our autodiff-decorated backend trait for our JIT Backend.</p>
<pre><code class="language-rust  ignore">impl&lt;R: CubeRuntime, F: FloatElement, I: IntElement&gt; AutodiffBackend
    for Autodiff&lt;CubeBackend&lt;R, F, I&gt;&gt;
{
}</code></pre>
<h2 id="conclusion-2"><a class="header" href="#conclusion-2">Conclusion</a></h2>
<p>In this guide, we've implemented a fused kernel using the <code>cubecl</code> compiler frontend, enabling
execution on any GPU and any <code>cubecl</code> backend. By delving into the inner workings of both the JIT
backend and the autodiff backend, we've gained a deeper understanding of these systems.</p>
<p>While extending a backend may be harder than working with straightforward tensors, the benefits can
be worth it. This approach enables the crafting of custom models with greater control over
execution, which can potentially greatly enhance the performance of your models.</p>
<p>As we conclude this guide, we hope that you have gained insights into Mabor's world of backend
extensions, and that it will help you to unleash the full potential of your projects.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-wgpu-kernel"><a class="header" href="#custom-wgpu-kernel">Custom WGPU Kernel</a></h1>
<p>In this section, you will learn how to create your own custom operation by writing your own kernel
with the WGPU backend. We will take the example of a common workflow in the deep learning field,
where we create a kernel to fuse multiple operations together. Note that <code>mabor</code> does this
automatically, but a manual implementation might be more efficient in some cases. We will fuse a
matmul kernel followed by an addition and the ReLU activation function, which is commonly found in
various models. All the code can be found under the
<a href="https://github.com/tracel-ai/burn/tree/main/examples/custom-wgpu-kernel">examples directory</a>.</p>
<h2 id="custom-backend-trait-1"><a class="header" href="#custom-backend-trait-1">Custom Backend Trait</a></h2>
<p>First, we need to determine the type signature of our newly created operation by defining our custom
backend traits. As we will use the associated type <code>TensorPrimitive</code> of the <code>Backend</code> trait, which
encapsulates the underlying tensor implementation of the backend, we will use a type alias to avoid
the ugly disambiguation with associated types.</p>
<pre><code class="language-rust  ignore">/// We create our own Backend trait that extends the Mabor backend trait.
pub trait Backend: mabor::tensor::backend::Backend {
    fn fused_matmul_add_relu(
        lhs: FloatTensor&lt;Self&gt;,
        rhs: FloatTensor&lt;Self&gt;,
        bias: FloatTensor&lt;Self&gt;,
    ) -&gt; FloatTensor&lt;Self&gt;;
}

/// We create our own AutodiffBackend trait that extends the Mabor autodiff backend trait.
pub trait AutodiffBackend: Backend + mabor::tensor::backend::AutodiffBackend {}</code></pre>
<p>In our project, we can use these traits instead of the
<code>mabor::tensor::backend::{Backend, AutodiffBackend}</code> traits provided by Mabor. Mabor's user APIs
typically make use of the <code>Tensor</code> struct rather than dealing directly with primitive tensor types.
Therefore, we can encapsulate our newly defined backend traits with functions that expose new
operations while maintaining a consistent API.</p>
<pre><code class="language-rust  ignore">/// We define our custom implementation using the added function on our custom backend.
pub fn matmul_add_relu_custom&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let output = B::fused_matmul_add_relu(
        lhs.into_primitive().tensor(),
        rhs.into_primitive().tensor(),
        bias.into_primitive().tensor(),
    );

    Tensor::from_primitive(TensorPrimitive::Float(output))
}

/// We define a reference implementation using basic tensor operations.
pub fn matmul_add_relu_reference&lt;B: Backend&gt;(
    lhs: Tensor&lt;B, 3&gt;,
    rhs: Tensor&lt;B, 3&gt;,
    bias: Tensor&lt;B, 3&gt;,
) -&gt; Tensor&lt;B, 3&gt; {
    let x = lhs.matmul(rhs) + bias;

    activation::relu(x)
}
</code></pre>
<p>Note that we also provide a reference implementation for testing purposes, which allows us to easily
validate our new implementation. While not mandatory, having a reference implementation can be
valuable, especially in projects where creating a reference implementation solely using basic tensor
operations is feasible.</p>
<h2 id="forward-kernel-1"><a class="header" href="#forward-kernel-1">Forward Kernel</a></h2>
<p>Now, let's proceed to write the fused kernel using the WGSL shading language. To keep things simple,
we'll create a straightforward matmul kernel without employing any intricate techniques. Although we
won't delve into the details of the WGSL syntax, as it falls beyond the scope of this guide, we
still provide the implementation below for readers who are curious. The actual matmul, add and relu
computations are found at the end, after an extensive overhead whose use is to correctly map each
compute unit to the data it is responsible of, with support for batches.</p>
<pre><code class="language-wgsl  ignore">@group(0)
@binding(0)
var&lt;storage, read_write&gt; lhs: array&lt;{{ elem }}&gt;;

@group(0)
@binding(1)
var&lt;storage, read_write&gt; rhs: array&lt;{{ elem }}&gt;;

@group(0)
@binding(2)
var&lt;storage, read_write&gt; bias: array&lt;{{ elem }}&gt;;

@group(0)
@binding(3)
var&lt;storage, read_write&gt; output: array&lt;{{ elem }}&gt;;

@group(0)
@binding(4)
var&lt;storage, read_write&gt; info: array&lt;u32&gt;;

const BLOCK_SIZE = {{ workgroup_size_x }}u;

@compute
@workgroup_size({{ workgroup_size_x }}, {{ workgroup_size_y }}, 1)
fn main(
    @builtin(global_invocation_id) global_id: vec3&lt;u32&gt;,
    @builtin(local_invocation_index) local_idx: u32,
    @builtin(workgroup_id) workgroup_id: vec3&lt;u32&gt;,
) {
    // Indices
    let row = workgroup_id.x * BLOCK_SIZE + (local_idx / BLOCK_SIZE);
    let col = workgroup_id.y * BLOCK_SIZE + (local_idx % BLOCK_SIZE);
    let batch = global_id.z;

    // Basic information
    let dim = info[0];
    let n_rows = info[6u * dim - 1u];
    let n_cols = info[6u * dim];
    let K = info[5u * dim - 1u];

    // Returns if outside the output dimension
    if row &gt;= n_rows || col &gt;= n_cols {
        return;
    }

    // Calculate the corresponding offsets with support for broadcasting.
    let offset_output = batch * n_rows * n_cols;
    var offset_lhs: u32 = 0u;
    var offset_rhs: u32 = 0u;

    let batch_dims = dim - 2u;
    for (var b: u32 = 1u; b &lt;= batch_dims; b++) {
        let stride_lhs = info[b];
        let stride_rhs = info[b + dim];
        let stride_output = info[b + 2u * dim];
        let shape_lhs = info[b + 3u * dim];
        let shape_rhs = info[b + 4u * dim];

        offset_lhs += offset_output / stride_output % shape_lhs * stride_lhs;
        offset_rhs += offset_output / stride_output % shape_rhs * stride_rhs;
    }

    // Basic matmul implementation
    var sum = 0.0;
    for (var k: u32 = 0u; k &lt; K; k++) {
        let lhs_index = row * K + k;
        let rhs_index = k * n_cols + col;

        sum += lhs[offset_lhs + lhs_index] * rhs[offset_rhs + rhs_index];
    }

    let output_index = row * n_cols + col;
    let index = offset_output + output_index;

    // Add and ReLU
    output[index] = max(sum + bias[index], 0.0);
}
</code></pre>
<p>Now, let's move on to the next step, which involves implementing the remaining code to launch the
kernel. The initial part entails loading the template and populating it with the appropriate
variables. The <code>register(name, value)</code> method simply replaces occurrences of <code>{{ name }}</code> in the
above WGSL code with some other string before it is compilated. In order to use templating
utilities, you will have to activate the <code>template</code> feature of Mabor in your <code>cargo.toml</code>.</p>
<pre><code class="language-rust  ignore">// Source the kernel written in WGSL.
kernel_wgsl!(FusedMatmulAddReluRaw, "./kernel.wgsl");

// Define our kernel type with cube information.
#[derive(new, Debug)]
struct FusedMatmulAddRelu&lt;E: FloatElement&gt; {
    cube_dim: CubeDim,
    _elem: PhantomData&lt;E&gt;,
}

// Implement the dynamic kernel trait for our kernel type.
impl&lt;E: FloatElement&gt; KernelSource for FusedMatmulAddRelu&lt;E&gt; {
    fn source(&amp;self) -&gt; SourceTemplate {
        // Extend our raw kernel with cube size information using the
        // `SourceTemplate` trait.
        FusedMatmulAddReluRaw::new()
            .source()
            .register("workgroup_size_x", self.cube_dim.x.to_string())
            .register("workgroup_size_y", self.cube_dim.y.to_string())
            .register("elem", E::type_name())
            .register("int", "i32")
    }

    fn id(&amp;self) -&gt; cubecl::KernelId {
        cubecl::KernelId::new::&lt;Self&gt;().info(self.cube_dim)
    }
}</code></pre>
<p>Subsequently, we'll go into implementing our custom backend trait for the WGPU backend. Note that we
won't go into supporting the <code>fusion</code> feature flag in this tutorial, so we implement the trait for
the raw <code>WgpuBackend</code> type.</p>
<pre><code class="language-rust  ignore">/// Implement our custom backend trait for the existing backend `WgpuBackend`.
impl&lt;F: FloatElement, I: IntElement&gt; Backend for CubeBackend&lt;WgpuRuntime, F, I&gt; {
    fn fused_matmul_add_relu(
        lhs: FloatTensor&lt;Self&gt;,
        rhs: FloatTensor&lt;Self&gt;,
        bias: FloatTensor&lt;Self&gt;,
    ) -&gt; FloatTensor&lt;Self&gt; {
        // Define cube dim, hardcoded for simplicity.
        let cube_dim = CubeDim { x: 16, y: 16, z: 1 };

        lhs.assert_is_on_same_device(&amp;rhs);
        lhs.assert_is_on_same_device(&amp;bias);

        // For simplicity, make sure each tensor is continuous.
        let lhs = into_contiguous(lhs);
        let rhs = into_contiguous(rhs);
        let bias = into_contiguous(bias);

        // Get the matmul relevant shapes.
        let ndims = lhs.shape.num_dims();
        let num_rows = lhs.shape.dims[ndims - 2];
        let num_cols = rhs.shape.dims[ndims - 1];

        // Compute shape of output, while tracking number of batches.
        let mut num_batches = 1;
        let mut shape_out = vec![0; ndims];
        for i in shape_out.clone().into_iter().take(ndims - 2) {
            shape_out[i] = usize::max(lhs.shape.dims[i], rhs.shape.dims[i]);
            num_batches *= shape_out[i];
        }
        shape_out[ndims - 2] = num_rows;
        shape_out[ndims - 1] = num_cols;
        let shape_out = Shape::from(shape_out);

        // Create a buffer for the output tensor.
        let buffer = lhs
            .client
            .empty(shape_out.num_elements() * core::mem::size_of::&lt;F&gt;());

        // Create the output tensor primitive.
        let output = CubeTensor::new_contiguous(
            lhs.client.clone(),
            lhs.device.clone(),
            shape_out,
            buffer,
            F::dtype(),
        );

        // Create the kernel.
        let kernel = FusedMatmulAddRelu::&lt;F&gt;::new(cube_dim);

        // Build info buffer with tensor information needed by the kernel, such as shapes and strides.
        let info = build_info::&lt;_, F&gt;(&amp;[&amp;lhs, &amp;rhs, &amp;output]);
        let info_handle = lhs.client.create(bytemuck::cast_slice(&amp;info));

        // Declare the wgsl workgroup with the number of cubes in x, y and z.
        let cubes_needed_in_x = f32::ceil(num_rows as f32 / cube_dim.x as f32) as u32;
        let cubes_needed_in_y = f32::ceil(num_cols as f32 / cube_dim.y as f32) as u32;
        let cube_count =
            CubeCount::Static(cubes_needed_in_x, cubes_needed_in_y, num_batches as u32);

        // Execute lazily the kernel with the launch information and the given buffers.
        lhs.client.execute(
            Box::new(SourceKernel::new(kernel, cube_dim)),
            cube_count,
            vec![
                lhs.handle.binding(),
                rhs.handle.binding(),
                bias.handle.binding(),
                output.handle.clone().binding(),
                info_handle.binding(),
            ],
        );

        // Return the output tensor.
        output
    }
}</code></pre>
<p>In the preceding code block, we demonstrated how to launch the kernel that modifies the correct
buffer. It's important to note that Rust's mutability safety doesn't apply here; the context has the
capability to execute any mutable operation on any buffer. While this isn't a problem in the
previous scenario where we only modify the newly created output buffer, it is wise to keep this in
mind.</p>
<h2 id="backward-1"><a class="header" href="#backward-1">Backward</a></h2>
<p>Now that the custom backend trait is implemented for the WGPU backend, you can use it to invoke the
<code>matmul_add_relu_custom</code> function. However, calculating gradients is not yet possible at this stage.
If your use case does not extend beyond inference, there is no need to implement any of the
following code.</p>
<p>For the backward pass, we will leverage the backend implementation from <code>mabor-autodiff</code>, which is
actually generic over the backend. Instead of crafting our own WGSL kernel for the backward pass, we
will use our fused kernel only for the forward pass, and compute the gradient using basic
operations.</p>
<pre><code class="language-rust  ignore">// Implement our custom backend trait for any backend that also implements our custom backend trait.
//
// Note that we could implement the backend trait only for the Wgpu backend instead of any backend that
// also implements our own API. This would allow us to call any function only implemented for Wgpu
// and potentially call a custom kernel crafted only for this task.
impl&lt;B: Backend, C: CheckpointStrategy&gt; Backend for Autodiff&lt;B, C&gt; {
    fn fused_matmul_add_relu(
        lhs: FloatTensor&lt;Self&gt;,
        rhs: FloatTensor&lt;Self&gt;,
        bias: FloatTensor&lt;Self&gt;,
    ) -&gt; FloatTensor&lt;Self&gt; {
        // Create our zero-sized type that will implement the Backward trait.
        #[derive(Debug)]
        struct FusedMatmulAddReluBackward;

        // Implement the backward trait for the given backend B, the node gradient
        // with three other gradients to calculate (lhs, rhs, and bias).
        impl&lt;B: Backend&gt; Backward&lt;B, 3&gt; for FusedMatmulAddReluBackward {
            // Our state that we must build during the forward pass to compute the backward pass.
            //
            // Note that we could improve the performance further by only keeping the state of
            // tensors that are tracked, improving memory management, but for simplicity, we avoid
            // that part.
            type State = (NodeID, NodeID, FloatTensor&lt;B&gt;, Shape);

            fn backward(
                self,
                ops: Ops&lt;Self::State, 3&gt;,
                grads: &amp;mut Gradients,
                checkpointer: &amp;mut Checkpointer,
            ) {
                // Get the nodes of each variable.
                let [node_lhs, node_rhs, node_bias] = ops.parents;
                // Fetch the gradient for the current node.
                let grad = grads.consume::&lt;B&gt;(&amp;ops.node);

                // Set our state.
                let (lhs_state, rhs_state, output, shape_bias) = ops.state;
                let lhs: FloatTensor&lt;B&gt; = checkpointer.retrieve_node_output(lhs_state);
                let rhs: FloatTensor&lt;B&gt; = checkpointer.retrieve_node_output(rhs_state);

                // Fetch shapes of our tensor to support broadcasting.
                let shape_lhs = lhs.shape();
                let shape_rhs = rhs.shape();

                // Compute the gradient of the output using the already existing `relu_backward`
                // function in the basic Mabor backend trait.
                let grad_output = B::relu_backward(output, grad);

                // Compute the lhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_lhs = broadcast_shape::&lt;B&gt;(
                    B::float_matmul(grad_output.clone(), B::float_transpose(rhs)),
                    &amp;shape_lhs,
                );
                // Compute the rhs gradient, which is the derivative of matmul with support for
                // broadcasting.
                let grad_rhs = broadcast_shape::&lt;B&gt;(
                    B::float_matmul(B::float_transpose(lhs), grad_output.clone()),
                    &amp;shape_rhs,
                );
                // The add derivative is only 1, so we just need to support broadcasting to
                // compute the bias gradient.
                let grad_bias = broadcast_shape::&lt;B&gt;(grad_output, &amp;shape_bias);

                // Register the gradient for each variable based on whether they are marked as
                // `tracked`.
                if let Some(node) = node_bias {
                    grads.register::&lt;B&gt;(node.id, grad_bias);
                }
                if let Some(node) = node_lhs {
                    grads.register::&lt;B&gt;(node.id, grad_lhs);
                }
                if let Some(node) = node_rhs {
                    grads.register::&lt;B&gt;(node.id, grad_rhs);
                }
            }
        }

        // Prepare a stateful operation with each variable node and corresponding graph.
        //
        // Each node can be fetched with `ops.parents` in the same order as defined here.
        match FusedMatmulAddReluBackward
            .prepare::&lt;C&gt;([lhs.node.clone(), rhs.node.clone(), bias.node.clone()])
            // Marks the operation as compute bound, meaning it will save its
            // state instead of recomputing itself during checkpointing
            .compute_bound()
            .stateful()
        {
            OpsKind::Tracked(mut prep) =&gt; {
                // When at least one node is tracked, we should register our backward step.

                // The state consists of what will be needed for this operation's backward pass.
                // Since we need the parents' outputs, we must checkpoint their ids to retrieve their node
                // output at the beginning of the backward. We can also save utilitary data such as the bias shape
                // If we also need this operation's output, we can either save it in the state or recompute it
                // during the backward pass. Here we choose to save it in the state because it's a compute bound operation.
                let lhs_state = prep.checkpoint(&amp;lhs);
                let rhs_state = prep.checkpoint(&amp;rhs);
                let bias_shape = bias.primitive.shape();

                let output = B::fused_matmul_add_relu(
                    lhs.primitive.clone(),
                    rhs.primitive.clone(),
                    bias.primitive,
                );

                let state = (lhs_state, rhs_state, output.clone(), bias_shape);

                prep.finish(state, output)
            }
            OpsKind::UnTracked(prep) =&gt; {
                // When no node is tracked, we can just compute the original operation without
                // keeping any state.
                let output = B::fused_matmul_add_relu(lhs.primitive, rhs.primitive, bias.primitive);
                prep.finish(output)
            }
        }
    }
}</code></pre>
<p>The previous code is self-documented to make it clearer, but here is what it does in summary.</p>
<p>We define <code>fused_matmul_add_relu</code> within <code>Autodiff&lt;B&gt;</code>, allowing any autodiff-decorated backend to
benefit from our implementation. In an autodiff-decorated backend, the forward pass must still be
implemented. This is achieved using a comprehensive match statement block where computation is
delegated to the inner backend, while keeping track of a state. The state comprises any information
relevant to the backward pass, such as input and output tensors, along with the bias shape. When an
operation isn't tracked (meaning there won't be a backward pass for this specific operation in the
graph), storing a state becomes unnecessary, and we simply perform the forward computation.</p>
<p>The backward pass uses the gradient obtained from the preceding node in the computation graph. It
calculates the derivatives for <code>relu</code> (<code>relu_backward</code>), add (no operation is required here, as the
derivative is one), and <code>matmul</code> (another <code>matmul</code> with transposed inputs). This results in
gradients for both input tensors and the bias, which are registered for consumption by subsequent
operation nodes.</p>
<p>The only remaining part is to implement our autodiff-decorated backend trait for our WGPU Backend.</p>
<pre><code class="language-rust  ignore">impl&lt;G: GraphicsApi, F: FloatElement, I: IntElement&gt; AutodiffBackend for Autodiff&lt;WgpuBackend&lt;G, F, I&gt;&gt;
{
}</code></pre>
<h2 id="conclusion-3"><a class="header" href="#conclusion-3">Conclusion</a></h2>
<p>In this guide, we've implemented a fused kernel using the WGPU backend, enabling execution on any
GPU. By delving into the inner workings of both the WGPU backend and the autodiff backend, we've
gained a deeper understanding of these systems.</p>
<p>While extending a backend may be harder than working with straightforward tensors, the benefits can
be worth it. This approach enables the crafting of custom models with greater control over
execution, which can potentially greatly enhance the performance of your models.</p>
<p>As we conclude this guide, we hope that you have gained insights into Mabor's world of backend
extensions, and that it will help you to unleash the full potential of your projects.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="webassembly"><a class="header" href="#webassembly">WebAssembly</a></h1>
<p>Mabor supports WebAssembly (WASM) execution using the <code>NdArray</code> and <code>WebGpu</code> backends, allowing
models to run directly in the browser.</p>
<p>Check out the following examples:</p>
<ul>
<li><a href="https://github.com/tracel-ai/burn/tree/main/examples/image-classification-web">Image Classification Web</a></li>
<li><a href="https://github.com/tracel-ai/burn/tree/main/examples/mnist-inference-web">MNIST Inference on Web</a></li>
</ul>
<p>When targeting WebAssembly, certain dependencies require additional configuration. In particular,
the <code>getrandom</code> crate requires explicit setting when using <code>WebGpu</code>.</p>
<p>Following the <a href="https://github.com/rust-random/getrandom/#webassembly-support">recommended usage</a>,
make sure to explicitly add the dependency with the <code>wasm_js</code> feature flag for your project.</p>
<pre><code class="language-toml">[dependencies]
getrandom = { version = "0.3.2", default-features = false, features = [
    "wasm_js",
] }
</code></pre>
<p>You also need to set the <code>getrandom_backend</code> accordingly via the rust-flags. The flag can either be
set by specifying the <code>rustflags</code> field in <code>.cargo/config.toml</code></p>
<pre><code class="language-toml">[target.wasm32-unknown-unknown]
rustflags = ['--cfg', 'getrandom_backend="wasm_js"']
</code></pre>
<p>Or by using the <code>RUSTFLAGS</code> environment variable:</p>
<pre><code>RUSTFLAGS='--cfg getrandom_backend="wasm_js"'
</code></pre>
<p>This change is now explicitly required with latest versions of Mabor, following the <code>getrandom</code>
recommendations. This avoids potential issues for WASM developers who do not target Web targets.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="no-standard-library"><a class="header" href="#no-standard-library">No Standard Library</a></h1>
<p>In this section, you will learn how to run an onnx inference model on an embedded system, with no standard library support on a Raspberry Pi Pico. This should be universally applicable to other platforms. All the code can be found under the
<a href="https://github.com/tracel-ai/burn/tree/main/examples/raspberry-pi-pico">examples directory</a>.</p>
<h2 id="step-by-step-guide-1"><a class="header" href="#step-by-step-guide-1">Step-by-Step Guide</a></h2>
<p>Let's walk through the process of running an embedded ONNX model:</p>
<h3 id="setup"><a class="header" href="#setup">Setup</a></h3>
<p>Follow the <a href="https://embassy.dev/book/#_getting_started">embassy guide</a> for your specific environment. Once setup, you should have something similar to the following.</p>
<pre><code>./inference
├── Cargo.lock
├── Cargo.toml
├── build.rs
├── memory.x
└── src
    └── main.rs
</code></pre>
<p>Some other dependencies have to be added</p>
<pre><code class="language-toml">[dependencies]
embedded-alloc = "0.6.0" # Only if there is no default allocator for your chip
mabor = { version = "0.18", default-features = false, features = ["ndarray"] } # Backend must be ndarray

[build-dependencies]
mabor-import = { version = "0.18" } # Used to auto generate the rust code to import the model
</code></pre>
<h3 id="import-the-model"><a class="header" href="#import-the-model">Import the Model</a></h3>
<p>Follow the directions to <a href="advanced/../import/README.html">import models</a>.</p>
<p>Use the following ModelGen config</p>
<pre><code class="language-rs">ModelGen::new()
    .input(my_model)
    .out_dir("model/")
    .record_type(RecordType::Bincode)
    .embed_states(true)
    .run_from_script();
</code></pre>
<h3 id="global-allocator"><a class="header" href="#global-allocator">Global Allocator</a></h3>
<p>First define a global allocator (if you are on a no_std system without alloc).</p>
<pre><code class="language-rs">use embedded_alloc::LlffHeap as Heap;

#[global_allocator]
static HEAP: Heap = Heap::empty();

#[embassy_executor::main]
async fn main(_spawner: Spawner) {
	{
        use core::mem::MaybeUninit;
        const HEAP_SIZE: usize = 100 * 1024; // This is dependent on the model size in memory.
        static mut HEAP_MEM: [MaybeUninit&lt;u8&gt;; HEAP_SIZE] = [MaybeUninit::uninit(); HEAP_SIZE];
        unsafe { HEAP.init(&amp;raw mut HEAP_MEM as usize, HEAP_SIZE) }
    }
}
</code></pre>
<h3 id="define-backend"><a class="header" href="#define-backend">Define Backend</a></h3>
<p>We are using ndarray, so we just need to define the NdArray backend as usual</p>
<pre><code class="language-rs">use mabor::{backend::NdArray, tensor::Tensor};

type Backend = NdArray&lt;f32&gt;;
type BackendDevice = &lt;Backend as mabor::tensor::backend::Backend&gt;::Device;
</code></pre>
<p>Then inside the <code>main</code> function add</p>
<pre><code class="language-rs">use your_model::Model;

// Get a default device for the backend
let device = BackendDevice::default();

// Create a new model and load the state
let model: Model&lt;Backend&gt; = Model::default();
</code></pre>
<h3 id="running-the-model"><a class="header" href="#running-the-model">Running the Model</a></h3>
<p>To run the model, just call it as you would normally</p>
<pre><code class="language-rs">// Define the tensor
let input = Tensor::&lt;Backend, 2&gt;::from_floats([[input]], &amp;device);

// Run the model on the input
let output = model.forward(input);
</code></pre>
<h2 id="conclusion-4"><a class="header" href="#conclusion-4">Conclusion</a></h2>
<p>Running a model in a no_std environment is pretty much identical to a normal environment. All that is needed is a global allocator.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>


    </div>
    </body>
</html>
